{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374c71ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3914 sentences.\n",
      "\n",
      "Example sentence:\n",
      " [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import random\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "# Make sure you have the corpora\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "\n",
    "# Convert to list so we can sample\n",
    "test_sentences = list(treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "# Now sample 100\n",
    "gpt_test_sentences = random.sample(test_sentences, 100)\n",
    "\n",
    "print(f\"Sampled {len(test_sentences)} sentences.\")\n",
    "print(\"\\nExample sentence:\\n\", test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5441a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9327d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 30 06:51:32 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   53C    P0              68W / 300W |  74293MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              71W / 300W |  29261MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   55C    P0              74W / 300W |  71866MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              60W / 300W |    751MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6affed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(input_words):\n",
    "    # Use only the last fold\n",
    "    pos_tags = viterbi_algo(input_words, 0)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b34df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(sentence, fold_index):\n",
    "    sentence = sentence.copy()\n",
    "    sentence.append('.')\n",
    "    # if re.search(r'[a-zA-Z]',sentence[-1]):\n",
    "    #   sentence.append('.')\n",
    "    #   print(\"Sentence didn't ended with a .\")\n",
    "    viterbi_table = [{} for _ in range(len(sentence))]\n",
    "    backpointer = [{} for _ in range(len(sentence))]\n",
    "    emission_prob = emission_prob_list[fold_index]\n",
    "    transition_prob = transition_prob_list[fold_index]\n",
    "\n",
    "    for tag in transition_prob[start_tag]:\n",
    "        word = sentence[0].lower()\n",
    "        viterbi_table[0][tag] = transition_prob[start_tag].get(tag, 1e-6) * emission_prob.get(word, {}).get(tag, 1e-6)\n",
    "        backpointer[0][tag] = start_tag\n",
    "\n",
    "    for t in range(1, len(sentence)):\n",
    "        word = sentence[t].lower()\n",
    "        for curr_tag in transition_prob:\n",
    "            max_prob, best_prev_tag = max(\n",
    "                (viterbi_table[t-1][prev_tag] * transition_prob[prev_tag].get(curr_tag, 1e-6) * emission_prob.get(word, {}).get(curr_tag, 1e-6), prev_tag)\n",
    "                for prev_tag in viterbi_table[t-1]\n",
    "            )\n",
    "            viterbi_table[t][curr_tag] = max_prob\n",
    "            backpointer[t][curr_tag] = best_prev_tag\n",
    "\n",
    "    best_path = []\n",
    "    best_last_tag = max(viterbi_table[-1], key=viterbi_table[-1].get)\n",
    "    best_path.append(best_last_tag)\n",
    "    for t in range(len(sentence) - 1, 0, -1):\n",
    "        best_last_tag = backpointer[t][best_last_tag]\n",
    "        best_path.insert(0, best_last_tag)\n",
    "    best_path.pop()\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da874a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOXeKIhfBLgP",
    "outputId": "ea22b263-ed5d-4301-e2bf-340275f1acf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'VERB', 'DET', 'NOUN', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from nltk.tag import map_tag\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "start_tag = '^'\n",
    "# Load JSON from file\n",
    "with open('emission_prob_list.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "# Convert JSON to Python object\n",
    "emission_prob_list = json.loads(json_data)\n",
    "num_folds = len(emission_prob_list)\n",
    "# Load JSON from file\n",
    "with open('transition_prob_list.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "# Convert JSON to Python object\n",
    "transition_prob_list = json.loads(json_data)\n",
    "sentence = \"This is the course.\"\n",
    "input_words = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "get_answer(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3185898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tags = ['.',\n",
    " 'ADJ',\n",
    " 'ADP',\n",
    " 'ADV',\n",
    " 'CONJ',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'NUM',\n",
    " 'PRON',\n",
    " 'PRT',\n",
    " 'VERB',\n",
    " 'X',\n",
    " '^']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4378f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = gpt_test_sentences\n",
    "X_test = [[word for word, tag in sent] for sent in test_sentences]\n",
    "y_test = [[tag for word, tag in sent] for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c76d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████| 100/100 [00:00<00:00, 335.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize confusion matrix\n",
    "per_pos_accuracy = defaultdict(lambda: defaultdict(int))\n",
    "for each_tag1 in sorted_tags:\n",
    "    for each_tag2 in sorted_tags:\n",
    "        per_pos_accuracy[each_tag1][each_tag2] = 0\n",
    "\n",
    "tag_predicted = []\n",
    "actual_tags = []\n",
    "\n",
    "def get_answer(input_words):\n",
    "    # Use only the last fold\n",
    "    pos_tags = viterbi_algo(input_words, 0)\n",
    "    return pos_tags\n",
    "\n",
    "# Evaluate on test set with tqdm\n",
    "num_folds = len(emission_prob_list)\n",
    "\n",
    "for sent, true_tags in tqdm(zip(X_test, y_test), total=len(X_test), desc=\"Evaluating\"):\n",
    "    # sent is already a list of words\n",
    "    pred_tags = get_answer(sent)\n",
    "    \n",
    "    actual_tags.extend(true_tags)\n",
    "    tag_predicted.extend(pred_tags)\n",
    "\n",
    "    # Update confusion matrix\n",
    "    for t_true, t_pred in zip(true_tags, pred_tags):\n",
    "        per_pos_accuracy[t_true][t_pred] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99653e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(actual, predicted):\n",
    "    correct = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
    "    total = len(actual)\n",
    "    return (correct / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3374503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.51%\n"
     ]
    }
   ],
   "source": [
    "# Compute overall accuracy\n",
    "test_accuracy = calc_acc(actual_tags, tag_predicted)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b9847",
   "metadata": {},
   "source": [
    "### HMM Analysis (Penn Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f6dde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.81\n",
      "Precision: 0.76\n",
      "Recall   : 0.81\n",
      "F1 Score : 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(actual_tags, tag_predicted)\n",
    "precision = precision_score(actual_tags, tag_predicted, average=\"weighted\")\n",
    "recall = recall_score(actual_tags, tag_predicted, average=\"weighted\")\n",
    "f1 = f1_score(actual_tags, tag_predicted, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1 Score : {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f639228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.89      0.91      0.90     11715\n",
      "         ADJ       0.77      0.82      0.79      6397\n",
      "         ADP       0.74      0.97      0.84      9857\n",
      "         ADV       0.87      0.78      0.83      3171\n",
      "        CONJ       0.99      0.99      0.99      2265\n",
      "         DET       0.71      0.94      0.81      8725\n",
      "        NOUN       0.87      0.85      0.86     28867\n",
      "         NUM       0.72      0.64      0.68      3546\n",
      "        PRON       0.67      0.69      0.68      2737\n",
      "         PRT       0.65      0.40      0.49      3219\n",
      "        VERB       0.80      0.92      0.86     13564\n",
      "           X       0.00      0.00      0.00      6613\n",
      "           ^       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.81    100676\n",
      "   macro avg       0.67      0.69      0.67    100676\n",
      "weighted avg       0.76      0.81      0.78    100676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(actual_tags, tag_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
