{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06586fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7b8e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from nltk.tag import map_tag\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adfc802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "start_tag = '^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a882aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93576848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VERB', '.', 'PRON', 'CONJ', 'PRT', 'NOUN', 'ADV', 'X', 'DET', 'ADJ', 'ADP', 'NUM'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X',\n",
       " '^']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = set(tag for sent in data for _, tag in sent)\n",
    "sorted_tags = sorted(tags)\n",
    "sorted_tags.append(start_tag)\n",
    "print(tags)\n",
    "sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbc81436",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_size = len(sorted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "811571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_idx = {c:i for i, c in enumerate(sorted_tags)}\n",
    "idx_to_tag = {i:c for i, c in enumerate(sorted_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba479a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56057\n"
     ]
    }
   ],
   "source": [
    "words = [word for sent in data for word, tag in sent]\n",
    "words = set(words)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42a37098",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "657cbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {c:i for i, c in enumerate(words)}\n",
    "idx_to_word = {i:c for i, c in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37e573ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "sent_by_word = []\n",
    "sent_by_tag = []\n",
    "for sent in data:\n",
    "    s = []\n",
    "    t = []\n",
    "    for word, tag in sent:\n",
    "        s.append(word)\n",
    "        t.append(tag)\n",
    "    sent_by_word.append(s)\n",
    "    sent_by_tag.append(t)\n",
    "print(len(sent_by_word))\n",
    "print(len(sent_by_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c9d9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adb02590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0a0+f70bd71a48.nv24.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.8)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e15b1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load once (downloads the model the first time)\n",
    "_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "793223bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6fc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45235d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports #####\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "train_X = sent_by_word\n",
    "train_y = sent_by_tag\n",
    "\n",
    "##### Helper Functions #####\n",
    "def Encode(text, vocab_size):\n",
    "#     output = np.zeros((vocab_size, 1))\n",
    "#     output[word_to_idx[text]] = 1\n",
    "    output = np.reshape(Embedding(text), (-1, 1))\n",
    "    \n",
    "\n",
    "    return output\n",
    "\n",
    "def Embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Returns a dense 384-dimensional embedding.\"\"\"\n",
    "    return _model.encode(text, normalize_embeddings=True)\n",
    "\n",
    "# Xavier Normalized Initialization\n",
    "def initWeights(input_size, output_size):\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6 / (input_size + output_size))\n",
    "\n",
    "##### Activation Functions #####\n",
    "def sigmoid(input, derivative = False):\n",
    "    if derivative:\n",
    "        return input * (1 - input)\n",
    "    \n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 - input ** 2\n",
    "    \n",
    "    return np.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.sum(np.exp(input))\n",
    "\n",
    "##### Long Short-Term Memory Network Class #####\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate\n",
    "        self.wf = initWeights(input_size, hidden_size)\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = initWeights(input_size, hidden_size)\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.wc = initWeights(input_size, hidden_size)\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = initWeights(input_size, hidden_size)\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.wy = initWeights(hidden_size, output_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    # Reset Network Memory\n",
    "    def reset(self):\n",
    "        self.concat_inputs = {}\n",
    "\n",
    "        self.hidden_states = {-1:np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states = {-1:np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.activation_outputs = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward Propogation\n",
    "    def forward(self, inputs):\n",
    "        self.reset()\n",
    "\n",
    "        outputs = []\n",
    "        for q in range(len(inputs)):\n",
    "            self.concat_inputs[q] = np.concatenate((self.hidden_states[q - 1], inputs[q]))\n",
    "\n",
    "            self.forget_gates[q] = sigmoid(np.dot(self.wf, self.concat_inputs[q]) + self.bf)\n",
    "            self.input_gates[q] = sigmoid(np.dot(self.wi, self.concat_inputs[q]) + self.bi)\n",
    "            self.candidate_gates[q] = tanh(np.dot(self.wc, self.concat_inputs[q]) + self.bc)\n",
    "            self.output_gates[q] = sigmoid(np.dot(self.wo, self.concat_inputs[q]) + self.bo)\n",
    "\n",
    "            self.cell_states[q] = self.forget_gates[q] * self.cell_states[q - 1] + self.input_gates[q] * self.candidate_gates[q]\n",
    "            self.hidden_states[q] = self.output_gates[q] * tanh(self.cell_states[q])\n",
    "\n",
    "            outputs += [np.dot(self.wy, self.hidden_states[q]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # Backward Propogation\n",
    "    def backward(self, errors, inputs):\n",
    "        d_wf, d_bf = 0, 0\n",
    "        d_wi, d_bi = 0, 0\n",
    "        d_wc, d_bc = 0, 0\n",
    "        d_wo, d_bo = 0, 0\n",
    "        d_wy, d_by = 0, 0\n",
    "\n",
    "        dh_next, dc_next = np.zeros_like(self.hidden_states[0]), np.zeros_like(self.cell_states[0])\n",
    "        for q in reversed(range(len(inputs))):\n",
    "            error = errors[q]\n",
    "\n",
    "            # Final Gate Weights and Biases Errors\n",
    "            d_wy += np.dot(error, self.hidden_states[q].T)\n",
    "            d_by += error\n",
    "\n",
    "            # Hidden State Error\n",
    "            d_hs = np.dot(self.wy.T, error) + dh_next\n",
    "\n",
    "            # Output Gate Weights and Biases Errors\n",
    "            d_o = tanh(self.cell_states[q]) * d_hs \n",
    "            d_wo += np.dot(d_o, inputs[q].T)* sigmoid(self.output_gates[q], derivative = True)\n",
    "            d_bo += d_o* sigmoid(self.output_gates[q], derivative = True)\n",
    "\n",
    "            # Cell State Error\n",
    "            d_cs = tanh(tanh(self.cell_states[q]), derivative = True) * self.output_gates[q] * d_hs + dc_next\n",
    "\n",
    "            # Forget Gate Weights and Biases Errors\n",
    "            d_f = d_cs * self.cell_states[q - 1] \n",
    "            d_wf += np.dot(d_f, inputs[q].T)* sigmoid(self.forget_gates[q], derivative = True)\n",
    "            d_bf += d_f* sigmoid(self.forget_gates[q], derivative = True)\n",
    "\n",
    "            # Input Gate Weights and Biases Errors\n",
    "            d_i = d_cs * self.candidate_gates[q] \n",
    "            d_wi += np.dot(d_i, inputs[q].T) * sigmoid(self.input_gates[q], derivative = True)\n",
    "            d_bi += d_i * sigmoid(self.input_gates[q], derivative = True)\n",
    "            \n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            d_c = d_cs * self.input_gates[q] \n",
    "            d_wc += np.dot(d_c, inputs[q].T) * tanh(self.candidate_gates[q], derivative = True)\n",
    "            d_bc += d_c * tanh(self.candidate_gates[q], derivative = True)\n",
    "\n",
    "            # Concatenated Input Error (Sum of Error at Each Gate!)\n",
    "            d_z = np.dot(self.wf.T, d_f * sigmoid(self.forget_gates[q], derivative = True)) + \n",
    "                    np.dot(self.wi.T, d_i*sigmoid(self.input_gates[q], derivative = True)) + \n",
    "                    np.dot(self.wc.T, d_c* tanh(self.candidate_gates[q], derivative = True)) + \n",
    "                    np.dot(self.wo.T, d_o* sigmoid(self.output_gates[q], derivative = True))\n",
    "\n",
    "            # Error of Hidden State and Cell State at Next Time Step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.forget_gates[q] * d_cs\n",
    "\n",
    "#         for d_ in (d_wf, d_bf, d_wi, d_bi, d_wc, d_bc, d_wo, d_bo, d_wy, d_by):\n",
    "#             np.clip(d_, -1, 1, out = d_)\n",
    "\n",
    "        self.wf -= d_wf * self.learning_rate\n",
    "        self.bf -= d_bf * self.learning_rate\n",
    "\n",
    "        self.wi -= d_wi * self.learning_rate\n",
    "        self.bi -= d_bi * self.learning_rate\n",
    "\n",
    "        self.wc -= d_wc * self.learning_rate\n",
    "        self.bc -= d_bc * self.learning_rate\n",
    "\n",
    "        self.wo -= d_wo * self.learning_rate\n",
    "        self.bo -= d_bo * self.learning_rate\n",
    "\n",
    "        self.wy -= d_wy * self.learning_rate\n",
    "        self.by -= d_by * self.learning_rate\n",
    "\n",
    "    # Train\n",
    "    def train(self, original_inputs, labels):\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            for inputs1, labels1 in zip(original_inputs, labels):\n",
    "                inputs = [Encode(input, vocab_size) for input in inputs1]\n",
    "\n",
    "                predictions = self.forward(inputs)\n",
    "\n",
    "                errors = []\n",
    "                for q in range(len(predictions)):\n",
    "#                     print(q)\n",
    "#                     print(labels1[q])\n",
    "                    errors += [softmax(predictions[q])]\n",
    "                    errors[-1][tag_to_idx[labels1[q]]] -= 1\n",
    "\n",
    "                self.backward(errors, self.concat_inputs)\n",
    "    \n",
    "    # Test\n",
    "    def test(self, original_inputs, labels):\n",
    "        accuracy = 0\n",
    "        to_divide = 0\n",
    "        for inputs, labels1 in zip(original_inputs, labels):\n",
    "            probabilities = self.forward([Encode(input, vocab_size) for input in inputs])\n",
    "\n",
    "            output = ''\n",
    "            to_divide = to_divide + len(labels1)\n",
    "            for q in range(len(labels1)):\n",
    "#                 print(np.random.choice([*range(vocab_size)]))\n",
    "#                 print(softmax(probabilities[q].reshape(-1)))\n",
    "                probs = softmax(probabilities[q].reshape(-1))\n",
    "                pred_idx = np.argmax(probs)   # deterministic prediction\n",
    "                prediction = idx_to_tag[pred_idx]\n",
    "\n",
    "                output += prediction\n",
    "\n",
    "                if prediction == labels1[q]:\n",
    "                    accuracy += 1\n",
    "                \n",
    "\n",
    "#         print(f'Ground Truth:\\nt{labels}\\n')\n",
    "#         print(f'Predictions:\\nt{\"\".join(output)}\\n')\n",
    "        \n",
    "        print(f'Accuracy: {round(accuracy * 100 / to_divide, 2)}%')\n",
    "        \n",
    "# Initialize Network\n",
    "hidden_size = 128\n",
    "\n",
    "lstm = LSTM(input_size = vocab_size + hidden_size, hidden_size = hidden_size, output_size = tag_size, num_epochs = 10, learning_rate = 0.05)\n",
    "\n",
    "##### Training #####\n",
    "lstm.train(train_X[:500], train_y[:500])\n",
    "\n",
    "##### Testing #####\n",
    "lstm.test(train_X[500:600], train_y[500:600])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
