{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81594529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN', 'PRT', 'PRON', 'ADP', '.', 'CONJ', 'ADV', 'VERB', 'DET', 'ADJ', 'NUM', 'X'}\n",
      "57340\n",
      "57340\n",
      "56057\n",
      "45872\n",
      "11468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from nltk.tag import map_tag\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "start_tag = '^'\n",
    "\n",
    "data = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tags = set(tag for sent in data for _, tag in sent)\n",
    "sorted_tags = sorted(tags)\n",
    "sorted_tags.append(start_tag)\n",
    "print(tags)\n",
    "\n",
    "\n",
    "sent_by_word = []\n",
    "sent_by_tag = []\n",
    "for sent in data:\n",
    "    s = []\n",
    "    t = []\n",
    "    for word, tag in sent:\n",
    "        s.append(word)\n",
    "        t.append(tag)\n",
    "    sent_by_word.append(s)\n",
    "    sent_by_tag.append(t)\n",
    "print(len(sent_by_word))\n",
    "print(len(sent_by_tag))\n",
    "\n",
    "data_X = sent_by_word\n",
    "data_y = sent_by_tag\n",
    "\n",
    "\n",
    "\n",
    "words = [word for sent in data for word, tag in sent]\n",
    "words = set(words)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "tag_to_idx = {c:i for i, c in enumerate(sorted_tags)}\n",
    "idx_to_tag = {i:c for i, c in enumerate(sorted_tags)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_X, data_y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load once (downloads the model the first time)\n",
    "_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vocab_size = 384\n",
    "\n",
    "\n",
    "# Xavier-like initialization (keeps same shape semantics as your numpy version)\n",
    "def initWeights(input_size, output_size):\n",
    "    limit = np.sqrt(6.0 / (input_size + output_size))\n",
    "    # create tensor on device with shape (output_size, input_size)\n",
    "    return torch.empty((output_size, input_size), device=device).uniform_(-limit, limit)\n",
    "\n",
    "##### Activation Functions #####\n",
    "def sigmoid(input, derivative = False):\n",
    "    if derivative:\n",
    "        # expecting input already sigmoid-activated when derivative=True\n",
    "        return input * (1 - input)\n",
    "    return 1.0 / (1.0 + torch.exp(-input))\n",
    "\n",
    "def tanh(input, derivative = False):\n",
    "    if derivative:\n",
    "        # expecting input already tanh-activated when derivative=True\n",
    "        return 1 - input ** 2\n",
    "    return torch.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    # input: column vector (C,1) or 1D; keep shape (C,1)\n",
    "#     if input.dim() == 2 and input.shape[1] == 1:\n",
    "#         vec = input.view(-1)\n",
    "#         s = torch.exp(vec) / torch.sum(torch.exp(vec))\n",
    "#         return s.view(-1, 1)\n",
    "#     else:\n",
    "#         vec = input.view(-1)\n",
    "#         s = torch.exp(vec) / torch.sum(torch.exp(vec))\n",
    "#         return s.view(-1, 1)\n",
    "    # input: column vector (C,1) or 1D -> returns (C,1)\n",
    "    vec = input.view(-1)\n",
    "    maxv = torch.max(vec)\n",
    "    exps = torch.exp(vec - maxv)\n",
    "    s = exps / torch.sum(exps)\n",
    "    return s.view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Helper Functions #####\n",
    "def Encode(text, vocab_size):\n",
    "    emb = _model.encode(text, normalize_embeddings=True)\n",
    "    if isinstance(emb, np.ndarray):\n",
    "        emb = torch.from_numpy(emb).float()\n",
    "    else:\n",
    "        emb = torch.tensor(emb, dtype=torch.float32)\n",
    "    emb = emb.reshape(-1, 1).to(device)   # ensure column vector and on device\n",
    "    return emb\n",
    "\n",
    "##### Long Short-Term Memory Network Class #####\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # NOTE: shapes mirror your numpy version: weight matrices are (output_size, input_size)\n",
    "        total_input = input_size  # when you call this, pass vocab_size + hidden_size as before\n",
    "\n",
    "        # Forget Gate\n",
    "        self.wf = initWeights(total_input, hidden_size)\n",
    "        self.bf = torch.zeros((hidden_size, 1), device=device)\n",
    "\n",
    "        # Input Gate\n",
    "        self.wi = initWeights(total_input, hidden_size)\n",
    "        self.bi = torch.zeros((hidden_size, 1), device=device)\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.wc = initWeights(total_input, hidden_size)\n",
    "        self.bc = torch.zeros((hidden_size, 1), device=device)\n",
    "\n",
    "        # Output Gate\n",
    "        self.wo = initWeights(total_input, hidden_size)\n",
    "        self.bo = torch.zeros((hidden_size, 1), device=device)\n",
    "\n",
    "        # Final Gate (wy maps hidden -> output)\n",
    "        self.wy = initWeights(hidden_size, output_size)\n",
    "        self.by = torch.zeros((output_size, 1), device=device)\n",
    "\n",
    "    # Reset Network Memory\n",
    "    def reset(self):\n",
    "        self.concat_inputs = {}\n",
    "        self.hidden_states = {-1: torch.zeros((self.hidden_size, 1), device=device)}\n",
    "        self.cell_states = {-1: torch.zeros((self.hidden_size, 1), device=device)}\n",
    "\n",
    "        self.activation_outputs = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward Propogation\n",
    "    def forward(self, inputs):\n",
    "        # inputs: list of column vector tensors (each shape (input_dim,1))\n",
    "        self.reset()\n",
    "        outputs = []\n",
    "        for q in range(len(inputs)):\n",
    "            # concat previous hidden and current input (both column vectors)\n",
    "            self.concat_inputs[q] = torch.cat((self.hidden_states[q - 1], inputs[q]), dim=0)\n",
    "\n",
    "            self.forget_gates[q] = sigmoid(torch.matmul(self.wf, self.concat_inputs[q]) + self.bf)\n",
    "            self.input_gates[q] = sigmoid(torch.matmul(self.wi, self.concat_inputs[q]) + self.bi)\n",
    "            self.candidate_gates[q] = tanh(torch.matmul(self.wc, self.concat_inputs[q]) + self.bc)\n",
    "            self.output_gates[q] = sigmoid(torch.matmul(self.wo, self.concat_inputs[q]) + self.bo)\n",
    "\n",
    "            self.cell_states[q] = self.forget_gates[q] * self.cell_states[q - 1] + self.input_gates[q] * self.candidate_gates[q]\n",
    "            self.hidden_states[q] = self.output_gates[q] * tanh(self.cell_states[q])\n",
    "\n",
    "            outputs.append(torch.matmul(self.wy, self.hidden_states[q]) + self.by)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # Backward Propogation (kept same algorithm, converted to torch ops)\n",
    "    def backward(self, errors, inputs):\n",
    "        d_wf = torch.zeros_like(self.wf)\n",
    "        d_wi = torch.zeros_like(self.wi)\n",
    "        d_wc = torch.zeros_like(self.wc)\n",
    "        d_wo = torch.zeros_like(self.wo)\n",
    "        d_wy = torch.zeros_like(self.wy)\n",
    "\n",
    "        d_bf = torch.zeros_like(self.bf)\n",
    "        d_bi = torch.zeros_like(self.bi)\n",
    "        d_bc = torch.zeros_like(self.bc)\n",
    "        d_bo = torch.zeros_like(self.bo)\n",
    "        d_by = torch.zeros_like(self.by)\n",
    "\n",
    "        dh_next = torch.zeros_like(self.hidden_states[0])\n",
    "        dc_next = torch.zeros_like(self.cell_states[0])\n",
    "        seq_len = len(inputs)\n",
    "\n",
    "        for q in reversed(range(seq_len)):\n",
    "            error = errors[q]  # shape (output_size, 1) tensor\n",
    "\n",
    "            # Final Gate Weights and Biases Errors\n",
    "            d_wy += torch.matmul(error, self.hidden_states[q].T)\n",
    "            d_by += error\n",
    "\n",
    "            # Hidden State Error\n",
    "            d_hs = torch.matmul(self.wy.T, error) + dh_next\n",
    "\n",
    "            # Output Gate Weights and Biases Errors\n",
    "            d_o = tanh(self.cell_states[q]) * d_hs\n",
    "            # note: inputs[q] here is concatenated input column vector\n",
    "            d_wo += torch.matmul(d_o, inputs[q].T) * sigmoid(self.output_gates[q], derivative = True)\n",
    "            d_bo += d_o * sigmoid(self.output_gates[q], derivative = True)\n",
    "\n",
    "            # Cell State Error\n",
    "            d_cs = tanh(tanh(self.cell_states[q]), derivative = True) * self.output_gates[q] * d_hs + dc_next\n",
    "\n",
    "            # Forget Gate Weights and Biases Errors\n",
    "            d_f = d_cs * self.cell_states[q - 1]\n",
    "            d_wf += torch.matmul(d_f, inputs[q].T) * sigmoid(self.forget_gates[q], derivative = True)\n",
    "            d_bf += d_f * sigmoid(self.forget_gates[q], derivative = True)\n",
    "\n",
    "            # Input Gate Weights and Biases Errors\n",
    "            d_i = d_cs * self.candidate_gates[q]\n",
    "            d_wi += torch.matmul(d_i, inputs[q].T) * sigmoid(self.input_gates[q], derivative = True)\n",
    "            d_bi += d_i * sigmoid(self.input_gates[q], derivative = True)\n",
    "\n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            d_c = d_cs * self.input_gates[q]\n",
    "            d_wc += torch.matmul(d_c, inputs[q].T) * tanh(self.candidate_gates[q], derivative = True)\n",
    "            d_bc += d_c * tanh(self.candidate_gates[q], derivative = True)\n",
    "\n",
    "            # Concatenated Input Error (Sum of Error at Each Gate!)\n",
    "            d_z = (\n",
    "                torch.matmul(self.wf.T, d_f * sigmoid(self.forget_gates[q], derivative = True))\n",
    "                + torch.matmul(self.wi.T, d_i * sigmoid(self.input_gates[q], derivative = True))\n",
    "                + torch.matmul(self.wc.T, d_c * tanh(self.candidate_gates[q], derivative = True))\n",
    "                + torch.matmul(self.wo.T, d_o * sigmoid(self.output_gates[q], derivative = True))\n",
    "            )\n",
    "\n",
    "            # Error of Hidden State and Cell State at Next Time Step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.forget_gates[q] * d_cs\n",
    "\n",
    "        # Parameter update (gradient descent)\n",
    "        self.wf = self.wf - d_wf * self.learning_rate\n",
    "        self.bf = self.bf - d_bf * self.learning_rate\n",
    "\n",
    "        self.wi = self.wi - d_wi * self.learning_rate\n",
    "        self.bi = self.bi - d_bi * self.learning_rate\n",
    "\n",
    "        self.wc = self.wc - d_wc * self.learning_rate\n",
    "        self.bc = self.bc - d_bc * self.learning_rate\n",
    "\n",
    "        self.wo = self.wo - d_wo * self.learning_rate\n",
    "        self.bo = self.bo - d_bo * self.learning_rate\n",
    "\n",
    "        self.wy = self.wy - d_wy * self.learning_rate\n",
    "        self.by = self.by - d_by * self.learning_rate\n",
    "\n",
    "    def train(self, original_inputs, labels):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            # progress bar per epoch\n",
    "            with tqdm(total=len(original_inputs),\n",
    "                      desc=f\"Epoch {epoch+1}/{self.num_epochs}\",\n",
    "                      unit=\"ex\") as pbar:\n",
    "                for inputs1, labels1 in zip(original_inputs, labels):\n",
    "                    # inputs1 is sequence of tokens; Encode returns (input_dim,1) column tensors\n",
    "                    inputs = [Encode(input, vocab_size) for input in inputs1]\n",
    "\n",
    "                    predictions = self.forward(inputs)\n",
    "\n",
    "                    errors = []\n",
    "                    for q in range(len(predictions)):\n",
    "                        # softmax returns (output_size,1)\n",
    "                        s = softmax(predictions[q])\n",
    "\n",
    "                        # subtract one at true label (as in original)\n",
    "                        label_idx = tag_to_idx[labels1[q]]\n",
    "\n",
    "                        epoch_loss += -torch.log(s[label_idx] + 1e-12).item()\n",
    "\n",
    "                        # compute accuracy\n",
    "                        if torch.argmax(s).item() == label_idx:\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "\n",
    "                        s[label_idx] = s[label_idx] - 1.0\n",
    "                        errors.append(s)\n",
    "\n",
    "                    # pass the dict of concatenated inputs (as original code does)\n",
    "                    self.backward(errors, self.concat_inputs)\n",
    "\n",
    "                    # update progress bar (1 example done)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # at end of epoch, print avg loss\n",
    "            avg_loss = epoch_loss / len(original_inputs)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # final accuracy\n",
    "        accuracy = 100.0 * correct / total\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Test\n",
    "    def test(self, original_inputs, labels):\n",
    "        accuracy = 0\n",
    "        to_divide = 0\n",
    "        for inputs, labels1 in zip(original_inputs, labels):\n",
    "            probabilities = self.forward([Encode(input, vocab_size) for input in inputs])\n",
    "\n",
    "            output = ''\n",
    "            to_divide = to_divide + len(labels1)\n",
    "            for q in range(len(labels1)):\n",
    "                probs = softmax(probabilities[q].reshape(-1, 1)).view(-1)  # 1D\n",
    "                pred_idx = torch.argmax(probs).item()   # deterministic prediction\n",
    "                prediction = idx_to_tag[pred_idx]\n",
    "\n",
    "                output += prediction\n",
    "\n",
    "                if prediction == labels1[q]:\n",
    "                    accuracy += 1\n",
    "\n",
    "        print(f'Accuracy: {round(accuracy * 100 / to_divide, 2)}%')\n",
    "\n",
    "\n",
    "def predict_pos(model, sentence):\n",
    "    \"\"\"\n",
    "    Predict POS tags for a single sentence using the custom LSTM model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTM instance\n",
    "        sentence: List of tokens (words)\n",
    "\n",
    "    Returns:\n",
    "        List of (word, predicted_tag) tuples\n",
    "    \"\"\"\n",
    "    model.reset()  # reset LSTM memory\n",
    "    inputs = [Encode(word, vocab_size) for word in sentence]  # encode each word\n",
    "    outputs = model.forward(inputs)\n",
    "\n",
    "    tagged_sentence = []\n",
    "    for word, out in zip(sentence, outputs):\n",
    "        probs = softmax(out).view(-1)           # convert to 1D\n",
    "        pred_idx = torch.argmax(probs).item()   # pick highest probability\n",
    "        pred_tag = idx_to_tag[pred_idx]         # convert index to tag\n",
    "        tagged_sentence.append((word, pred_tag))\n",
    "\n",
    "    return tagged_sentence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Load the model back\n",
    "loaded_model = torch.load(\"our_complete_lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611d695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8dafe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model, Testing started\n",
      "[('The', 'DET'), ('cat', 'NOUN'), ('sat', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('mat', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded Model, Testing started\")\n",
    "# loaded_model.test(X_test, y_test)\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "tagged = predict_pos(loaded_model, sentence)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7fc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
