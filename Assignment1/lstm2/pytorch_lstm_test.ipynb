{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262987bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd47bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 29 18:54:27 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   53C    P0              68W / 300W |  75260MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              75W / 300W |  29938MiB / 81920MiB |     10%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   58C    P0              76W / 300W |  71866MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              42W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb35092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7710e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53fc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from nltk.tag import map_tag\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2f3ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "start_tag = '^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2069d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.tagged_sents(tagset='universal')\n",
    "# data = [(word.lower(),tag) for i in data for word, tag in i]\n",
    "# train_size = int(len(data) * 0.8)\n",
    "# train_data = data[:train_size]\n",
    "# test_data = data[train_size:]\n",
    "\n",
    "# train_data\n",
    "K = 5\n",
    "start = 0\n",
    "end = int(len(data)/K) -1\n",
    "\n",
    "emission_prob_list = []\n",
    "transition_prob_list = []\n",
    "\n",
    "data_fold_wise = []\n",
    "for i in range(K):\n",
    "  data_fold_wise.append(data[start:end])\n",
    "  start += int(len(data)/K)\n",
    "  end += int(len(data)/K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9cc352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VERB', 'CONJ', 'PRON', 'NUM', 'NOUN', 'ADJ', 'X', 'ADP', 'ADV', '.', 'PRT', 'DET'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X',\n",
       " '^']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = set(tag for sent in data for _, tag in sent)\n",
    "sorted_tags = sorted(tags)\n",
    "sorted_tags.append(start_tag)\n",
    "print(tags)\n",
    "sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ccaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_size = len(sorted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e515b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_idx = {c:i for i, c in enumerate(sorted_tags)}\n",
    "idx_to_tag = {i:c for i, c in enumerate(sorted_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596e5fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56057\n"
     ]
    }
   ],
   "source": [
    "words = [word for sent in data for word, tag in sent]\n",
    "words = set(words)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9c87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e4caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {c:i for i, c in enumerate(words)}\n",
    "idx_to_word = {i:c for i, c in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4c792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "sent_by_word = []\n",
    "sent_by_tag = []\n",
    "for sent in data:\n",
    "    s = []\n",
    "    t = []\n",
    "    for word, tag in sent:\n",
    "        s.append(word)\n",
    "        t.append(tag)\n",
    "    sent_by_word.append(s)\n",
    "    sent_by_tag.append(t)\n",
    "print(len(sent_by_word))\n",
    "print(len(sent_by_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e04f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aebc7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33551e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load once (downloads the model the first time)\n",
    "_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85114414",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8403f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = sent_by_word\n",
    "data_y = sent_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10bacb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_X, data_y, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64420c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions #####\n",
    "def Encode(text, vocab_size):\n",
    "    emb = _model.encode(text, normalize_embeddings=True)\n",
    "    if isinstance(emb, np.ndarray):\n",
    "        emb = torch.from_numpy(emb).float()\n",
    "    else:\n",
    "        emb = torch.tensor(emb, dtype=torch.float32)\n",
    "    emb = emb.reshape(-1, 1).to(device)   # ensure column vector and on device\n",
    "    return emb\n",
    "\n",
    "def Embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Returns a dense 384-dimensional embedding.\"\"\"\n",
    "    return _model.encode(text, normalize_embeddings=True)\n",
    "\n",
    "# Xavier Normalized Initialization\n",
    "def initWeights(input_size, output_size):\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6 / (input_size + output_size))\n",
    "\n",
    "##### Activation Functions #####\n",
    "def sigmoid(input, derivative = False):\n",
    "    if derivative:\n",
    "        return input * (1 - input)\n",
    "    \n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 - input ** 2\n",
    "    \n",
    "    return np.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.sum(np.exp(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdb596a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports #####\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "##### Assume you already have these #####\n",
    "# train_X = list of sentences (each sentence = list of tokens)\n",
    "# train_y = list of tags (each tag sequence aligned with train_X)\n",
    "# vocab_size = number of tokens in vocabulary\n",
    "# tag_size = number of possible tags\n",
    "# word_to_idx, tag_to_idx, idx_to_tag dictionaries\n",
    "# Embedding(word) -> returns np.array of shape (embedding_dim,)\n",
    "\n",
    "##### Hyperparameters #####\n",
    "hidden_size = 32       # changed\n",
    "num_epochs = 10\n",
    "learning_rate = 0.05\n",
    "embedding_dim = 384    # your Embedding() outputs 384-dim\n",
    "PAD_IDX = -1           # not really needed since no padding\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##### Model #####\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)          # [1, seq_len, hidden_dim]\n",
    "        logits = self.fc(lstm_out)          # [1, seq_len, output_dim]\n",
    "        return logits\n",
    "\n",
    "##### Weight Initialization #####\n",
    "def init_lstm_weights(lstm):\n",
    "    for name, param in lstm.named_parameters():\n",
    "        if \"weight_ih\" in name:\n",
    "            init.xavier_uniform_(param.data)\n",
    "        elif \"weight_hh\" in name:\n",
    "            init.orthogonal_(param.data)\n",
    "        elif \"bias\" in name:\n",
    "            param.data.fill_(0)\n",
    "\n",
    "model = LSTMTagger(embedding_dim, hidden_size, tag_size).to(device)\n",
    "init_lstm_weights(model.lstm)\n",
    "init.xavier_uniform_(model.fc.weight)\n",
    "\n",
    "##### Loss & Optimizer #####\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(model, train_X, train_y, num_epochs=10, learning_rate=0.05, tag_size=None, device=None):\n",
    "    \"\"\"\n",
    "    Train an LSTM tagger with stochastic gradient descent (sentence-by-sentence).\n",
    "    \n",
    "    Args:\n",
    "        model: LSTMTagger instance\n",
    "        train_X: list of sentences (list of tokens)\n",
    "        train_y: list of tag sequences aligned with train_X\n",
    "        num_epochs: number of training epochs\n",
    "        learning_rate: learning rate for SGD\n",
    "        tag_size: number of possible tags\n",
    "        device: torch device (cpu or cuda)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in tqdm(range(len(train_X)), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            sent = train_X[i]\n",
    "            tags = train_y[i]\n",
    "\n",
    "            # Convert sentence and tags to tensors\n",
    "            X_tensor = torch.tensor(np.array([Embedding(w) for w in sent]), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            y_tensor = torch.tensor([tag_to_idx[t] for t in tags], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_tensor)  # [1, seq_len, tag_size]\n",
    "            loss = criterion(outputs.view(-1, tag_size), y_tensor.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_X)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "##### Testing #####\n",
    "def test(model, X, y):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []  # store predictions sentence-wise\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sent, tags in zip(X, y):\n",
    "            X_tensor = torch.tensor(\n",
    "                np.array([Embedding(w) for w in sent]),\n",
    "                dtype=torch.float32\n",
    "            ).unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(X_tensor)  # [1, seq_len, tag_size]\n",
    "            preds = torch.argmax(outputs, dim=-1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Convert indices to tags\n",
    "            pred_tags = [idx_to_tag[p] for p in preds]\n",
    "            all_preds.append(pred_tags)\n",
    "            \n",
    "            for p_tag, t in zip(pred_tags, tags):\n",
    "                if p_tag == t:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return all_preds\n",
    "\n",
    "# train(model, X_train, y_train, num_epochs=5, learning_rate=0.05, tag_size=tag_size, device=device)\n",
    "# test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4abdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ##### Option 1: Load full model #####\n",
    "# loaded_model = torch.load(\"pytorch_lstm_model_state.pth\")\n",
    "# loaded_model.to(device)\n",
    "# # Use test function separately\n",
    "# test(loaded_model, X_test[:10], y_test[:10])\n",
    "\n",
    "##### Option 2: Load state dict (recommended) #####\n",
    "# Recreate model instance first\n",
    "loaded_model2 = LSTMTagger(embedding_dim, hidden_size, tag_size).to(device)\n",
    "loaded_model2.load_state_dict(torch.load(\"complete_pytorch_lstm_model_state.pth\"))\n",
    "loaded_model2.to(device)\n",
    "\n",
    "\n",
    "y_pred = test(loaded_model2, X_test[:5], y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895e19be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ADJ', 'NOUN', 'NOUN'],\n",
       " ['CONJ', 'PRON', 'VERB', 'PRON', 'VERB', 'NOUN', 'NOUN', '.'],\n",
       " ['NOUN',\n",
       "  'VERB',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.'],\n",
       " ['DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'PRON',\n",
       "  'PRT',\n",
       "  'VERB',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'ADV',\n",
       "  'ADJ',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'VERB',\n",
       "  '.'],\n",
       " ['DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'VERB',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  '.']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13146b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
