{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DDghhjha-xUK"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ADfNyi1sB9L3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1760775897.498251] [bfd74565809f:7718 :f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "299AqvvaEdkf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# path = '/kaggle/input/marathi/mar_train.csv'\n",
    "# path_val = '/kaggle/input/marathi/mar_valid.csv'\n",
    "# path_test = '/kaggle/input/marathi/mar_test.csv'\n",
    "\n",
    "lang = 'hin'\n",
    "path = f\"../../aks_dataset/{lang}/train.csv\"\n",
    "path_val = f\"../../aks_dataset/{lang}/valid.csv\"\n",
    "path_test = f\"../../aks_dataset/{lang}/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtpQ9UGxFhkU",
    "outputId": "f1e70afc-85a2-4816-94cd-00e207f0d9ac"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path , header = None)\n",
    "df_val = pd.read_csv(path_val , header = None)\n",
    "df_test = pd.read_csv(path_test , header = None)\n",
    "english_words_val = df_val[0]\n",
    "marathi_words_val = df_val[1]\n",
    "english_words_test = df_test[0]\n",
    "marathi_words_test = df_test[1]\n",
    "english_words = df[0]\n",
    "marathi_words = df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beemon'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(english_words_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtlfObKubBNq",
    "outputId": "6f0701f1-4823-47fc-b128-56c9446fd71a"
   },
   "outputs": [],
   "source": [
    "# creating list of charecters in both languages\n",
    "\n",
    "english_char_list = []\n",
    "max_length_word_english = -1\n",
    "for word in english_words:\n",
    "  max_length_word_english = max(max_length_word_english,len(word)) \n",
    "  for char in word :\n",
    "    english_char_list.append(char);\n",
    "english_char_list = list(set(english_char_list))\n",
    "english_char_list.sort()\n",
    "\n",
    "marathi_char_list = []\n",
    "max_length_word_marathi = -1\n",
    "for word in marathi_words:\n",
    "  max_length_word_marathi = max(max_length_word_marathi,len(word))\n",
    "  for char in word :\n",
    "    marathi_char_list.append(char);\n",
    "marathi_char_list = list(set(marathi_char_list))\n",
    "marathi_char_list.sort()\n",
    "\n",
    "\n",
    "# finding out the maximum size word for english and marathi from validation and test data.\n",
    "for word in english_words_val:\n",
    "  max_length_word_english = max(max_length_word_english,len(word))\n",
    "for word in english_words_test:\n",
    "  max_length_word_english = max(max_length_word_english,len(word)) \n",
    "for word in marathi_words_val:\n",
    "  max_length_word_marathi = max(max_length_word_marathi,len(word))\n",
    "for word in marathi_words_test:\n",
    "  max_length_word_marathi = max(max_length_word_marathi,len(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(max_length_word_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(max_length_word_marathi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fdcqBaU70W3v"
   },
   "outputs": [],
   "source": [
    "# english word to vector size = 27 ie. max_length_word_english\n",
    "# marathi word to vector size = 20 ie. max_length_word_marathi\n",
    "# for one word.\n",
    "def word2vec(word, lang):\n",
    "  vec = []\n",
    "  if(lang == \"english\"):\n",
    "    vec.append(len(english_char_list) + 1)\n",
    "    for char in word:\n",
    "      for i in range(len(english_char_list)):\n",
    "        if(english_char_list[i] == char):\n",
    "          vec.append(i+1)\n",
    "    while(len(vec) < max_length_word_english + 1): # padding with max_length + 1.\n",
    "        vec.append(0)\n",
    "    vec.append(0)\n",
    "  else :\n",
    "    vec.append(len(marathi_char_list) + 1)\n",
    "    for char in word:\n",
    "      for i in range(len(marathi_char_list)):\n",
    "        if( marathi_char_list[i] == char):\n",
    "          vec.append(i+1)\n",
    "    while(len(vec) < max_length_word_marathi + 1):  # padding with max_length + 1.\n",
    "        vec.append(0)\n",
    "    vec.append(0)\n",
    "  return(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heu_03Y3332P",
    "outputId": "bd542307-bcad-4812-9711-0bf468c89a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ज़ोला\n",
      "[65, 23, 50, 62, 43, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec(marathi_words[10],\"marathi\")\n",
    "print(marathi_words[10])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pRObgwk5pcEz"
   },
   "outputs": [],
   "source": [
    "# creating matrix of representation for whole words of english and marathi.\n",
    "\n",
    "def ip_matrix_construct(words, lang):\n",
    "  ans = []\n",
    "  for word in words:\n",
    "    ans.append(word2vec(word, lang))\n",
    "  return(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FgS76C6rluA",
    "outputId": "ef164c52-28ad-470c-f430-b4ab0b25c91f"
   },
   "outputs": [],
   "source": [
    "# calculated representations of whole english and marathi words in variables english and marathi matrix.\n",
    "english_matrix = ip_matrix_construct(english_words, \"english\")\n",
    "marathi_matrix = ip_matrix_construct(marathi_words, \"marathi\")\n",
    "english_matrix = torch.tensor(english_matrix)\n",
    "marathi_matrix = torch.tensor(marathi_matrix)\n",
    "\n",
    "english_matrix_val = ip_matrix_construct(english_words_val, \"english\")\n",
    "marathi_matrix_val = ip_matrix_construct(marathi_words_val, \"marathi\")\n",
    "english_matrix_val = torch.tensor(english_matrix_val)\n",
    "marathi_matrix_val = torch.tensor(marathi_matrix_val)\n",
    "english_matrix_test = ip_matrix_construct(english_words_test, \"english\")\n",
    "marathi_matrix_test =ip_matrix_construct(marathi_words_test, \"marathi\")\n",
    "english_matrix_test = torch.tensor(english_matrix_test)\n",
    "marathi_matrix_test = torch.tensor(marathi_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cvjQGsLrsBJZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_size, embedding_size, hidden_size, enc_layers, p, cell_type, bidirectional):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.enc_layers = enc_layers\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.cell_type = cell_type\n",
    "    self.bidirectional = bidirectional\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    if(cell_type == \"GRU\"):\n",
    "      self.gru = nn.GRU(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
    "    if(cell_type == \"RNN\"):\n",
    "      self.rnn = nn.RNN(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
    "    if(cell_type == \"LSTM\"):\n",
    "      self.lstm = nn.LSTM(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    if(self.cell_type == \"GRU\"):\n",
    "      output, hidden = self.gru(embedding)\n",
    "    if(self.cell_type == \"RNN\"):\n",
    "      output, hidden = self.rnn(embedding)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "      outputs, (hidden,cell) = self.lstm(embedding)\n",
    "      return outputs, hidden, cell\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nea9Nz5E-xUP"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.dec_layers = dec_layers\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.cell_type = cell_type\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    if(cell_type == \"GRU\"):\n",
    "      self.gru = nn.GRU(embedding_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"RNN\"):\n",
    "      self.rnn = nn.RNN(embedding_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"LSTM\"):\n",
    "      self.lstm = nn.LSTM(embedding_size, hidden_size, dec_layers, dropout = p)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n",
    "  \n",
    "  def forward(self,x,output, hidden, cell = 0):\n",
    "    x = x.unsqueeze(0).int()\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    if(self.cell_type == \"GRU\"):\n",
    "        outputs, hidden = self.gru(embedding, hidden)\n",
    "    if(self.cell_type == \"RNN\"):\n",
    "        outputs, hidden = self.rnn(embedding, hidden)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "    # shape of outputs: (1, N, hidden_size)\n",
    "    predictions = self.fc(outputs)\n",
    "    # shape of predictions: (1, N, length_of_vocab)\n",
    "    predictions = predictions.squeeze(0)\n",
    "    # shape of predictions: (N, length_of_vocab)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        return predictions, hidden, cell\n",
    "    return predictions, hidden\n",
    "\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pmqgOzhw0t37"
   },
   "outputs": [],
   "source": [
    "class Atten_decoder(nn.Module):\n",
    "  def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type, bidirectional):\n",
    "    super(Atten_decoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.max_length = len(english_matrix[0])  \n",
    "    self.dec_layers = dec_layers\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.cell_type = cell_type\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    if(cell_type == \"GRU\"):\n",
    "      self.gru = nn.GRU(hidden_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"RNN\"):\n",
    "      self.rnn = nn.RNN(hidden_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"LSTM\"):\n",
    "      self.lstm = nn.LSTM(hidden_size, hidden_size, dec_layers, dropout = p)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n",
    "    self.attn = nn.Linear(hidden_size+embedding_size, self.max_length)\n",
    "    if(bidirectional):\n",
    "      self.attn_combine = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n",
    "    else :\n",
    "      self.attn_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "\n",
    "  def forward(self, x,output, hidden, cell = 0):\n",
    "    x = x.unsqueeze(0)\n",
    "    output=output.permute(1,0,2)\n",
    "    embedded = self.embedding(x)\n",
    "    embedded = self.dropout(embedded)\n",
    "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0],hidden[0]), 1)), dim = 1)\n",
    "    attn_applied = torch.bmm(attn_weights.unsqueeze(1),output)\n",
    "    attn_applied = attn_applied.squeeze(1)\n",
    "    op = torch.cat((embedded[0], attn_applied), 1)\n",
    "\n",
    "    op = self.attn_combine(op).unsqueeze(0)\n",
    "    op = F.relu(op)\n",
    "    if(self.cell_type == \"GRU\"):\n",
    "        outputs, hidden = self.gru(op, hidden)\n",
    "    if(self.cell_type == \"RNN\"):\n",
    "        outputs, hidden = self.rnn(op, hidden)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        outputs, (hidden, cell) = self.lstm(op, (hidden, cell))\n",
    "    predictions = self.fc(outputs)\n",
    "    predictions = predictions.squeeze(0)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        return predictions, hidden, cell\n",
    "    return predictions, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZkWSEAca-Okj"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, cell_type, bidirectional, enc_layers, dec_layers):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(marathi_char_list) + 2  \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        if(self.cell_type == \"LSTM\"):\n",
    "            encoder_output, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            encoder_output, hidden = self.encoder(source)\n",
    "        if(self.enc_layers != self.dec_layers or self.bidirectional == True):\n",
    "          hidden = hidden[self.enc_layers - 1] + hidden[self.enc_layers - 1]\n",
    "          hidden = hidden.repeat(self.dec_layers,1,1)\n",
    "          if(self.cell_type == \"LSTM\"):\n",
    "              cell = cell[self.enc_layers - 1] + cell[self.enc_layers - 1]\n",
    "              cell = cell.repeat(self.dec_layers,1,1)\n",
    "        \n",
    "        x = target[0]\n",
    "    \n",
    "        for t in range(1, target_len):\n",
    "            if(self.cell_type == \"LSTM\"):\n",
    "                output, hidden, cell = self.decoder(x, encoder_output, hidden, cell)\n",
    "            else :\n",
    "                output, hidden = self.decoder(x, encoder_output, hidden)\n",
    "            outputs[t] = output\n",
    "\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SYajaq47-xUQ"
   },
   "outputs": [],
   "source": [
    "def Accuracy(model, english_matrix, marathi_matrix, epoch, batch_size):\n",
    "    correct_count = 0\n",
    "    for batch_idx in range((int)(len(english_matrix) / batch_size)):\n",
    "        inp_data = english_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        target = marathi_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        output = model.forward(inp_data.T, target.T, 0)\n",
    "        output = nn.Softmax(dim=2)(output)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.T\n",
    "        for i in range(batch_size):\n",
    "            if(torch.equal(output[i][1:],target[i][1:])):\n",
    "                correct_count += 1\n",
    "    accuracy = correct_count * 100 / len(english_matrix)\n",
    "    return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vg6VQCWWzhoy"
   },
   "outputs": [],
   "source": [
    "# creating list of expected marathi word and predicted marathi word.\n",
    "predictions_vanilla_train = []\n",
    "predictions_vanilla_val = []\n",
    "predictions_vanilla_test = []\n",
    "def matrix_to_words(model, english_matrix, marathi_matrix, batch_size, data_type):\n",
    "  for batch_idx in range((int)(len(english_matrix) / batch_size) + 1):\n",
    "        inp_data = english_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        target = marathi_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        output = model.forward(inp_data.T, target.T, 0)\n",
    "        output = nn.Softmax(dim=2)(output)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.T\n",
    "        for i in range(len(target)):\n",
    "          target_word = target[i]\n",
    "          output_word = output[i]\n",
    "          word1 = \"\"\n",
    "          word2 = \"\"\n",
    "          for j in range(len(target_word)):\n",
    "            if(target_word[j]>0 and target_word[j]<64):\n",
    "              word1 += marathi_char_list[target_word[j] - 1]\n",
    "          for j in range(len(output_word)):\n",
    "            if(output_word[j]>0 and output_word[j]<64):\n",
    "              word2 += marathi_char_list[output_word[j] - 1]\n",
    "          temp = [word1, word2]\n",
    "          if(data_type == \"train\"):\n",
    "            predictions_vanilla_train.append(temp)\n",
    "          if(data_type == \"validation\"):\n",
    "            predictions_vanilla_val.append(temp)\n",
    "          if(data_type == \"test\"):\n",
    "            predictions_vanilla_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9QCnSmaTMRSR"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def neural_network(cell_type, bidirectional, enc_layers, dec_layers, batch_size, embedding_size, hidden_size, enc_dropout, dec_dropout, attention):\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 20\n",
    "    input_size_encoder = len(english_char_list) + 2  \n",
    "    input_size_decoder = len(marathi_char_list) + 2  \n",
    "    output_size        = len(marathi_char_list) + 2  \n",
    "\n",
    "    encoder_net = Encoder(input_size_encoder, embedding_size, hidden_size, enc_layers, enc_dropout, cell_type,bidirectional).to(device)\n",
    "    if(attention):\n",
    "        decoder_net = Atten_decoder(input_size_decoder,embedding_size,hidden_size,output_size,dec_layers,dec_dropout, cell_type, bidirectional).to(device)\n",
    "    else:\n",
    "        decoder_net = Decoder(input_size_decoder,embedding_size,hidden_size,output_size,dec_layers,dec_dropout, cell_type).to(device)\n",
    "\n",
    "    model = Seq2Seq(encoder_net, decoder_net, cell_type, bidirectional, enc_layers, dec_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    pad_idx = len(marathi_char_list) + 1  # 64 # pading index for marathi\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(\"epoch = \",epoch)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        step = 0\n",
    "        for batch_idx in range((int)(len(english_matrix) / batch_size)):\n",
    "            inp_data = english_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "            target = marathi_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "            target = target.T\n",
    "            output = model(inp_data.T, target)\n",
    "\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "        print(\"total loss = \",total_loss / step)\n",
    "        training_accuracy = Accuracy(model, english_matrix, marathi_matrix, epoch, batch_size)\n",
    "        print(\"Training Accuracy = \", training_accuracy)\n",
    "        val_accuracy = Accuracy(model, english_matrix_val, marathi_matrix_val, epoch, batch_size)\n",
    "        print(\"Validation accuracy = \",val_accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkkdB4_mxLWV",
    "outputId": "b72ad08d-3643-4cb1-c283-cb55a2ad4700"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "total loss =  tensor(0.9301, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  2.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                               | 1/20 [00:59<18:46, 59.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  5.50574170206072\n",
      "epoch =  1\n",
      "total loss =  tensor(0.3085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  29.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▍                                                                           | 2/20 [01:59<17:56, 59.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  31.949032562529496\n",
      "epoch =  2\n",
      "total loss =  tensor(0.2066, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  37.367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▌                                                                       | 3/20 [02:58<16:52, 59.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  36.84127733207488\n",
      "epoch =  3\n",
      "total loss =  tensor(0.1707, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  43.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 4/20 [03:58<15:56, 59.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  39.98741544753815\n",
      "epoch =  4\n",
      "total loss =  tensor(0.1541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  46.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 5/20 [04:58<14:53, 59.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  40.50652823658959\n",
      "epoch =  5\n",
      "total loss =  tensor(0.1396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  50.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▏                                                          | 6/20 [05:57<13:52, 59.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.45713386817681\n",
      "epoch =  6\n",
      "total loss =  tensor(0.1281, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  53.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████▍                                                      | 7/20 [06:56<12:53, 59.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.00094384143464\n",
      "epoch =  7\n",
      "total loss =  tensor(0.1153, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  56.256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 8/20 [07:56<11:52, 59.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.630171464527294\n",
      "epoch =  8\n",
      "total loss =  tensor(0.1068, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  58.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████▊                                              | 9/20 [08:54<10:50, 59.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.64590215510461\n",
      "epoch =  9\n",
      "total loss =  tensor(0.0976, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  59.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 10/20 [09:53<09:49, 58.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.063866603743904\n",
      "epoch =  10\n",
      "total loss =  tensor(0.0880, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  62.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████▋                                     | 11/20 [10:54<08:57, 59.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.52005663048608\n",
      "epoch =  11\n",
      "total loss =  tensor(0.0804, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  64.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 12/20 [11:55<08:00, 60.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.94478527607362\n",
      "epoch =  12\n",
      "total loss =  tensor(0.0760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  64.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████▉                             | 13/20 [12:56<07:01, 60.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  43.039169419537515\n",
      "epoch =  13\n",
      "total loss =  tensor(0.0699, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  66.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 14/20 [13:56<06:01, 60.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.18971212836244\n",
      "epoch =  14\n",
      "total loss =  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  68.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████▎                    | 15/20 [14:56<05:00, 60.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.36274972471291\n",
      "epoch =  15\n",
      "total loss =  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  71.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 16/20 [15:57<04:01, 60.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.29982696240365\n",
      "epoch =  16\n",
      "total loss =  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  72.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████████▌            | 17/20 [16:58<03:01, 60.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  41.84363693566148\n",
      "epoch =  17\n",
      "total loss =  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  73.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 18/20 [17:56<01:59, 59.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.29982696240365\n",
      "epoch =  18\n",
      "total loss =  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  74.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████▊    | 19/20 [18:56<00:59, 59.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.69309422683656\n",
      "epoch =  19\n",
      "total loss =  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training Accuracy =  76.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [19:56<00:00, 59.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy =  42.86613182318704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cell_type = \"LSTM\"\n",
    "bidirectional = False\n",
    "enc_layers = 2\n",
    "dec_layers = 2\n",
    "batch_size = 256\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "enc_dropout = 0\n",
    "dec_dropout = 0\n",
    "attention = True\n",
    "\n",
    "\n",
    "model = neural_network(cell_type, bidirectional, enc_layers, dec_layers, batch_size, embedding_size, hidden_size, enc_dropout, dec_dropout, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(marathi_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bW22hUA2vTpV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/model_LSTM_uni_attn.pt\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"models/model_{cell_type}_{'bi' if bidirectional else 'uni'}_{'attn' if attention else 'noattn'}.pt\"\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bW22hUA2vTpV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, model_name)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BMyJeinM-xUS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(28, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2)\n",
       "  )\n",
       "  (decoder): Atten_decoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(66, 256)\n",
       "    (lstm): LSTM(512, 512, num_layers=2)\n",
       "    (fc): Linear(in_features=512, out_features=66, bias=True)\n",
       "    (attn): Linear(in_features=768, out_features=31, bias=True)\n",
       "    (attn_combine): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(model_name)\n",
    "model.eval()  # Important: set to eval mode before testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_to_words(model, english_matrix_test, marathi_matrix_test, batch_size, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file \"predictions/updated/uni_attn.csv\" has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# --- define data ---\n",
    "filename_test = 'predictions/updated/uni_attn.csv'\n",
    "\n",
    "# english_words: list of English strings\n",
    "# predictions_vanilla_test: list of [actual_word, predicted_word] pairs\n",
    "\n",
    "\n",
    "\n",
    "# Combine into rows: [English, Actual, Predicted]\n",
    "rows = []\n",
    "for eng, (actual, predicted) in zip(english_words_test, predictions_vanilla_test):\n",
    "    rows.append([eng, actual, predicted])\n",
    "\n",
    "# --- write to CSV ---\n",
    "with open(filename_test, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['eng', 'actual', 'predicted'])  # header\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f'✅ CSV file \\\"{filename_test}\\\" has been created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIS3GbEF5dsP",
    "outputId": "2f4b1594-3f7a-4502-f7e5-994c753b7a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"predictions_vanilla_test_attention.csv\" has been created.\n"
     ]
    }
   ],
   "source": [
    "# creating csv file of predictions\n",
    "filename_test = 'predictions_vanilla_test_attention.csv'\n",
    "\n",
    "with open(filename_test, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_NONE)\n",
    "    writer.writerows(predictions_vanilla_test)\n",
    "\n",
    "print(f'CSV file \"{filename_test}\" has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_accuracies(csv_path):\n",
    "    # Read CSV (no headers)\n",
    "    df = pd.read_csv(csv_path, header=None, sep=',', names=['actual', 'predicted'])\n",
    "    \n",
    "    # Drop any empty rows\n",
    "    df = df.dropna()\n",
    "    \n",
    "    total_words = len(df)\n",
    "    correct_words = 0\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        actual = str(row['actual']).strip()\n",
    "        predicted = str(row['predicted']).strip()\n",
    "\n",
    "        # --- Word-level accuracy ---\n",
    "        if actual == predicted:\n",
    "            correct_words += 1\n",
    "\n",
    "        # --- Character-level accuracy ---\n",
    "        max_len = max(len(actual), len(predicted))\n",
    "        total_chars += max_len\n",
    "        for a, b in zip(actual, predicted):\n",
    "            if a == b:\n",
    "                correct_chars += 1\n",
    "\n",
    "    word_acc = correct_words / total_words * 100 if total_words > 0 else 0\n",
    "    char_acc = correct_chars / total_chars * 100 if total_chars > 0 else 0\n",
    "\n",
    "    print(f\"✅ Word-level accuracy: {word_acc:.2f}%\")\n",
    "    print(f\"✅ Character-level accuracy: {char_acc:.2f}%\")\n",
    "\n",
    "    return word_acc, char_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "67svBk48GAaX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word-level accuracy: 43.09%\n",
      "✅ Character-level accuracy: 73.86%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43.08894230769231, 73.85511945914529)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"predictions_vanilla_test_attention.csv\"\n",
    "calculate_accuracies(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- utility helpers (tweak names / sizes to match your project) ---\n",
    "def decode_indices_to_marathi(idx_tensor, marathi_char_list):\n",
    "    \"\"\"idx_tensor: 1-D cpu tensor of ints (values are token indices as in your data).\n",
    "       marathi_char_list: list where index 0 -> first char.\n",
    "       Accepts indices where 1 maps to marathi_char_list[0].\"\"\"\n",
    "    s = []\n",
    "    max_valid = len(marathi_char_list)\n",
    "    for v in idx_tensor.tolist():\n",
    "        if 0 < v <= max_valid:     # <= not < — use exact length check\n",
    "            s.append(marathi_char_list[v - 1])\n",
    "    return \"\".join(s)\n",
    "\n",
    "def safe_max(tensor):\n",
    "    return int(torch.max(tensor).item()) if tensor.numel() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LyTr7QZKMqQG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model_name = 'models/model_LSTM_uni_attn.pt'\n",
    "# # === 1. Load saved model ===\n",
    "# model = torch.load(model_name, map_location=device)\n",
    "# model.eval()\n",
    "# print(\"✅ Model loaded successfully\")\n",
    "\n",
    "# === 2. Helper function to convert English word to tensor ===\n",
    "def encode_word(word, lang_dict, max_len):\n",
    "    tensor = torch.zeros(max_len, dtype=torch.long)\n",
    "    for i, ch in enumerate(word):\n",
    "        if ch in lang_dict:\n",
    "            tensor[i] = lang_dict[ch] + 1\n",
    "        else:\n",
    "            tensor[i] = 0  # unknown char or padding\n",
    "    return tensor.unsqueeze(1).to(device)  # shape: (seq_len, 1)\n",
    "\n",
    "# === 3. Helper function to decode tensor back to Marathi string ===\n",
    "def decode_tensor(tensor, marathi_char_list):\n",
    "    word = \"\"\n",
    "    for idx in tensor:\n",
    "        if 0 < idx < len(marathi_char_list) + 1:\n",
    "            word += marathi_char_list[idx - 1]\n",
    "    return word\n",
    "\n",
    "def predict_word(model, english_word, word2vec, max_length_word_marathi, marathi_char_list, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eng_vec = word2vec(english_word, \"english\")\n",
    "        inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "        # dummy target used only for shape/compatibility; length should be reasonably long\n",
    "        target_tensor = torch.zeros(max_length_word_marathi, 1, dtype=torch.long).to(device)\n",
    "\n",
    "        # reset hidden if applicable\n",
    "        if hasattr(model, \"reset_hidden\"):\n",
    "            model.reset_hidden(batch_size=1)\n",
    "\n",
    "        logits = model.forward(inp_tensor, target_tensor, 0)  # assume (seq_len, batch, vocab)\n",
    "        probs = nn.functional.softmax(logits, dim=2)\n",
    "        pred = torch.argmax(probs, dim=2).squeeze(1).cpu()   # (seq_len,)\n",
    "        predicted_word = decode_indices_to_marathi(pred, marathi_char_list)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1reXcQN1-xUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: beemon\n",
      "Predicted Marathi: ईमों\n"
     ]
    }
   ],
   "source": [
    "# === 5. Example usage ===\n",
    "english_word = \"beemon\"  # any test English transliteration\n",
    "predicted_marathi = predict_word(\n",
    "    model, \n",
    "    english_word, word2vec, max_length_word_marathi, marathi_char_list, device\n",
    ")\n",
    "print(f\"English: {english_word}\")\n",
    "print(f\"Predicted Marathi: {predicted_marathi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BMyJeinM-xUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_vec (first 30): [27, 2, 5, 5, 13, 15, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "logits.shape: torch.Size([26, 1, 66])\n",
      "raw preds indices (first 20): [0, 7, 40, 62, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 1) Check encoder input indices for 'beemon'\n",
    "eng_vec = word2vec(\"beemon\", \"english\")\n",
    "print(\"eng_vec (first 30):\", eng_vec[:30])    # are chars mapped? first slots should be non-zero for 'b','e','e','m','o','n'\n",
    "\n",
    "# 2) Check model output raw indices (before decoding)\n",
    "with torch.no_grad():\n",
    "    inp = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)   # (seq_len,1)\n",
    "    target = torch.zeros(max_length_word_marathi, 1, dtype=torch.long).to(device)\n",
    "    logits = model.forward(inp, target, 0)    # shape assumed (seq_len, batch, vocab)\n",
    "    print(\"logits.shape:\", logits.shape)\n",
    "    preds = torch.argmax(torch.softmax(logits, dim=2), dim=2).squeeze(1).cpu().tolist()\n",
    "    print(\"raw preds indices (first 20):\", preds[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common first-token values (token_id: count):\n",
      "[(65, 100000)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# marathi_matrix: tensor shape (N, seq_len)\n",
    "first_tokens = marathi_matrix[:, 0].cpu().numpy()\n",
    "counts = Counter(first_tokens)\n",
    "print(\"most common first-token values (token_id: count):\")\n",
    "print(counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "d9sr6TtgPdmn"
   },
   "outputs": [],
   "source": [
    "def predict_with_sos(model, english_word, word2vec, max_length_word_marathi, marathi_char_list, SOS_IDX=65, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eng_vec = word2vec(english_word, \"english\")\n",
    "        inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)   # (seq_len, 1)\n",
    "\n",
    "        target_len = max_length_word_marathi\n",
    "        target_tensor = torch.zeros(target_len, 1, dtype=torch.long).to(device)\n",
    "        target_tensor[0, 0] = SOS_IDX   # <-- important\n",
    "\n",
    "        if hasattr(model, \"reset_hidden\"):\n",
    "            model.reset_hidden(batch_size=1)\n",
    "\n",
    "        logits = model.forward(inp_tensor, target_tensor, 0)    # (seq_len_dec, 1, vocab)\n",
    "        preds = torch.argmax(torch.softmax(logits, dim=2), dim=2).squeeze(1).cpu().tolist()\n",
    "\n",
    "        # robust decoding: skip leading special tokens (PAD or SOS) then decode until EOS if present\n",
    "        PAD_IDX = 0\n",
    "        EOS_IDX = None  # set if you know it\n",
    "        # skip leading PAD/SOS\n",
    "        i = 0\n",
    "        while i < len(preds) and (preds[i] == PAD_IDX or preds[i] == SOS_IDX):\n",
    "            i += 1\n",
    "        decoded = []\n",
    "        for idx in preds[i:]:\n",
    "            if EOS_IDX is not None and idx == EOS_IDX:\n",
    "                break\n",
    "            if 0 < idx <= len(marathi_char_list):\n",
    "                decoded.append(marathi_char_list[idx - 1])\n",
    "        predicted_word = \"\".join(decoded)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'बीमों'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_with_sos(model, 'beemon', word2vec, max_length_word_marathi, marathi_char_list, 65, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from heapq import nlargest\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def beam_search_predict(\n",
    "    model,\n",
    "    english_word: str,\n",
    "    word2vec,\n",
    "    max_length_word_marathi: int,\n",
    "    marathi_char_list: List[str],\n",
    "    SOS_IDX: int,\n",
    "    device: torch.device,\n",
    "    beam_width: int = 4,\n",
    "    PAD_IDX: int = 0,\n",
    "    EOS_IDX: Optional[int] = None,\n",
    "    length_norm_alpha: float = 0.0,   # 0.0 -> no length norm; >0 gives light normalization\n",
    "    return_beams: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Beam-search inference for a single example using model.forward(inp, target, 0)\n",
    "    - model.forward expects (seq_len_enc, batch, ...) for inp and (seq_len_dec, batch) for target\n",
    "    - Works for batch size = 1\n",
    "    - beam_width: number of beams kept\n",
    "    - length_norm_alpha: if >0, use (score / (length**alpha)) scoring for tie-breaking\n",
    "    - EOS_IDX: optional; if provided the beam ends when EOS is generated\n",
    "    Returns: (predicted_word, beams) if return_beams else predicted_word\n",
    "    Beams format: list of tuples (token_list, score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # prepare encoder input\n",
    "        eng_vec = word2vec(english_word, \"english\")\n",
    "        inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len_enc, 1)\n",
    "\n",
    "        # reset model state if applicable\n",
    "        if hasattr(model, \"reset_hidden\"):\n",
    "            try:\n",
    "                model.reset_hidden(batch_size=1)\n",
    "            except TypeError:\n",
    "                model.reset_hidden()\n",
    "\n",
    "        # beam represented as tuple (token_list, logprob_sum, finished_flag)\n",
    "        # token_list includes SOS at position 0\n",
    "        init_beam = ([SOS_IDX], 0.0, False)\n",
    "        beams: List[Tuple[List[int], float, bool]] = [init_beam]\n",
    "\n",
    "        for step in range(1, max_length_word_marathi):\n",
    "            all_candidates: List[Tuple[List[int], float, bool]] = []\n",
    "\n",
    "            # expand each beam\n",
    "            for tokens, logprob, finished in beams:\n",
    "                if finished:\n",
    "                    # carry over finished beams unchanged\n",
    "                    all_candidates.append((tokens, logprob, True))\n",
    "                    continue\n",
    "\n",
    "                # build target tensor for this partial sequence\n",
    "                tgt_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)  # (cur_len, 1)\n",
    "                # forward - we only need logits at last timestep\n",
    "                logits = model.forward(inp_tensor, tgt_tensor, 0)   # (seq_len_dec, batch=1, vocab)\n",
    "                last_logits = logits[-1, 0, :]                      # (vocab,)\n",
    "                log_probs = F.log_softmax(last_logits, dim=0)       # (vocab,)\n",
    "\n",
    "                # pick top-k token candidates for expansion (local pruning)\n",
    "                topk_vals, topk_idx = torch.topk(log_probs, k=min(beam_width, log_probs.size(0)))\n",
    "                topk_vals = topk_vals.cpu().tolist()\n",
    "                topk_idx = topk_idx.cpu().tolist()\n",
    "\n",
    "                for v, idx_token in zip(topk_vals, topk_idx):\n",
    "                    new_tokens = tokens + [int(idx_token)]\n",
    "                    new_logprob = logprob + float(v)\n",
    "                    is_finished = (EOS_IDX is not None and idx_token == EOS_IDX)\n",
    "                    all_candidates.append((new_tokens, new_logprob, is_finished))\n",
    "\n",
    "            # select top beam_width candidates by score (apply optional length norm for ranking)\n",
    "            def score_for_ranking(candidate):\n",
    "                token_list, s, finished_flag = candidate\n",
    "                # length used for normalization: exclude SOS\n",
    "                length = max(1, len(token_list) - 1)\n",
    "                if length_norm_alpha > 0.0:\n",
    "                    return s / (length ** length_norm_alpha)\n",
    "                return s\n",
    "\n",
    "            # keep the top-k candidates\n",
    "            beams = nlargest(beam_width, all_candidates, key=score_for_ranking)\n",
    "\n",
    "            # if all beams finished early, stop\n",
    "            if all(f for (_, _, f) in beams):\n",
    "                break\n",
    "\n",
    "        # choose final best beam: prefer finished beams (if any)\n",
    "        finished_beams = [b for b in beams if b[2]]\n",
    "        if finished_beams:\n",
    "            best_beam = max(finished_beams, key=lambda b: score_for_ranking(b))\n",
    "        else:\n",
    "            # fallback: choose best by raw score (or normalized)\n",
    "            best_beam = max(beams, key=lambda b: score_for_ranking(b))\n",
    "\n",
    "        best_tokens, best_logprob, _ = best_beam\n",
    "\n",
    "        # robust decoding: drop leading PAD/SOS then decode until EOS (if provided)\n",
    "        i = 0\n",
    "        while i < len(best_tokens) and (best_tokens[i] == PAD_IDX or best_tokens[i] == SOS_IDX):\n",
    "            i += 1\n",
    "\n",
    "        decoded_chars = []\n",
    "        for idx in best_tokens[i:]:\n",
    "            if EOS_IDX is not None and idx == EOS_IDX:\n",
    "                break\n",
    "            if 0 < idx <= len(marathi_char_list):\n",
    "                decoded_chars.append(marathi_char_list[idx - 1])\n",
    "\n",
    "        predicted_word = \"\".join(decoded_chars)\n",
    "\n",
    "        if return_beams:\n",
    "            # produce readable beams: (token_list, normalized_score)\n",
    "            readable = []\n",
    "            for tokens, s, finished in beams:\n",
    "                norm_score = s / (max(1, len(tokens) - 1) ** length_norm_alpha) if length_norm_alpha > 0 else s\n",
    "                readable.append((tokens, float(norm_score), finished))\n",
    "            return predicted_word, readable\n",
    "\n",
    "        return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_IDX = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam best prediction: शलाका\n",
      "Top beams (tokens, score, finished):\n",
      "([65, 1, 46, 43, 51, 16, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], -5.110152897524507, False)\n",
      "([65, 0, 46, 43, 51, 16, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], -5.110152897524507, False)\n",
      "([65, 2, 46, 43, 51, 16, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], -5.110152897524507, False)\n",
      "([65, 3, 46, 43, 51, 16, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], -5.110152897524507, False)\n"
     ]
    }
   ],
   "source": [
    "pred, beams = beam_search_predict(\n",
    "    model=model,\n",
    "    english_word=\"shalaka\",\n",
    "    word2vec=word2vec,\n",
    "    max_length_word_marathi=max_length_word_marathi,\n",
    "    marathi_char_list=marathi_char_list,\n",
    "    SOS_IDX=SOS_IDX,            # set this from your training config / Counter output\n",
    "    device=device,\n",
    "    beam_width=4,\n",
    "    EOS_IDX=None,            # set to your EOS index if known, otherwise None\n",
    "    length_norm_alpha=0.0,\n",
    "    return_beams=True\n",
    ")\n",
    "print(\"Beam best prediction:\", pred[1:])\n",
    "print(\"Top beams (tokens, score, finished):\")\n",
    "for b in beams:\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "crRv0WD-Hu6V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: beemon\n",
      "Predicted Marathi: बीमों\n"
     ]
    }
   ],
   "source": [
    "# === 5. Example usage ===\n",
    "english_word = \"beemon\"  # any test English transliteration\n",
    "predicted_marathi = predict_with_sos( \n",
    "    model, english_word, word2vec, max_length_word_marathi, marathi_char_list, 65, device\n",
    ")\n",
    "print(f\"English: {english_word}\")\n",
    "print(f\"Predicted Marathi: {predicted_marathi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyTr7QZKMqQG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1reXcQN1-xUT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrNT-rKM-xUT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "aPoWLZG3-xUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10112/10112 [02:42<00:00, 62.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to bi_lstm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load the test CSV (no headers) ---\n",
    "original_csv = \"../../aks_dataset/hin/test.csv\"\n",
    "df_test = pd.read_csv(original_csv, header=None)\n",
    "\n",
    "# --- Give column names to df_test ---\n",
    "df_test.columns = ['eng', 'actual']  # first column = English, second column = reference Hindi\n",
    "\n",
    "# --- Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Make sure your model is on the device\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# --- Predict function wrapper ---\n",
    "def predict_words_batch(model, words):\n",
    "    \"\"\"Predict multiple words using GPU.\"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for word in tqdm(words):\n",
    "            word = str(word).strip()\n",
    "            pred = predict_with_sos( \n",
    "    model, word, word2vec, max_length_word_marathi, marathi_char_list, 65, device\n",
    ")  # your existing function\n",
    "            predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# --- Get first column (English words) ---\n",
    "first_column = df_test['eng'].tolist()\n",
    "\n",
    "# --- Run predictions ---\n",
    "predicted = predict_words_batch(model, first_column)\n",
    "\n",
    "# --- Save predictions back to DataFrame ---\n",
    "df_test['predicted'] = predicted\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_test.to_csv(\"predictions/greedy/uni_attn.csv\", index=False)\n",
    "print(\"Predictions saved to bi_lstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:37<00:00,  2.51it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (1000) does not match length of index (10112)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m predicted \u001b[38;5;241m=\u001b[39m predict_words_batch(model, first_column)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# --- Save predictions back to DataFrame ---\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m predicted\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# --- Save to CSV ---\u001b[39;00m\n\u001b[1;32m    180\u001b[0m df_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions/beam/uni_attn.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4298\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4504\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4505\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4510\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4512\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4515\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4516\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4517\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4518\u001b[0m     ):\n\u001b[1;32m   4519\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (1000) does not match length of index (10112)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load the test CSV (no headers) ---\n",
    "original_csv = \"../../aks_dataset/hin/test.csv\"\n",
    "df_test = pd.read_csv(original_csv, header=None)\n",
    "\n",
    "# --- Give column names to df_test ---\n",
    "df_test.columns = ['eng', 'actual']  # first column = English, second column = reference Hindi\n",
    "\n",
    "# --- Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Make sure your model is on the device\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from heapq import nlargest\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def beam_search_predict(\n",
    "    model,\n",
    "    english_word: str,\n",
    "    word2vec,\n",
    "    max_length_word_marathi: int,\n",
    "    marathi_char_list: List[str],\n",
    "    SOS_IDX: int,\n",
    "    device: torch.device,\n",
    "    beam_width: int = 4,\n",
    "    PAD_IDX: int = 0,\n",
    "    EOS_IDX: Optional[int] = None,\n",
    "    length_norm_alpha: float = 0.0,   # 0.0 -> no length norm; >0 gives light normalization\n",
    "    return_beams: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Beam-search inference for a single example using model.forward(inp, target, 0)\n",
    "    - model.forward expects (seq_len_enc, batch, ...) for inp and (seq_len_dec, batch) for target\n",
    "    - Works for batch size = 1\n",
    "    - beam_width: number of beams kept\n",
    "    - length_norm_alpha: if >0, use (score / (length**alpha)) scoring for tie-breaking\n",
    "    - EOS_IDX: optional; if provided the beam ends when EOS is generated\n",
    "    Returns: (predicted_word, beams) if return_beams else predicted_word\n",
    "    Beams format: list of tuples (token_list, score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # prepare encoder input\n",
    "        eng_vec = word2vec(english_word, \"english\")\n",
    "        inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len_enc, 1)\n",
    "\n",
    "        # reset model state if applicable\n",
    "        if hasattr(model, \"reset_hidden\"):\n",
    "            try:\n",
    "                model.reset_hidden(batch_size=1)\n",
    "            except TypeError:\n",
    "                model.reset_hidden()\n",
    "\n",
    "        # beam represented as tuple (token_list, logprob_sum, finished_flag)\n",
    "        # token_list includes SOS at position 0\n",
    "        init_beam = ([SOS_IDX], 0.0, False)\n",
    "        beams: List[Tuple[List[int], float, bool]] = [init_beam]\n",
    "\n",
    "        for step in range(1, max_length_word_marathi):\n",
    "            all_candidates: List[Tuple[List[int], float, bool]] = []\n",
    "\n",
    "            # expand each beam\n",
    "            for tokens, logprob, finished in beams:\n",
    "                if finished:\n",
    "                    # carry over finished beams unchanged\n",
    "                    all_candidates.append((tokens, logprob, True))\n",
    "                    continue\n",
    "\n",
    "                # build target tensor for this partial sequence\n",
    "                tgt_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)  # (cur_len, 1)\n",
    "                # forward - we only need logits at last timestep\n",
    "                logits = model.forward(inp_tensor, tgt_tensor, 0)   # (seq_len_dec, batch=1, vocab)\n",
    "                last_logits = logits[-1, 0, :]                      # (vocab,)\n",
    "                log_probs = F.log_softmax(last_logits, dim=0)       # (vocab,)\n",
    "\n",
    "                # pick top-k token candidates for expansion (local pruning)\n",
    "                topk_vals, topk_idx = torch.topk(log_probs, k=min(beam_width, log_probs.size(0)))\n",
    "                topk_vals = topk_vals.cpu().tolist()\n",
    "                topk_idx = topk_idx.cpu().tolist()\n",
    "\n",
    "                for v, idx_token in zip(topk_vals, topk_idx):\n",
    "                    new_tokens = tokens + [int(idx_token)]\n",
    "                    new_logprob = logprob + float(v)\n",
    "                    is_finished = (EOS_IDX is not None and idx_token == EOS_IDX)\n",
    "                    all_candidates.append((new_tokens, new_logprob, is_finished))\n",
    "\n",
    "            # select top beam_width candidates by score (apply optional length norm for ranking)\n",
    "            def score_for_ranking(candidate):\n",
    "                token_list, s, finished_flag = candidate\n",
    "                # length used for normalization: exclude SOS\n",
    "                length = max(1, len(token_list) - 1)\n",
    "                if length_norm_alpha > 0.0:\n",
    "                    return s / (length ** length_norm_alpha)\n",
    "                return s\n",
    "\n",
    "            # keep the top-k candidates\n",
    "            beams = nlargest(beam_width, all_candidates, key=score_for_ranking)\n",
    "\n",
    "            # if all beams finished early, stop\n",
    "            if all(f for (_, _, f) in beams):\n",
    "                break\n",
    "\n",
    "        # choose final best beam: prefer finished beams (if any)\n",
    "        finished_beams = [b for b in beams if b[2]]\n",
    "        if finished_beams:\n",
    "            best_beam = max(finished_beams, key=lambda b: score_for_ranking(b))\n",
    "        else:\n",
    "            # fallback: choose best by raw score (or normalized)\n",
    "            best_beam = max(beams, key=lambda b: score_for_ranking(b))\n",
    "\n",
    "        best_tokens, best_logprob, _ = best_beam\n",
    "\n",
    "        # robust decoding: drop leading PAD/SOS then decode until EOS (if provided)\n",
    "        i = 0\n",
    "        while i < len(best_tokens) and (best_tokens[i] == PAD_IDX or best_tokens[i] == SOS_IDX):\n",
    "            i += 1\n",
    "\n",
    "        decoded_chars = []\n",
    "        for idx in best_tokens[i:]:\n",
    "            if EOS_IDX is not None and idx == EOS_IDX:\n",
    "                break\n",
    "            if 0 < idx <= len(marathi_char_list):\n",
    "                decoded_chars.append(marathi_char_list[idx - 1])\n",
    "\n",
    "        predicted_word = \"\".join(decoded_chars)\n",
    "\n",
    "        if return_beams:\n",
    "            # produce readable beams: (token_list, normalized_score)\n",
    "            readable = []\n",
    "            for tokens, s, finished in beams:\n",
    "                norm_score = s / (max(1, len(tokens) - 1) ** length_norm_alpha) if length_norm_alpha > 0 else s\n",
    "                readable.append((tokens, float(norm_score), finished))\n",
    "            return predicted_word, readable\n",
    "\n",
    "        return predicted_word\n",
    "\n",
    "\n",
    "# --- Predict function wrapper ---\n",
    "def predict_words_batch(model, words):\n",
    "    \"\"\"Predict multiple words using GPU.\"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for word in tqdm(words):\n",
    "            word = str(word).strip()\n",
    "            pred, beams = beam_search_predict(\n",
    "    model=model,\n",
    "    english_word=word,\n",
    "    word2vec=word2vec,\n",
    "    max_length_word_marathi=max_length_word_marathi,\n",
    "    marathi_char_list=marathi_char_list,\n",
    "    SOS_IDX=SOS_IDX,            # set this from your training config / Counter output\n",
    "    device=device,\n",
    "    beam_width=2,\n",
    "    EOS_IDX=None,            # set to your EOS index if known, otherwise None\n",
    "    length_norm_alpha=0.0,\n",
    "    return_beams=True\n",
    ")  # your existing function\n",
    "            predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# --- Get first column (English words) ---\n",
    "first_column = df_test['eng'].tolist()[:1000]\n",
    "\n",
    "# --- Run predictions ---\n",
    "predicted = predict_words_batch(model, first_column)\n",
    "\n",
    "# --- Save predictions back to DataFrame ---\n",
    "df_test['predicted'] = predicted\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_test.to_csv(\"predictions/beam/uni_attn.csv\", index=False)\n",
    "print(\"Predictions saved to bi_lstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to predictions/beam/uni_attn.csv\n",
      "              eng           actual       predicted\n",
      "0    maitrologist    मैट्रोलॉजिस्ट   मेट्रोलोजिस्ट\n",
      "1           phwcs  पीएचडब्ल्यूसीएस        फ्वीएचएस\n",
      "2  pratidwandiyon  प्रतिद्वन्दियों  प्रतिद्वंदियों\n",
      "3      pratiyukti      प्रतियुक्ति     प्रतियुक्ति\n",
      "4      eksisatens       एक्सिसटेंस       एकसीसेटें\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# make sure the folder exists\n",
    "os.makedirs(\"predictions/beam\", exist_ok=True)\n",
    "\n",
    "# --- Build a 1000-sample subset ---\n",
    "new_df = pd.DataFrame()\n",
    "new_df['eng'] = df_test['eng'].tolist()[:1000]\n",
    "new_df['actual'] = df_test['actual'].tolist()[:1000]\n",
    "\n",
    "np = []\n",
    "for p in predicted:\n",
    "    np.append(p[1:])\n",
    "new_df['predicted'] = np[:1000]   # ensure matching length\n",
    "\n",
    "# --- Save to CSV ---\n",
    "out_path = \"predictions/beam/uni_attn.csv\"\n",
    "new_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"✅ Predictions saved to {out_path}\")\n",
    "print(new_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
