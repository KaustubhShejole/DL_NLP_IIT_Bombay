{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd43ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Compute Levenshtein edit distance between s1 and s2 (DP).\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1, start=1):\n",
    "        current_row = [i]\n",
    "        for j, c2 in enumerate(s2, start=1):\n",
    "            insertions = previous_row[j] + 1\n",
    "            deletions = current_row[j - 1] + 1\n",
    "            substitutions = previous_row[j - 1] + (0 if c1 == c2 else 1)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def ngrams(seq, n):\n",
    "    return [tuple(seq[i:i + n]) for i in range(len(seq) - n + 1)] if len(seq) >= n else []\n",
    "\n",
    "def count_clip_matches(reference, hypothesis, n):\n",
    "    ref_ngrams = Counter(ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(ngrams(hypothesis, n))\n",
    "    return sum(min(hyp_ngrams[ng], ref_ngrams.get(ng, 0)) for ng in hyp_ngrams)\n",
    "\n",
    "def sentence_bleu(reference, hypothesis, max_n=4, smoothing=True):\n",
    "    \"\"\"Compute BLEU for a single pair (char-based).\"\"\"\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        matched = count_clip_matches(reference, hypothesis, n)\n",
    "        total = max(1, len(hypothesis) - n + 1)\n",
    "        if smoothing:\n",
    "            p_n = (matched + 1) / (total + 1)\n",
    "        else:\n",
    "            p_n = matched / total\n",
    "        precisions.append(p_n)\n",
    "    if min(precisions) == 0:\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        log_prec_sum = sum((1 / max_n) * math.log(p) for p in precisions)\n",
    "        geo_mean = math.exp(log_prec_sum)\n",
    "    ref_len = len(reference)\n",
    "    hyp_len = len(hypothesis)\n",
    "    if hyp_len == 0:\n",
    "        bp = 0.0\n",
    "    elif hyp_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1 - ref_len / hyp_len)\n",
    "    return bp * geo_mean * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4254dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Compute Levenshtein edit distance between s1 and s2 (DP).\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1, start=1):\n",
    "        current_row = [i]\n",
    "        for j, c2 in enumerate(s2, start=1):\n",
    "            insertions = previous_row[j] + 1\n",
    "            deletions = current_row[j - 1] + 1\n",
    "            substitutions = previous_row[j - 1] + (0 if c1 == c2 else 1)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def ngrams(seq, n):\n",
    "    return [tuple(seq[i:i + n]) for i in range(len(seq) - n + 1)] if len(seq) >= n else []\n",
    "\n",
    "def count_clip_matches(reference, hypothesis, n):\n",
    "    ref_ngrams = Counter(ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(ngrams(hypothesis, n))\n",
    "    return sum(min(hyp_ngrams[ng], ref_ngrams.get(ng, 0)) for ng in hyp_ngrams)\n",
    "\n",
    "def sentence_bleu(reference, hypothesis, max_n=4, smoothing=True):\n",
    "    \"\"\"Compute BLEU for a single pair (char-based).\"\"\"\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        matched = count_clip_matches(reference, hypothesis, n)\n",
    "        total = max(1, len(hypothesis) - n + 1)\n",
    "        if smoothing:\n",
    "            p_n = (matched + 1) / (total + 1)\n",
    "        else:\n",
    "            p_n = matched / total\n",
    "        precisions.append(p_n)\n",
    "    if min(precisions) == 0:\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        log_prec_sum = sum((1 / max_n) * math.log(p) for p in precisions)\n",
    "        geo_mean = math.exp(log_prec_sum)\n",
    "    ref_len = len(reference)\n",
    "    hyp_len = len(hypothesis)\n",
    "    if hyp_len == 0:\n",
    "        bp = 0.0\n",
    "    elif hyp_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1 - ref_len / hyp_len)\n",
    "    return bp * geo_mean * 100.0\n",
    "\n",
    "def calculate_accuracies(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    expected_cols = {'eng', 'actual', 'predicted'}\n",
    "    if not expected_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV must have columns {expected_cols}\")\n",
    "\n",
    "    total_words = len(df)\n",
    "    correct_words = 0\n",
    "    total_chars_for_acc = 0\n",
    "    correct_chars = 0\n",
    "    total_edit_distance = 0\n",
    "    total_ref_chars = 0\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    row_metrics = []  # store per-row metrics\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        english = str(row['eng']).strip()\n",
    "        actual = str(row['actual']).strip()\n",
    "        predicted = str(row['predicted']).strip()\n",
    "\n",
    "        references.append(actual)\n",
    "        hypotheses.append(predicted)\n",
    "\n",
    "        # --- Word-level accuracy ---\n",
    "        word_correct = 1 if actual == predicted else 0\n",
    "        if word_correct:\n",
    "            correct_words += 1\n",
    "\n",
    "        # --- Character accuracy ---\n",
    "        max_len = max(len(actual), len(predicted))\n",
    "        total_chars_for_acc += max_len\n",
    "        char_matches = sum(a == b for a, b in zip(actual, predicted))\n",
    "        correct_chars += char_matches\n",
    "        char_acc = (char_matches / max_len * 100) if max_len > 0 else 0\n",
    "\n",
    "        # --- CER ---\n",
    "        ed = levenshtein_distance(actual, predicted)\n",
    "        total_edit_distance += ed\n",
    "        total_ref_chars += len(actual)\n",
    "        cer = (ed / len(actual) * 100) if len(actual) > 0 else 0\n",
    "\n",
    "        # --- Sentence BLEU ---\n",
    "        bleu = sentence_bleu(actual, predicted, max_n=4, smoothing=True)\n",
    "\n",
    "        row_metrics.append({\n",
    "            'english': english,\n",
    "            'actual': actual,\n",
    "            'predicted': predicted,\n",
    "            'word_match': word_correct * 100,\n",
    "            'char_acc': char_acc,\n",
    "            'cer': cer,\n",
    "            'bleu': bleu\n",
    "        })\n",
    "\n",
    "    # --- Corpus-level metrics ---\n",
    "    word_acc = correct_words / total_words * 100 if total_words > 0 else 0\n",
    "    char_acc = correct_chars / total_chars_for_acc * 100 if total_chars_for_acc > 0 else 0\n",
    "    cer = total_edit_distance / total_ref_chars * 100 if total_ref_chars > 0 else 0\n",
    "    corpus_bleu = sum(m['bleu'] for m in row_metrics) / len(row_metrics)\n",
    "\n",
    "    print(f\"тЬЕ Word-level accuracy:      {word_acc:.2f}%\")\n",
    "    print(f\"тЬЕ Character-level accuracy: {char_acc:.2f}%\")\n",
    "    print(f\"тЬЕ Character Error Rate (CER): {cer:.2f}%\")\n",
    "    print(f\"тЬЕ Avg Sentence BLEU-4 (chars, smoothed): {corpus_bleu:.2f}%\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(row_metrics)\n",
    "\n",
    "    def show_top_and_bottom(metric):\n",
    "        print(f\"\\nЁЯФ╣ Top 10 by {metric}:\")\n",
    "        print(metrics_df.sort_values(metric, ascending=False)[['english', 'actual', 'predicted', metric]].head(10).to_string(index=False))\n",
    "        print(f\"\\nЁЯФ╕ Bottom 10 by {metric}:\")\n",
    "        print(metrics_df.sort_values(metric, ascending=True)[['english','actual', 'predicted', metric]].head(10).to_string(index=False))\n",
    "\n",
    "    for metric in ['word_match', 'char_acc', 'cer', 'bleu']:\n",
    "        show_top_and_bottom(metric)\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59125f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Word-level accuracy:      35.30%\n",
      "тЬЕ Character-level accuracy: 67.41%\n",
      "тЬЕ Character Error Rate (CER): 18.51%\n",
      "тЬЕ Avg Sentence BLEU-4 (chars, smoothed): 69.33%\n",
      "\n",
      "ЁЯФ╣ Top 10 by word_match:\n",
      "           english           actual        predicted  word_match\n",
      "            beemon            рдмреАрдореЛрдВ            рдмреАрдореЛрдВ         100\n",
      "           durniti         рджреБрд░реНрдиреАрддрд┐         рджреБрд░реНрдиреАрддрд┐         100\n",
      "upamukhyamantriyon рдЙрдкрдореБрдЦреНрдпрдордВрддреНрд░рд┐рдпреЛрдВ рдЙрдкрдореБрдЦреНрдпрдордВрддреНрд░рд┐рдпреЛрдВ         100\n",
      "        mukeshvari        рдореБрдХреЗрд╢реНрд╡рд░реА        рдореБрдХреЗрд╢реНрд╡рд░реА         100\n",
      "          tadipaar          рддрдбрд╝реАрдкрд╛рд░          рддрдбрд╝реАрдкрд╛рд░         100\n",
      "            jeenon            рдЬреАрдиреЛрдВ            рдЬреАрдиреЛрдВ         100\n",
      "    vishwasaniyata      рд╡рд┐рд╢реНрд╡рд╕рдиреАрдпрддрд╛      рд╡рд┐рд╢реНрд╡рд╕рдиреАрдпрддрд╛         100\n",
      "         exkleshan       рдПрдХреНрд╕рдХреНрд▓реЗрд╢рди       рдПрдХреНрд╕рдХреНрд▓реЗрд╢рди         100\n",
      "          kachchhe            рдХрдЪреНрдЫреЗ            рдХрдЪреНрдЫреЗ         100\n",
      "              amuk             рдЕрдореБрдХ             рдЕрдореБрдХ         100\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by word_match:\n",
      "     english        actual      predicted  word_match\n",
      "maitrologist рдореИрдЯреНрд░реЛрд▓реЙрдЬрд┐рд╕реНрдЯ рдорд╛рдЗрдЯреНрд░реЛрд▓реЙрдЬрд┐рд╕реНрдЯ           0\n",
      "   whirlpool     рд╡реНрд╣рд░реНрд▓рдкреВрд▓    рд╡реНрд╣реНрдпреВрд▓реНрд▓реВрд▓           0\n",
      "       hasni          рд╣рд╕рдиреА           рд╣рд╛рд╕реА           0\n",
      "  shivamogga       рд╢рд┐рд╡рдореЛрдЧрд╛     рд╢рд┐рд╡рд╛реЛрдЧрдЧреНрдЧрд╛           0\n",
      "      aizawl        рдЕрдЗрдЬрд╝реЛрд▓        рдПрдЬрд╝рд╛рдЬрд╝рд▓           0\n",
      "     behuria       рдмрд╣реБрд░рд┐рдпрд╛       рдмреЗрд╣реБрд░рд┐рдпрд╛           0\n",
      "      kadapa          рдХрдбрдкрд╛          рдХрдбрд╝рдкрд╛           0\n",
      "       essel        рдПрд╕реНрд╕реЗрд▓           рдПрд╕реЗрд▓           0\n",
      "    belagavi       рдмреЗрд▓рдЧрд╛рд╡реА        рдмреЗрд▓рд╛рдЧрд╡реА           0\n",
      "     belinda       рдмреЗрд▓рд┐рдВрдбрд╛        рдмреЗрд▓рд┐рдВрджрд╛           0\n",
      "\n",
      "ЁЯФ╣ Top 10 by char_acc:\n",
      "   english     actual  predicted  char_acc\n",
      "    beemon      рдмреАрдореЛрдВ      рдмреАрдореЛрдВ     100.0\n",
      " cambridge   рдХреИрдВрдмреНрд░рд┐рдЬ   рдХреИрдВрдмреНрд░рд┐рдЬ     100.0\n",
      "  langaron     рд▓рдВрдЧрд░реЛрдВ     рд▓рдВрдЧрд░реЛрдВ     100.0\n",
      "      bodh        рдмреЛрдз        рдмреЛрдз     100.0\n",
      "    radico     рд░реЗрдбрд┐рдХреЛ     рд░реЗрдбрд┐рдХреЛ     100.0\n",
      " equipment рдЗрдХреНрд╡рд┐рдкрдореЗрдВрдЯ рдЗрдХреНрд╡рд┐рдкрдореЗрдВрдЯ     100.0\n",
      "rasaayanik    рд░рд╕рд╛рдпрдирд┐рдХ    рд░рд╕рд╛рдпрдирд┐рдХ     100.0\n",
      "    nashik      рдирд╛рд╢рд┐рдХ      рдирд╛рд╢рд┐рдХ     100.0\n",
      "   siyaram    рд╕рд┐рдпрд╛рд░рд╛рдо    рд╕рд┐рдпрд╛рд░рд╛рдо     100.0\n",
      "   pittman     рдкрд┐рдЯрдореИрди     рдкрд┐рдЯрдореИрди     100.0\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by char_acc:\n",
      "   english  actual predicted  char_acc\n",
      "    nguyen   рдЧреБрдпреЗрди   рдиреНрдЧрдпреВрдПрди       0.0\n",
      "      bhav    рдмрд╣рд╛рд╡       рднрд╛рд╡       0.0\n",
      "   dhundhi   рдвреВрдВрдвреА      рдзрдВрдзреА       0.0\n",
      "       gae      рдЧрдП       рдЬреАрдП       0.0\n",
      "      zand     рдЭрдВрдб     рдЬрд╝рд╛рдВрдж       0.0\n",
      "schaeffler рд╕реНрдХреИрдлрд▓рд░     рд╢реИрдлрд▓рд░       0.0\n",
      "chayaniton рдЪрдпрдирд┐рддреЛрдВ  рдЫрд╛рдпрдирд┐рддреЛрдВ       0.0\n",
      "     datia   рджрддрд┐рдпрд╛    рдбрд╛рдЯрд┐рдпрд╛       0.0\n",
      "     damle   рджрд╛рдорд▓реЗ      рдбрдорд▓реЗ       0.0\n",
      "        to      рддреЛ       рдЯреАрдУ       0.0\n",
      "\n",
      "ЁЯФ╣ Top 10 by cer:\n",
      "english actual  predicted        cer\n",
      "     jh      рдЭ       рдЬреЗрдПрдЪ 400.000000\n",
      "    cup     рдХрдк     рд╕реАрдпреВрдкреА 250.000000\n",
      "    mum     рдордо     рдореБрдпреВрдПрдо 200.000000\n",
      "  ainth    рдРрдВрда     рдПрдЖрдИрдПрдереА 200.000000\n",
      "    usi    рдЙрд╕реА     рдпреВрдПрд╕рдЖрд╛ 166.666667\n",
      "   bass    рдмрд╛рд╕    рдмреАрдПрдПрд╕рдПрд╕ 166.666667\n",
      "     qi    рдХрд╝рд┐     рдХреНрдпреВрдЖрдИ 166.666667\n",
      "   pham    рдлрд╛рдо     рдкреАрдПрдордПрдЪ 166.666667\n",
      "    uda    рдЙрджрд╛      рдпреВрдбреАрдП 166.666667\n",
      "  kping  рдХрдкрд┐рдВрдЧ рдХреЗрдкреАрдЖрдЗрдПрдЬрдЬреА 160.000000\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by cer:\n",
      "   english    actual predicted  cer\n",
      "   sankary     рд╕рдВрдХрд░реА     рд╕рдВрдХрд░реА  0.0\n",
      "      phek       рдлреЗрдХ       рдлреЗрдХ  0.0\n",
      "    kumari    рдХреБрдорд╛рд░реА    рдХреБрдорд╛рд░реА  0.0\n",
      "dharmshala  рдзрд░реНрдорд╢рд╛рд▓рд╛  рдзрд░реНрдорд╢рд╛рд▓рд╛  0.0\n",
      "  deshmukh    рджреЗрд╢рдореБрдЦ    рджреЗрд╢рдореБрдЦ  0.0\n",
      "management рдореИрдиреЗрдЬрдореЗрдВрдЯ рдореИрдиреЗрдЬрдореЗрдВрдЯ  0.0\n",
      "      anup      рдЕрдиреВрдк      рдЕрдиреВрдк  0.0\n",
      "     mehta     рдореЗрд╣рддрд╛     рдореЗрд╣рддрд╛  0.0\n",
      "innovation   рдЗрдиреЛрд╡реЗрд╢рди   рдЗрдиреЛрд╡реЗрд╢рди  0.0\n",
      "  madikeri   рдорджрд┐рдХреЗрд░реА   рдорджрд┐рдХреЗрд░реА  0.0\n",
      "\n",
      "ЁЯФ╣ Top 10 by bleu:\n",
      "   english   actual predicted  bleu\n",
      "    beemon    рдмреАрдореЛрдВ     рдмреАрдореЛрдВ 100.0\n",
      "    karghe     рдХрд░рдШреЗ      рдХрд░рдШреЗ 100.0\n",
      "daramyaani рджрд░рдореНрдпрд╛рдиреА  рджрд░рдореНрдпрд╛рдиреА 100.0\n",
      "  khairati   рдЦреИрд░рд╛рддреА    рдЦреИрд░рд╛рддреА 100.0\n",
      "   jhuthla    рдЭреБрдард▓рд╛     рдЭреБрдард▓рд╛ 100.0\n",
      "  nipatara  рдирд┐рдкрдЯрд╛рд░рд╛   рдирд┐рдкрдЯрд╛рд░рд╛ 100.0\n",
      "  zamaanaa   рдЬрд╝рдорд╛рдирд╛    рдЬрд╝рдорд╛рдирд╛ 100.0\n",
      "   raatiyo   рд░рд╛рддрд┐рдпреЛ    рд░рд╛рддрд┐рдпреЛ 100.0\n",
      "  sevaalaa   рд╕реЗрд╡рд╛рд▓рд╛    рд╕реЗрд╡рд╛рд▓рд╛ 100.0\n",
      "   yuddhak   рдпреБрджреНрдзрдХ    рдпреБрджреНрдзрдХ 100.0\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by bleu:\n",
      "english           actual predicted      bleu\n",
      "    dow       рдбреАрдУрдбрдмреНрд▓реНрдпреВ        рдбрд╛  0.984072\n",
      "    win      рдбрдмреНрд▓реНрдпреВрдЖрдИрдПрди       рд╡рд┐рди  3.139269\n",
      "   qiwi  рдХреНрдпреВрдЖрдИрдбрдмреНрд▓реНрдпреВрдЖрдИ     рдХрд╝рд┐рд╡реА  3.694817\n",
      "  phwcs  рдкреАрдПрдЪрдбрдмреНрд▓реНрдпреВрд╕реАрдПрд╕     рдлрд┐рд╡реНрд╕  4.088987\n",
      "    wef       рдбрдмреНрд▓реНрдпреВрдИрдПрдл       рд╡реЗрдИ  4.381203\n",
      "    mwp      рдПрдордбрдмреНрд▓реНрдпреВрдкреА      рдордореНрд╡  6.909867\n",
      "    rak              рд░рд╛рдХ         рд░  8.047084\n",
      "    dwn      рдбреАрдбрдмреНрд▓реНрдпреВрдПрди      рдбреАрдЖрди  8.830023\n",
      "  qwvga рдХреНрдпреВрдбрдмреНрд▓реНрдпреВрд╡реАрдЬреАрдП  рдХреНрд╡реАрдЧреНрд╡рд╛ 10.218289\n",
      "    pwt      рдкреАрдбрдмреНрд▓реНрдпреВрдЯреА     рдкреНрд╡реНрдЯ 10.339832\n"
     ]
    }
   ],
   "source": [
    "metrics_df = calculate_accuracies('predictions/bi_no_attn_lstm_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f1b7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Word-level accuracy:      17.19%\n",
      "тЬЕ Character-level accuracy: 52.23%\n",
      "тЬЕ Character Error Rate (CER): 26.78%\n",
      "тЬЕ Avg Sentence BLEU-4 (chars, smoothed): 56.97%\n",
      "\n",
      "ЁЯФ╣ Top 10 by word_match:\n",
      "     english     actual  predicted  word_match\n",
      "  baharampur   рдмрд╣рд░рд╛рдордкреБрд░   рдмрд╣рд░рд╛рдордкреБрд░         100\n",
      "    varshiya     рд╡рд░реНрд╖рд┐рдп     рд╡рд░реНрд╖рд┐рдп         100\n",
      "  vaahikayen   рд╡рд╛рд╣рд┐рдХрд╛рдПрдВ   рд╡рд╛рд╣рд┐рдХрд╛рдПрдВ         100\n",
      "paathyaatmak рдкрд╛рдареНрдпрд╛рддреНрдордХ рдкрд╛рдареНрдпрд╛рддреНрдордХ         100\n",
      "     gopalak     рдЧреЛрдкрд╛рд▓рдХ     рдЧреЛрдкрд╛рд▓рдХ         100\n",
      " vishnupuraa рд╡рд┐рд╖реНрдгреБрдкреБрд░рд╛ рд╡рд┐рд╖реНрдгреБрдкреБрд░рд╛         100\n",
      "    bandhuon     рдмрдВрдзреБрдУрдВ     рдмрдВрдзреБрдУрдВ         100\n",
      "niveshkartaa рдирд┐рд╡реЗрд╢рдХрд░реНрддрд╛ рдирд┐рд╡реЗрд╢рдХрд░реНрддрд╛         100\n",
      "  updraviyon рдЙрдкрджреНрд░рд╡рд┐рдпреЛрдВ рдЙрдкрджреНрд░рд╡рд┐рдпреЛрдВ         100\n",
      "      antrit    рдЖрдВрддреНрд░рд┐рдд    рдЖрдВрддреНрд░рд┐рдд         100\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by word_match:\n",
      "     english        actual     predicted  word_match\n",
      "maitrologist рдореИрдЯреНрд░реЛрд▓реЙрдЬрд┐рд╕реНрдЯ рдореИрдЯреНрд░реЛрд▓рд╛рдЬрд┐рд╕реНрдЯ           0\n",
      "       ibiza         рдЗрдмреАрдЬрд╛        рдЗрд┐рдмрдЬрд╝рд╛           0\n",
      "       pulse          рдкрд▓реНрд╕          рдкрд▓рд╕реЗ           0\n",
      "     yudhrat       рдпреБрджреНрдзрд░рдд        рдпреБрдзрд░рд░рдд           0\n",
      "      bahaav          рдмрд╣рд╛рд╡           рд╣рд╛рд╡           0\n",
      "       pyade        рдкреНрдпрд╛рджреЗ         реНрдпрд╛рдбреЗ           0\n",
      " varsaryache       рд╡рд░реНрд╕рд╛рдЪреЗ      рд╡рд░рд╕реНрдпрд╛рдЪреЗ           0\n",
      "       colan          рдХреЛрд▓рди            рдХрд▓           0\n",
      "         daw           рджрд╛рд╡             рдб           0\n",
      "      adhiik          рдЕрдзреАрдХ          рдЕрд┐рдХрд┐           0\n",
      "\n",
      "ЁЯФ╣ Top 10 by char_acc:\n",
      "   english   actual predicted  char_acc\n",
      "baharampur рдмрд╣рд░рд╛рдордкреБрд░  рдмрд╣рд░рд╛рдордкреБрд░     100.0\n",
      "     mitra   рдорд┐рддреНрд░рд╛    рдорд┐рддреНрд░рд╛     100.0\n",
      "    mangla    рдордВрдЧрд▓рд╛     рдордВрдЧрд▓рд╛     100.0\n",
      "   pravesh   рдкреНрд░рд╡реЗрд╢    рдкреНрд░рд╡реЗрд╢     100.0\n",
      "    yamala     рдпрдорд▓рд╛      рдпрдорд▓рд╛     100.0\n",
      "jamshedpur рдЬрдорд╢реЗрджрдкреБрд░  рдЬрдорд╢реЗрджрдкреБрд░     100.0\n",
      " chaurasia  рдЪреМрд░рд╕рд┐рдпрд╛   рдЪреМрд░рд╕рд┐рдпрд╛     100.0\n",
      "   solanki   рд╕реЛрд▓рдВрдХреА    рд╕реЛрд▓рдВрдХреА     100.0\n",
      "    masood     рдорд╕реВрдж      рдорд╕реВрдж     100.0\n",
      "  srinagar  рд╢реНрд░реАрдирдЧрд░   рд╢реНрд░реАрдирдЧрд░     100.0\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by char_acc:\n",
      "  english          actual predicted  char_acc\n",
      "  sankary           рд╕рдВрдХрд░реА     рдВрдХрд░реАрдп       0.0\n",
      "     bean             рдмреАрди      реАрдпрд╛рди       0.0\n",
      "    avika           рдЕрд╡рд┐рдХрд╛      рд╡рд┐рдХрд╛       0.0\n",
      "    myers           рдордпрд░реНрд╕    рдПрдИрдЖрд░рдПрд╕       0.0\n",
      "     qiwi рдХреНрдпреВрдЖрдИрдбрдмреНрд▓реНрдпреВрдЖрдИ      рд╡реАрдЖрдИ       0.0\n",
      "bishadhee          рдмрд┐рд╢рдбрд╝реА      рд┐рд╢рдзреА       0.0\n",
      "     bush             рдмреБрд╢        реВрд╢       0.0\n",
      "    dhaga            рдзрд╛рдЧрд╛        рдврдЧ       0.0\n",
      "     mesa            рдореЗрд╕рд╛       реЗрд╕рд╛       0.0\n",
      " erickson         рдПрд░рд┐рдХреНрд╕рди    рд░рд┐рдХреНрд╕рди       0.0\n",
      "\n",
      "ЁЯФ╣ Top 10 by cer:\n",
      "english actual predicted        cer\n",
      "    cup     рдХрдк    рдХреНрдпреВрдкреА 200.000000\n",
      "    mum     рдордо    рдореНрдпреВрдПрдо 200.000000\n",
      "    ail     рдРрд▓     рдПрдЖрдИрдПрд▓ 200.000000\n",
      "   thki    рдердХреА    рдЯреАрдПрдЪрдЖрдИ 200.000000\n",
      "     ti     рддреА     реАрдЖрдИрдЯреА 200.000000\n",
      "    jha      рдЭ        рдврд╝ 200.000000\n",
      "    usi    рдЙрд╕реА    рдпреВрдПрд╕рдЖрдИ 166.666667\n",
      "    mam     рдордо     рдордордПрдПрдо 150.000000\n",
      "    gae     рдЧрдП       nan 150.000000\n",
      "  chhai     рдЫреИ       рдЪрдИрдП 150.000000\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by cer:\n",
      "     english          actual       predicted  cer\n",
      "    apvartit        рдЕрдкрд╡рд░реНрддрд┐рдд        рдЕрдкрд╡рд░реНрддрд┐рдд  0.0\n",
      "  chandrapur        рдЪрдВрджреНрд░рдкреБрд░        рдЪрдВрджреНрд░рдкреБрд░  0.0\n",
      "      satpal           рд╕рддрдкрд╛рд▓           рд╕рддрдкрд╛рд▓  0.0\n",
      "      rozlyn          рд░реЛрдЬрд▓рд┐рди          рд░реЛрдЬрд▓рд┐рди  0.0\n",
      "        soma            рд╕реЛрдорд╛            рд╕реЛрдорд╛  0.0\n",
      " narayangarh       рдирд╛рд░рд╛рдпрдгрдЧрдврд╝       рдирд╛рд░рд╛рдпрдгрдЧрдврд╝  0.0\n",
      "distributary рдбрд┐рд╕реНрдЯреНрд░реАрдмреНрдпреВрдЯрд░реА рдбрд┐рд╕реНрдЯреНрд░реАрдмреНрдпреВрдЯрд░реА  0.0\n",
      "    noobiyaa          рдиреВрдмрд┐рдпрд╛          рдиреВрдмрд┐рдпрд╛  0.0\n",
      "     seemaai           рд╕реАрдорд╛рдИ           рд╕реАрдорд╛рдИ  0.0\n",
      "   pansariya         рдкрдВрд╕рд░рд┐рдпрд╛         рдкрдВрд╕рд░рд┐рдпрд╛  0.0\n",
      "\n",
      "ЁЯФ╣ Top 10 by bleu:\n",
      "      english     actual  predicted  bleu\n",
      "   baharampur   рдмрд╣рд░рд╛рдордкреБрд░   рдмрд╣рд░рд╛рдордкреБрд░ 100.0\n",
      "      isteefe    рдЗрд╕реНрддреАрдлреЗ    рдЗрд╕реНрддреАрдлреЗ 100.0\n",
      "      choodhe      рдЪреВрдбрд╝реЗ      рдЪреВрдбрд╝реЗ 100.0\n",
      "shaharwasiyon рд╢рд╣рд░рд╡рд╛рд╕рд┐рдпреЛрдВ рд╢рд╣рд░рд╡рд╛рд╕рд┐рдпреЛрдВ 100.0\n",
      "           rp       рдЖрд░рдкреА       рдЖрд░рдкреА 100.0\n",
      "     upadravi    рдЙрдкрджреНрд░рд╡реА    рдЙрдкрджреНрд░рд╡реА 100.0\n",
      "         puru       рдкреБрд░реВ       рдкреБрд░реВ 100.0\n",
      "     andhapan     рдЕрдВрдзрд╛рдкрди     рдЕрдВрдзрд╛рдкрди 100.0\n",
      "       lendal      рд▓реЗрдВрдбрд▓      рд▓реЗрдВрдбрд▓ 100.0\n",
      "   ummidwaari рдЙрдореНрдореАрджрд╡рд╛рд░реА рдЙрдореНрдореАрджрд╡рд╛рд░реА 100.0\n",
      "\n",
      "ЁЯФ╕ Bottom 10 by bleu:\n",
      "english          actual predicted     bleu\n",
      "    fry           рдлреНрд░рд╛рдИ         рдП 0.915782\n",
      "   tuth           рдЯреНрд░реБрде         рдд 0.915782\n",
      "   agha            рдЖрдЧрд╝рд╛         рдП 2.489353\n",
      "     hr            рдПрдЪрдЖрд░         рд╣ 2.489353\n",
      "     jh            рдЬреЗрдПрдЪ         рдЭ 2.489353\n",
      "  thane            рдерд╛рдгреЗ         рди 2.489353\n",
      "    win     рдбрдмреНрд▓реНрдпреВрдЖрдИрдПрди       nan 2.639800\n",
      "     nn            рдПрдирдПрди         рди 2.960357\n",
      "   qiwi рдХреНрдпреВрдЖрдИрдбрдмреНрд▓реНрдпреВрдЖрдИ      рд╡реАрдЖрдИ 3.022962\n",
      "    nws     рдПрдирдбрдмреНрд▓реНрдпреВрдПрд╕       рдиреНрд╕ 3.733241\n"
     ]
    }
   ],
   "source": [
    "metrics_df1 = calculate_accuracies('predictions/uni_no_attn_lstm_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fa083",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df2 = calculate_accuracies('predictions/uni_attn_lstm_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
