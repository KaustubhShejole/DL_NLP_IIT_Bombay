{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRdpoWePeYHn"
   },
   "source": [
    "## Importing Libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 18 09:22:10 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   53C    P0              78W / 300W |  40494MiB / 81920MiB |     21%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              70W / 300W |    827MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   58C    P0              88W / 300W |   3858MiB / 81920MiB |     22%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              60W / 300W |    663MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z4ZVrIumZcDt"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import wandb\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwL09v65CIse",
    "outputId": "5ea72523-6a50-474c-b617-b77e16d72ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44xIRolL_T_d"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y4zemXiyE6Fi"
   },
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {'#': 0, '$': 1, '^': 2}   # '^': start of sequence, '$' : unknown char, '#' : padding\n",
    "        self.index2char = {0: '#', 1: '$', 2: '^'}\n",
    "        self.vocab_size = 3  # Count\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.vocab_size\n",
    "            self.index2char[self.vocab_size] = char\n",
    "            self.vocab_size += 1\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.char2index[ch] for ch in s]\n",
    "\n",
    "    def decode(self, l):\n",
    "        return ''.join([self.index2char[i] for i in l])\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.char2index.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns maximum length of input and output words\n",
    "def maxLength(data):\n",
    "    ip_mlen, op_mlen = 0, 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        input = data[0][i]\n",
    "        output = data[1][i]\n",
    "        if(len(input)>ip_mlen):\n",
    "            ip_mlen=len(input)\n",
    "\n",
    "        if(len(output)>op_mlen):\n",
    "            op_mlen=len(output)\n",
    "\n",
    "    return ip_mlen, op_mlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getMaxLengthValues(lang):\n",
    "    base_path = f\"../../aks_dataset/{lang}\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(f\"{base_path}/train.csv\", header=None)\n",
    "    val_df = pd.read_csv(f\"{base_path}/valid.csv\", header=None)\n",
    "    test_df = pd.read_csv(f\"{base_path}/test.csv\", header=None)\n",
    "\n",
    "    # Initialize language vocabularies\n",
    "    input_lang = Language('eng')\n",
    "    output_lang = Language(lang)\n",
    "    \n",
    "    # Build vocabulary only from train data\n",
    "    for _, row in train_df.iterrows():\n",
    "        input_lang.addWord(str(row[0]))\n",
    "        output_lang.addWord(str(row[1]))\n",
    "    \n",
    "    # Compute max input/output lengths for each split\n",
    "    m1, m01 = maxLength(train_df)\n",
    "    m2, m02 = maxLength(test_df)\n",
    "    m3, m03 = maxLength(val_df)\n",
    "\n",
    "    # Return the largest values across all splits\n",
    "    return max(m1, m2, m3), max(m01, m02, m03)\n",
    "\n",
    "# Example usage\n",
    "input_max_len, output_max_len = getMaxLengthValues('hin')\n",
    "print(input_max_len, output_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IDGaCO8DkYpc"
   },
   "outputs": [],
   "source": [
    "input_shape = 0\n",
    "def preprocess(data, input_lang, output_lang, input_max_len, output_max_len, s=''):\n",
    "\n",
    "    unknown = input_lang.char2index['$']\n",
    "\n",
    "    n = len(data)\n",
    "    input = torch.zeros((n, input_max_len + 1), device = device)\n",
    "    output = torch.zeros((n, output_max_len + 2), device = device)\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        inp = data[0][i].ljust(input_max_len + 1, '#')\n",
    "        op = '^' + data[1][i]       # add start symbol to output\n",
    "        op = op.ljust(output_max_len + 2, '#')\n",
    "\n",
    "        for index, char in enumerate(inp):\n",
    "            if char in input_lang.char2index:\n",
    "                input[i][index] = input_lang.char2index[char]\n",
    "            else:\n",
    "                input[i][index] = unknown\n",
    "\n",
    "        for index, char in enumerate(op):\n",
    "            if char in output_lang.char2index:\n",
    "                output[i][index] = output_lang.char2index[char]\n",
    "            else:\n",
    "                output[i][index] = unknown\n",
    "\n",
    "    print(s, ' dataset')\n",
    "    print(input.shape)\n",
    "    print(output.shape)\n",
    "\n",
    "    return TensorDataset(input.to(torch.int32), output.to(torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdS5OXKxfdCX",
    "outputId": "283fb51a-9a4a-4fc5-bad1-ea66373b29b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  dataset\n",
      "torch.Size([10112, 30])\n",
      "torch.Size([10112, 28])\n"
     ]
    }
   ],
   "source": [
    "def load_prepare_data(lang):\n",
    "    train_df = pd.read_csv(f\"../../aks_dataset/{lang}/train.csv\", header = None)\n",
    "    val_df = pd.read_csv(f\"../../aks_dataset/{lang}/valid.csv\", header = None)\n",
    "    test_df = pd.read_csv(f\"../../aks_dataset/{lang}/test.csv\", header = None)\n",
    "\n",
    "    input_lang = Language('eng')\n",
    "    output_lang = Language(lang)\n",
    "\n",
    "    # create vocablury\n",
    "    for i in range(len(train_df)):\n",
    "        input_lang.addWord(train_df[0][i]) # 'eng'\n",
    "        output_lang.addWord(train_df[1][i]) # 'hin'\n",
    "\n",
    "    # encode the datasets\n",
    "    test_data = preprocess(test_df, input_lang, output_lang,input_max_len, output_max_len, 'test')\n",
    "\n",
    "    return test_data, input_lang, output_lang\n",
    "\n",
    "\n",
    "test_data, input_lang, output_lang = load_prepare_data('hin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data[23][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nu-NTR6BDj8e",
    "outputId": "bd3dba2a-092d-4846-a5fb-f703f119b56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jadule########################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^जडूले######################'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_lang.decode(test_data[23][0].tolist()))\n",
    "output_lang.decode(test_data[23][1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJI8iU6dBSE0",
    "outputId": "818815ee-503e-4dcd-b7a6-5f00a06b5ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  8, 32, 29, 13, 30,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[23][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PugX7KHvc65u"
   },
   "outputs": [],
   "source": [
    "# encoder specific detail\n",
    "input_vocab_size = input_lang.vocab_size\n",
    "encoder_block_size = len(test_data[0][0])\n",
    "\n",
    "# decoder specific detail\n",
    "output_vocab_size = output_lang.vocab_size\n",
    "decoder_block_size = len(test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(encoder_block_size)\n",
    "print(decoder_block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdltQ7oJCq1j"
   },
   "source": [
    "### Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uiluDiY7FAMU"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one self-attention head \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, d_k, dropout, mask=0): # d_k is dimention of key , nomaly d_k = n_embd / 4\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.key = nn.Linear(n_embd, d_k, bias=False, device=device)\n",
    "        self.query = nn.Linear(n_embd, d_k, bias=False, device=device)\n",
    "        self.value = nn.Linear(n_embd, d_k, bias=False, device=device)\n",
    "        if mask:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(encoder_block_size, encoder_block_size, device=device)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output = None):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        if encoder_output is not None:\n",
    "            k = self.key(encoder_output)\n",
    "            Be, Te, Ce = encoder_output.shape\n",
    "        else:\n",
    "            k = self.key(x) # (B,T,d_k)\n",
    "\n",
    "        q = self.query(x) # (B,T,d_k)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,T)\n",
    "\n",
    "        if self.mask:\n",
    "            if encoder_output is not None:\n",
    "                wei = wei.masked_fill(self.tril[:T, :Te] == 0, float('-inf')) # (B,T,T)\n",
    "            else:\n",
    "                wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform weighted aggregation of values\n",
    "        if encoder_output is not None:\n",
    "            v = self.value(encoder_output)\n",
    "        else:\n",
    "            v = self.value(x)\n",
    "        out = wei @ v # (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple self attention heads in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_head, d_k, dropout, mask=0):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, d_k, dropout, mask) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        out = torch.cat([h(x, encoder_output) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" multiple self attention heads in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class encoderBlock(nn.Module):\n",
    "    \"\"\" Tranformer encoder block : communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        d_k = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, d_k, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        x = x + self.sa(self.ln1(x), encoder_output)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(input_vocab_size, n_embd) # n_embd: input embedding dimension\n",
    "        self.position_embedding_table = nn.Embedding(encoder_block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[encoderBlock(n_embd, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
    "        x = self.blocks(x) # apply one attention layer (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgPU486JC8Mz"
   },
   "source": [
    "### Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JteOV0CdC_bv"
   },
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    \"\"\" Tranformer decoder block : self communication then cross communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        d_k = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, d_k, dropout, mask = 1)\n",
    "        self.ca = MultiHeadAttention(n_embd, n_head, d_k, dropout, mask = 1)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, device=device)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, device=device)\n",
    "        self.ln3 = nn.LayerNorm(n_embd, device=device)\n",
    "\n",
    "    def forward(self, x_encoder_output):\n",
    "        x = x_encoder_output[0]\n",
    "        encoder_output = x_encoder_output[1]\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ca(self.ln2(x), encoder_output)\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return (x,encoder_output)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(output_vocab_size, n_embd) # n_embd: input embedding dimension\n",
    "        self.position_embedding_table = nn.Embedding(decoder_block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[decoderBlock(n_embd, n_head=n_head, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, output_vocab_size)\n",
    "\n",
    "    def forward(self, idx, encoder_output, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
    "\n",
    "        x =self.blocks((x, encoder_output))\n",
    "        x = self.ln_f(x[0]) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,output_vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            temp_logits = logits.view(B*T, C)\n",
    "            targets = targets.reshape(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(temp_logits, targets.long())\n",
    "\n",
    "        # print(logits)\n",
    "        # out = torch.argmax(logits)\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4M3aMxTl-zb"
   },
   "source": [
    "## generate output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mfIxu6njkxuD"
   },
   "outputs": [],
   "source": [
    "def generate(input):\n",
    "    B, T = input.shape\n",
    "    encoder_output = encoder(input)\n",
    "    idx = torch.full((B, 1), 2, dtype=torch.long, device=device) # (B,1)\n",
    "\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(decoder_block_size-1):\n",
    "        # get the predictions\n",
    "        logits, loss = decoder(idx, encoder_output) # logits (B, T, vocab_size)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeB2nYeFmXy8"
   },
   "source": [
    "## Check Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dIzXiSLBkxuD",
    "outputId": "ebe1d201-32bb-4372-e64a-62ebe173799d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def check(encoder, decoder, test_data, device='cpu', batch_size=64, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      - word-level accuracy: fraction of sequences where all non-pad tokens match exactly\n",
    "      - char-level accuracy: fraction of non-pad tokens that are predicted correctly\n",
    "      - avg validation loss (per-batch loss averaged over batches)\n",
    "    Parameters:\n",
    "      encoder, decoder : your models\n",
    "      test_data         : dataset (not a dataloader) or anything accepted by DataLoader\n",
    "      device            : 'cpu' or 'cuda'\n",
    "      batch_size        : dataloader batch size\n",
    "      pad_idx           : integer index used for padding tokens; set to None to not mask pads\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_sequences = 0           # total number of sequences (examples)\n",
    "    total_nonpad_tokens = 0       # total number of non-pad tokens counted across all sequences\n",
    "    char_correct = 0              # count of correctly predicted tokens (char-level)\n",
    "    word_exact_correct = 0        # count of sequences with exact match on all non-pad tokens\n",
    "    running_loss_val = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in test_loader:\n",
    "            val_x = val_x.to(device)\n",
    "            val_y = val_y.to(device)\n",
    "\n",
    "            # generate should produce token predictions of same shape as val_y\n",
    "            output = generate(val_x)  # expected shape: (batch, seq_len)\n",
    "            # get logits and loss from decoder as before; decoder returns (logits, loss)\n",
    "            encoder_output = encoder(val_x)\n",
    "            logits, loss = decoder(val_y[:, :-1], encoder_output, val_y[:, 1:])\n",
    "            # accumulate scalar loss\n",
    "            running_loss_val += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # Align lengths: compare output[:,1:] with val_y[:,1:]\n",
    "            pred = output[:, 1:]\n",
    "            target = val_y[:, 1:]\n",
    "\n",
    "            # ensure same shape (if generate produced different length, adjust or slice)\n",
    "            min_len = min(pred.size(1), target.size(1))\n",
    "            pred = pred[:, :min_len]\n",
    "            target = target[:, :min_len]\n",
    "\n",
    "            if pad_idx is not None:\n",
    "                mask = (target != pad_idx)          # True where token is not padding\n",
    "                nonpad_count = mask.sum().item()\n",
    "                # char-level correct: count positions where pred == target and target != pad\n",
    "                char_correct += ( (pred == target) & mask ).sum().item()\n",
    "                total_nonpad_tokens += nonpad_count\n",
    "\n",
    "                # word-level exact: for each sequence, check equality only on non-pad positions\n",
    "                # If a sequence has zero nonpad tokens (weird), treat as not correct.\n",
    "                seq_equal = torch.all( ((pred == target) | (~mask)), dim=1 )\n",
    "                # but we should exclude sequences with zero nonpad tokens from denominator:\n",
    "                nonpad_per_seq = mask.sum(dim=1)\n",
    "                valid_seq_mask = (nonpad_per_seq > 0)\n",
    "                if valid_seq_mask.any():\n",
    "                    word_exact_correct += seq_equal[valid_seq_mask].sum().item()\n",
    "                    total_sequences += valid_seq_mask.sum().item()\n",
    "                # sequences with zero nonpad tokens are ignored for word-level stats\n",
    "            else:\n",
    "                # no padding: count all positions\n",
    "                total_nonpad_tokens += target.numel()\n",
    "                char_correct += (pred == target).sum().item()\n",
    "\n",
    "                seq_equal = torch.all(pred == target, dim=1)\n",
    "                word_exact_correct += seq_equal.sum().item()\n",
    "                total_sequences += pred.size(0)\n",
    "\n",
    "    # final metrics\n",
    "    avg_loss = running_loss_val / n_batches if n_batches > 0 else float('nan')\n",
    "    char_acc = (char_correct / total_nonpad_tokens * 100.0) if total_nonpad_tokens > 0 else float('nan')\n",
    "    word_acc = (word_exact_correct / total_sequences * 100.0) if total_sequences > 0 else float('nan')\n",
    "\n",
    "    print(f\"Validation avg loss (per-batch): {avg_loss:.6f}\")\n",
    "    print(f\"Char-level accuracy (non-pad tokens): {char_acc:.4f}%  ({char_correct}/{total_nonpad_tokens})\")\n",
    "    print(f\"Word-level (sequence-exact) accuracy: {word_acc:.4f}%  ({word_exact_correct}/{total_sequences})\")\n",
    "\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"char_acc\": char_acc,\n",
    "        \"word_acc\": word_acc,\n",
    "        \"char_correct\": char_correct,\n",
    "        \"total_nonpad_tokens\": total_nonpad_tokens,\n",
    "        \"word_exact_correct\": word_exact_correct,\n",
    "        \"total_sequences\": total_sequences\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Make sure Encoder and Decoder class definitions are already imported or defined above this line!\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load saved encoder and decoder\n",
    "encoder = torch.load('models/transformer-encoder.pth', map_location=device)\n",
    "decoder = torch.load('models/transformer-decoder.pth', map_location=device)\n",
    "\n",
    "# Put them in eval mode before testing\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"Models loaded successfully on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck\u001b[49m(encoder, decoder, test_data, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'check' is not defined"
     ]
    }
   ],
   "source": [
    "check(encoder, decoder, test_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FnHR_oql6-S4"
   },
   "outputs": [],
   "source": [
    "# input_lang.char2index\n",
    "# output_lang.index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def encode_str(input_str, input_lang, input_max_len, s=''):\n",
    "    input_words = input_str.split(' ')\n",
    "    unknown = input_lang.char2index.get('$', 0)  # fallback in case '$' is missing\n",
    "\n",
    "    n = len(input_words)\n",
    "    input_tensor = torch.zeros((n, input_max_len + 1), dtype=torch.long, device=device)\n",
    "\n",
    "    for i, word in enumerate(input_words):\n",
    "        inp = word.ljust(input_max_len + 1, '#')  # pad to fixed length\n",
    "        for index, char in enumerate(inp):\n",
    "            input_tensor[i][index] = input_lang.char2index.get(char, unknown)\n",
    "\n",
    "#     print(s, ' dataset')\n",
    "#     print(input_tensor.shape)\n",
    "\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_lang' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m encode_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput str how\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43minput_lang\u001b[49m, input_max_len)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(encoded_input)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_lang' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_input = encode_str(\"input str how\", input_lang, input_max_len)\n",
    "# print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input#########################\n",
      "str###########################\n",
      "how###########################\n"
     ]
    }
   ],
   "source": [
    "# for enc in enc_list:\n",
    "#     print(input_lang.decode(enc.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate(input, max_len=None, start_token=2):\n",
    "    \"\"\"\n",
    "    Auto-regressive greedy decoding from encoder output.\n",
    "    \n",
    "    Parameters:\n",
    "        input      : torch.Tensor of shape (B, T_in) - input sequences\n",
    "        max_len    : maximum length to decode (optional)\n",
    "        start_token: integer index for start-of-sequence token\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (B, T_out) with predicted token IDs\n",
    "    \"\"\"\n",
    "    encoder_output = encoder(input)  # (B, ...), whatever your encoder outputs\n",
    "    B = input.size(0)\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = decoder_block_size  # default decoding length\n",
    "\n",
    "    # start with start token\n",
    "    idx = torch.full((B, 1), start_token, dtype=torch.long, device=input.device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits, _ = decoder(idx, encoder_output)  # logits: (B, T_so_far, vocab_size)\n",
    "        # take last timestep\n",
    "        logits_last = logits[:, -1, :]            # (B, vocab_size)\n",
    "        # greedy: pick highest probability token\n",
    "        next_token = torch.argmax(logits_last, dim=-1, keepdim=True)  # (B, 1)\n",
    "        # append to sequence\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def beam_search_generate(input_ids,\n",
    "                         max_len=None,\n",
    "                         start_token=2,\n",
    "                         beam_width=4,\n",
    "                         EOS_IDX=None,\n",
    "                         length_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Batched beam search decoding.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.LongTensor): (B, T_in) encoder inputs.\n",
    "        max_len (int or None): max decode length (including start token). If None, uses decoder_block_size.\n",
    "        start_token (int): start-of-sequence token id.\n",
    "        beam_width (int): beam width.\n",
    "        EOS_IDX (int or None): optional end-of-sequence token id.\n",
    "        length_penalty (float): exponent alpha. final_score = score / (length ** alpha).\n",
    "                               Use 0.0 to disable.\n",
    "\n",
    "    Returns:\n",
    "        best_sequences: (B, L) LongTensor -- best beam per batch (includes start_token)\n",
    "        all_beams: dict with keys:\n",
    "            'sequences' -> (B, beam_width, L) LongTensor (all beams)\n",
    "            'scores'    -> (B, beam_width) FloatTensor (raw log-prob sums, before length penalty)\n",
    "    Notes:\n",
    "        - Expects `encoder(input_ids)` and `decoder(decoder_input_ids, encoder_output)` to be defined in scope.\n",
    "        - `decoder` is expected to return (logits, ...). logits shape: (B*beam_width, cur_len, V)\n",
    "    \"\"\"\n",
    "    device = input_ids.device\n",
    "    B = input_ids.size(0)\n",
    "\n",
    "    # run encoder (adjust if your encoder returns tuple)\n",
    "    encoder_output = encoder(input_ids)   # shape dependent on your model\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = decoder_block_size  # assumed to be defined in scope\n",
    "\n",
    "    # initial length (we store sequences including the start token)\n",
    "    cur_len = 1\n",
    "\n",
    "    # sequences: (B, beam_width, cur_len)\n",
    "    sequences = torch.full((B, beam_width, cur_len),\n",
    "                           fill_value=start_token,\n",
    "                           dtype=torch.long,\n",
    "                           device=device)\n",
    "\n",
    "    # scores: log-prob sums for each beam. initialize -inf for beams > 0\n",
    "    neg_inf = -1e9\n",
    "    scores = torch.full((B, beam_width), neg_inf, device=device)\n",
    "    scores[:, 0] = 0.0\n",
    "\n",
    "    # finished flags per beam\n",
    "    finished = torch.zeros((B, beam_width), dtype=torch.bool, device=device)\n",
    "\n",
    "    # We'll keep expanding up to max_len-1 additional tokens (since we already have start token)\n",
    "    for step in range(1, max_len):\n",
    "        # Flatten sequences to feed to decoder: (B*beam_width, cur_len)\n",
    "        flat_seq = sequences.view(B * beam_width, cur_len)\n",
    "\n",
    "        # Repeat encoder output to match beam dimension.\n",
    "        # If encoder_output is a tensor of shape (B, ...), repeat_interleave works.\n",
    "        # If it's a tuple (e.g., (enc_out, enc_mask)), adapt accordingly.\n",
    "        try:\n",
    "            enc_flat = encoder_output.repeat_interleave(beam_width, dim=0)\n",
    "        except Exception:\n",
    "            # Fallback: if encoder_output is a tuple/list, repeat each tensor inside\n",
    "            if isinstance(encoder_output, (tuple, list)):\n",
    "                enc_flat = tuple(x.repeat_interleave(beam_width, dim=0) for x in encoder_output)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Call decoder once for all beams\n",
    "        logits, _ = decoder(flat_seq, enc_flat)            # (B*beam_width, cur_len, V)\n",
    "        last_logits = logits[:, -1, :]                     # (B*beam_width, V)\n",
    "        log_probs = F.log_softmax(last_logits, dim=-1)     # (B*beam_width, V)\n",
    "\n",
    "        V = log_probs.size(-1)\n",
    "        log_probs = log_probs.view(B, beam_width, V)       # (B, beam_width, V)\n",
    "\n",
    "        # For beams that are finished, prevent expansion except allowing EOS to preserve score.\n",
    "        if EOS_IDX is not None:\n",
    "            # Build mask: (B, beam_width, V) True -> allow, False -> block\n",
    "            allow_mask = torch.ones_like(log_probs, dtype=torch.bool, device=device)\n",
    "\n",
    "            # For finished beams, allow only EOS token to have zero mask; others blocked\n",
    "            finished_exp = finished.unsqueeze(-1).expand(B, beam_width, V)\n",
    "            if finished_exp.any():\n",
    "                allow_mask = ~finished_exp  # disallow all tokens for finished beams\n",
    "                # exception: allow EOS token for finished beams (so it can be \"re-selected\")\n",
    "                allow_mask[..., EOS_IDX] = allow_mask[..., EOS_IDX] | finished_exp[..., 0]\n",
    "\n",
    "            # apply mask: set disallowed tokens to -inf\n",
    "            log_probs = torch.where(allow_mask, log_probs, torch.tensor(neg_inf, device=device, dtype=log_probs.dtype))\n",
    "\n",
    "        # candidate scores: scores[:, b] + log_probs[:, b, v]\n",
    "        scores_expanded = scores.unsqueeze(-1)               # (B, beam_width, 1)\n",
    "        candidate_scores = scores_expanded + log_probs       # (B, beam_width, V)\n",
    "\n",
    "        # flatten beam and vocab dims -> (B, beam_width * V)\n",
    "        candidate_scores_flat = candidate_scores.view(B, beam_width * V)\n",
    "\n",
    "        # pick top-k overall candidates per batch (k = beam_width)\n",
    "        k = min(beam_width, candidate_scores_flat.size(1))\n",
    "        topk_scores, topk_indices = torch.topk(candidate_scores_flat, k=k, dim=-1)  # (B, k)\n",
    "\n",
    "        # decode topk indices -> previous beam & token id\n",
    "        prev_beam_idx = topk_indices // V                    # (B, k)\n",
    "        token_idx = topk_indices % V                         # (B, k)\n",
    "\n",
    "        # prepare new tensors for next step\n",
    "        new_sequences = torch.zeros((B, beam_width, cur_len + 1), dtype=torch.long, device=device)\n",
    "        new_scores = torch.full((B, beam_width), neg_inf, device=device)\n",
    "        new_finished = torch.zeros((B, beam_width), dtype=torch.bool, device=device)\n",
    "\n",
    "        # populate new beams\n",
    "        for i in range(B):\n",
    "            for k_i in range(k):\n",
    "                pb = int(prev_beam_idx[i, k_i].item())\n",
    "                tk = int(token_idx[i, k_i].item())\n",
    "                new_sequences[i, k_i, :cur_len] = sequences[i, pb]\n",
    "                new_sequences[i, k_i, cur_len] = tk\n",
    "                new_scores[i, k_i] = topk_scores[i, k_i]\n",
    "                # finished if previously finished or we just produced EOS\n",
    "                new_finished[i, k_i] = finished[i, pb].item() or (EOS_IDX is not None and tk == EOS_IDX)\n",
    "\n",
    "        # replace beam state\n",
    "        sequences = new_sequences\n",
    "        scores = new_scores\n",
    "        finished = new_finished\n",
    "        cur_len += 1\n",
    "\n",
    "        # if all beams finished for all batches -> break\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    # At this point, `sequences` is (B, beam_width, cur_len) and `scores` are raw log-prob sums.\n",
    "    # Apply length penalty when ranking final beams (if requested)\n",
    "    if length_penalty != 0.0:\n",
    "        # length to use for penalty: number of generated tokens excluding the start token\n",
    "        length = cur_len - 1\n",
    "        # avoid division by zero; use max(1, length)\n",
    "        length_for_penalty = max(1, length)\n",
    "        penalty = (length_for_penalty ** length_penalty)\n",
    "        final_scores = scores / penalty\n",
    "    else:\n",
    "        final_scores = scores\n",
    "\n",
    "    # pick best beam per batch\n",
    "    best_beam_idx = torch.argmax(final_scores, dim=1)   # (B,)\n",
    "\n",
    "    # extract best sequences\n",
    "    best_sequences = torch.zeros((B, cur_len), dtype=torch.long, device=device)\n",
    "    for i in range(B):\n",
    "        best_sequences[i] = sequences[i, best_beam_idx[i]]\n",
    "\n",
    "    # Return both best sequences and all beam states if caller wants to inspect n-best.\n",
    "    all_beams = {'sequences': sequences, 'scores': scores}\n",
    "    return best_sequences, all_beams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Transformer Model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m encode_str(input_str, input_lang, input_max_len)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# generate prediction\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m predicted_tokens, all_beams \u001b[38;5;241m=\u001b[39m \u001b[43mtest_generate\u001b[49m(encoded_input\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_beams)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# decode to string\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_generate' is not defined"
     ]
    }
   ],
   "source": [
    "input_str = \"pushpak shalaka kaustubh sandhyaa\"\n",
    "\n",
    "print('I am a Transformer Model')\n",
    "\n",
    "# encode input\n",
    "encoded_input = encode_str(input_str, input_lang, input_max_len)\n",
    "# generate prediction\n",
    "predicted_tokens = test_generate(encoded_input.to(device))\n",
    "\n",
    "# decode to string\n",
    "for seq in predicted_tokens:\n",
    "    print(output_lang.decode(seq.tolist()).split('^')[1].split('#')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Transformer Model\n",
      "पुष्पक\n",
      "शालका\n",
      "कौस्तुभ\n",
      "संध्या\n",
      "{'sequences': tensor([[[ 2,  3, 19, 48, 18,  3,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 2,  3, 19, 41, 18,  3,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 41, 12, 13,  5, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 2, 41, 13,  5, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  5, 40, 17, 18, 24, 19, 35,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 2,  5, 40, 17, 18,  7, 19, 35,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 17, 11, 51, 18,  9, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 2, 17, 11, 51, 18,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]], device='cuda:0'), 'scores': tensor([[-0.1200, -3.5401],\n",
      "        [-1.3435, -2.0797],\n",
      "        [-0.3980, -2.2163],\n",
      "        [-0.5008, -1.0455]], device='cuda:0', grad_fn=<CopySlices>)}\n"
     ]
    }
   ],
   "source": [
    "input_str = \"pushpak shalaka kaustubh sandhyaa\"\n",
    "print('I am a Transformer Model')\n",
    "\n",
    "# --- encode input ---\n",
    "encoded_input = encode_str(input_str, input_lang, input_max_len)\n",
    "\n",
    "# ensure batch dimension (B, T)\n",
    "if encoded_input.dim() == 1:\n",
    "    encoded_input = encoded_input.unsqueeze(0)   # shape -> (1, T)\n",
    "\n",
    "# --- try to discover EOS / SOS indices from your output_lang if available ---\n",
    "SOS_IDX = 2   # your earlier default; change if your project uses a different SOS\n",
    "EOS_IDX = None\n",
    "try:\n",
    "    if hasattr(output_lang, \"token2index\"):\n",
    "        EOS_IDX = output_lang.token2index.get(\"<EOS>\") or output_lang.token2index.get(\"</s>\") or EOS_IDX\n",
    "    if hasattr(output_lang, \"eos_idx\"):\n",
    "        EOS_IDX = output_lang.eos_idx\n",
    "except Exception:\n",
    "    EOS_IDX = EOS_IDX\n",
    "\n",
    "# --- run beam search (beam_width = 2) ---\n",
    "predicted_tokens, all_beams = beam_search_generate(\n",
    "    encoded_input.to(device),\n",
    "    max_len=decoder_block_size,\n",
    "    start_token=SOS_IDX,\n",
    "    beam_width=2,\n",
    "    EOS_IDX=EOS_IDX,\n",
    "    length_penalty=0.0\n",
    ")  # returns tensor shape (B, T_out) including start token\n",
    "\n",
    "# --- decode to strings exactly like you did earlier ---\n",
    "for seq in predicted_tokens:\n",
    "    # if you want to remove the start token first: seq = seq.tolist()[1:]\n",
    "    decoded = output_lang.decode(seq.tolist())\n",
    "    print(decoded.split('^')[1].split('#')[0])\n",
    "\n",
    "print(all_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hRdpoWePeYHn",
    "44xIRolL_T_d",
    "XdltQ7oJCq1j",
    "GgPU486JC8Mz",
    "658W9RARGEUf",
    "q7fAgs5uQni_",
    "n4rGh7vuQqaa",
    "nvyRJWUUbR2f",
    "8ETW0BG_Pa24",
    "MQPGy32rnD3V",
    "z_aYZvDD1OHU",
    "pKvBd5mKf0Hf",
    "FYMa5jTQRUaB",
    "zfuv5FoA1wt2",
    "W7CYNChRGuGK"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4721249,
     "sourceId": 8013732,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
