{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDghhjha-xUK"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ADfNyi1sB9L3"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "299AqvvaEdkf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# path = '/kaggle/input/marathi/mar_train.csv'\n",
    "# path_val = '/kaggle/input/marathi/mar_valid.csv'\n",
    "# path_test = '/kaggle/input/marathi/mar_test.csv'\n",
    "\n",
    "lang = 'hin'\n",
    "path = f\"../../aks_dataset/{lang}/train.csv\"\n",
    "path_val = f\"../../aks_dataset/{lang}/valid.csv\"\n",
    "path_test = f\"../../aks_dataset/{lang}/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtpQ9UGxFhkU",
    "outputId": "f1e70afc-85a2-4816-94cd-00e207f0d9ac"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path , header = None)\n",
    "df_val = pd.read_csv(path_val , header = None)\n",
    "df_test = pd.read_csv(path_test , header = None)\n",
    "english_words_val = df_val[0]\n",
    "marathi_words_val = df_val[1]\n",
    "english_words_test = df_test[0]\n",
    "marathi_words_test = df_test[1]\n",
    "english_words = df[0]\n",
    "marathi_words = df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtlfObKubBNq",
    "outputId": "6f0701f1-4823-47fc-b128-56c9446fd71a"
   },
   "outputs": [],
   "source": [
    "# creating list of charecters in both languages\n",
    "\n",
    "english_char_list = []\n",
    "max_length_word_english = -1\n",
    "for word in english_words:\n",
    "  max_length_word_english = max(max_length_word_english,len(word)) \n",
    "  for char in word :\n",
    "    english_char_list.append(char);\n",
    "english_char_list = list(set(english_char_list))\n",
    "english_char_list.sort()\n",
    "\n",
    "marathi_char_list = []\n",
    "max_length_word_marathi = -1\n",
    "for word in marathi_words:\n",
    "  max_length_word_marathi = max(max_length_word_marathi,len(word))\n",
    "  for char in word :\n",
    "    marathi_char_list.append(char);\n",
    "marathi_char_list = list(set(marathi_char_list))\n",
    "marathi_char_list.sort()\n",
    "\n",
    "\n",
    "# finding out the maximum size word for english and marathi from validation and test data.\n",
    "for word in english_words_val:\n",
    "  max_length_word_english = max(max_length_word_english,len(word))\n",
    "for word in english_words_test:\n",
    "  max_length_word_english = max(max_length_word_english,len(word)) \n",
    "for word in marathi_words_val:\n",
    "  max_length_word_marathi = max(max_length_word_marathi,len(word))\n",
    "for word in marathi_words_test:\n",
    "  max_length_word_marathi = max(max_length_word_marathi,len(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_word_english\n",
    "max_length_word_marathi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_word_marathi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save to pickle file\n",
    "with open('models/marathi_char_list.pkl', 'wb') as f:\n",
    "    pickle.dump(marathi_char_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save to pickle file\n",
    "with open('models/english_char_list.pkl', 'wb') as f:\n",
    "    pickle.dump(english_char_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fdcqBaU70W3v"
   },
   "outputs": [],
   "source": [
    "# english word to vector size = 27 ie. max_length_word_english\n",
    "# marathi word to vector size = 20 ie. max_length_word_marathi\n",
    "# for one word.\n",
    "def word2vec(word, lang):\n",
    "  vec = []\n",
    "  if(lang == \"english\"):\n",
    "    vec.append(len(english_char_list) + 1)\n",
    "    for char in word:\n",
    "      for i in range(len(english_char_list)):\n",
    "        if(english_char_list[i] == char):\n",
    "          vec.append(i+1)\n",
    "    while(len(vec) < max_length_word_english + 1): # padding with max_length + 1.\n",
    "        vec.append(0)\n",
    "    vec.append(0)\n",
    "  else :\n",
    "    vec.append(len(marathi_char_list) + 1)\n",
    "    for char in word:\n",
    "      for i in range(len(marathi_char_list)):\n",
    "        if( marathi_char_list[i] == char):\n",
    "          vec.append(i+1)\n",
    "    while(len(vec) < max_length_word_marathi + 1):  # padding with max_length + 1.\n",
    "        vec.append(0)\n",
    "    vec.append(0)\n",
    "  return(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heu_03Y3332P",
    "outputId": "bd542307-bcad-4812-9711-0bf468c89a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "[65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec('hi',\"marathi\")\n",
    "print('hi')\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pRObgwk5pcEz"
   },
   "outputs": [],
   "source": [
    "# creating matrix of representation for whole words of english and marathi.\n",
    "\n",
    "def ip_matrix_construct(words, lang):\n",
    "  ans = []\n",
    "  for word in words:\n",
    "    ans.append(word2vec(word, lang))\n",
    "  return(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FgS76C6rluA",
    "outputId": "ef164c52-28ad-470c-f430-b4ab0b25c91f"
   },
   "outputs": [],
   "source": [
    "# calculated representations of whole english and marathi words in variables english and marathi matrix.\n",
    "english_matrix = ip_matrix_construct(english_words, \"english\")\n",
    "marathi_matrix = ip_matrix_construct(marathi_words, \"marathi\")\n",
    "english_matrix = torch.tensor(english_matrix)\n",
    "marathi_matrix = torch.tensor(marathi_matrix)\n",
    "\n",
    "english_matrix_val = ip_matrix_construct(english_words_val, \"english\")\n",
    "marathi_matrix_val = ip_matrix_construct(marathi_words_val, \"marathi\")\n",
    "english_matrix_val = torch.tensor(english_matrix_val)\n",
    "marathi_matrix_val = torch.tensor(marathi_matrix_val)\n",
    "english_matrix_test = ip_matrix_construct(english_words_test, \"english\")\n",
    "marathi_matrix_test =ip_matrix_construct(marathi_words_test, \"marathi\")\n",
    "english_matrix_test = torch.tensor(english_matrix_test)\n",
    "marathi_matrix_test = torch.tensor(marathi_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cvjQGsLrsBJZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_size, embedding_size, hidden_size, enc_layers, p, cell_type, bidirectional):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.enc_layers = enc_layers\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.cell_type = cell_type\n",
    "    self.bidirectional = bidirectional\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    if(cell_type == \"GRU\"):\n",
    "      self.gru = nn.GRU(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
    "    if(cell_type == \"RNN\"):\n",
    "      self.rnn = nn.RNN(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
    "    if(cell_type == \"LSTM\"):\n",
    "      self.lstm = nn.LSTM(embedding_size, hidden_size, enc_layers, dropout = p, bidirectional = bidirectional)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    if(self.cell_type == \"GRU\"):\n",
    "      output, hidden = self.gru(embedding)\n",
    "    if(self.cell_type == \"RNN\"):\n",
    "      output, hidden = self.rnn(embedding)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "      outputs, (hidden,cell) = self.lstm(embedding)\n",
    "      return outputs, hidden, cell\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nea9Nz5E-xUP"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.dec_layers = dec_layers\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.cell_type = cell_type\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    if(cell_type == \"GRU\"):\n",
    "      self.gru = nn.GRU(embedding_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"RNN\"):\n",
    "      self.rnn = nn.RNN(embedding_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"LSTM\"):\n",
    "      self.lstm = nn.LSTM(embedding_size, hidden_size, dec_layers, dropout = p)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n",
    "  \n",
    "  def forward(self,x,output, hidden, cell = 0):\n",
    "    x = x.unsqueeze(0).int()\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    if(self.cell_type == \"GRU\"):\n",
    "        outputs, hidden = self.gru(embedding, hidden)\n",
    "    if(self.cell_type == \"RNN\"):\n",
    "        outputs, hidden = self.rnn(embedding, hidden)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "    # shape of outputs: (1, N, hidden_size)\n",
    "    predictions = self.fc(outputs)\n",
    "    # shape of predictions: (1, N, length_of_vocab)\n",
    "    predictions = predictions.squeeze(0)\n",
    "    # shape of predictions: (N, length_of_vocab)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        return predictions, hidden, cell\n",
    "    return predictions, hidden\n",
    "\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pmqgOzhw0t37"
   },
   "outputs": [],
   "source": [
    "class Atten_decoder(nn.Module):\n",
    "  def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type, bidirectional):\n",
    "    super(Atten_decoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.max_length = len(english_matrix[0])  \n",
    "    self.dec_layers = dec_layers\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.cell_type = cell_type\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    if(cell_type == \"GRU\"):\n",
    "      self.gru = nn.GRU(hidden_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"RNN\"):\n",
    "      self.rnn = nn.RNN(hidden_size, hidden_size, dec_layers, dropout = p)\n",
    "    if(cell_type == \"LSTM\"):\n",
    "      self.lstm = nn.LSTM(hidden_size, hidden_size, dec_layers, dropout = p)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n",
    "    self.attn = nn.Linear(hidden_size+embedding_size, self.max_length)\n",
    "    if(bidirectional):\n",
    "      self.attn_combine = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n",
    "    else :\n",
    "      self.attn_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "\n",
    "  def forward(self, x,output, hidden, cell = 0):\n",
    "    x = x.unsqueeze(0)\n",
    "    output=output.permute(1,0,2)\n",
    "    embedded = self.embedding(x)\n",
    "    embedded = self.dropout(embedded)\n",
    "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0],hidden[0]), 1)), dim = 1)\n",
    "    attn_applied = torch.bmm(attn_weights.unsqueeze(1),output)\n",
    "    attn_applied = attn_applied.squeeze(1)\n",
    "    op = torch.cat((embedded[0], attn_applied), 1)\n",
    "\n",
    "    op = self.attn_combine(op).unsqueeze(0)\n",
    "    op = F.relu(op)\n",
    "    if(self.cell_type == \"GRU\"):\n",
    "        outputs, hidden = self.gru(op, hidden)\n",
    "    if(self.cell_type == \"RNN\"):\n",
    "        outputs, hidden = self.rnn(op, hidden)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        outputs, (hidden, cell) = self.lstm(op, (hidden, cell))\n",
    "    predictions = self.fc(outputs)\n",
    "    predictions = predictions.squeeze(0)\n",
    "    if(self.cell_type == \"LSTM\"):\n",
    "        return predictions, hidden, cell\n",
    "    return predictions, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZkWSEAca-Okj"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, cell_type, bidirectional, enc_layers, dec_layers):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(marathi_char_list) + 2  \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        if(self.cell_type == \"LSTM\"):\n",
    "            encoder_output, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            encoder_output, hidden = self.encoder(source)\n",
    "        if(self.enc_layers != self.dec_layers or self.bidirectional == True):\n",
    "          hidden = hidden[self.enc_layers - 1] + hidden[self.enc_layers - 1]\n",
    "          hidden = hidden.repeat(self.dec_layers,1,1)\n",
    "          if(self.cell_type == \"LSTM\"):\n",
    "              cell = cell[self.enc_layers - 1] + cell[self.enc_layers - 1]\n",
    "              cell = cell.repeat(self.dec_layers,1,1)\n",
    "        \n",
    "        x = target[0]\n",
    "    \n",
    "        for t in range(1, target_len):\n",
    "            if(self.cell_type == \"LSTM\"):\n",
    "                output, hidden, cell = self.decoder(x, encoder_output, hidden, cell)\n",
    "            else :\n",
    "                output, hidden = self.decoder(x, encoder_output, hidden)\n",
    "            outputs[t] = output\n",
    "\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SYajaq47-xUQ"
   },
   "outputs": [],
   "source": [
    "def Accuracy(model, english_matrix, marathi_matrix, epoch, batch_size):\n",
    "    correct_count = 0\n",
    "    for batch_idx in range((int)(len(english_matrix) / batch_size)):\n",
    "        inp_data = english_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        target = marathi_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        output = model.forward(inp_data.T, target.T, 0)\n",
    "        output = nn.Softmax(dim=2)(output)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.T\n",
    "        for i in range(batch_size):\n",
    "            if(torch.equal(output[i][1:],target[i][1:])):\n",
    "                correct_count += 1\n",
    "    accuracy = correct_count * 100 / len(english_matrix)\n",
    "    return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Vg6VQCWWzhoy"
   },
   "outputs": [],
   "source": [
    "# creating list of expected marathi word and predicted marathi word.\n",
    "predictions_vanilla_train = []\n",
    "predictions_vanilla_val = []\n",
    "predictions_vanilla_test = []\n",
    "def matrix_to_words(model, english_matrix, marathi_matrix, batch_size, data_type):\n",
    "  for batch_idx in range((int)(len(english_matrix) / batch_size)+1):\n",
    "        inp_data = english_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        target = marathi_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "        output = model.forward(inp_data.T, target.T, 0)\n",
    "        output = nn.Softmax(dim=2)(output)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.T\n",
    "        for i in range(len(target)):\n",
    "          target_word = target[i]\n",
    "          output_word = output[i]\n",
    "          word1 = \"\"\n",
    "          word2 = \"\"\n",
    "          for j in range(len(target_word)):\n",
    "            if(target_word[j]>0 and target_word[j]<64):\n",
    "              word1 += marathi_char_list[target_word[j] - 1]\n",
    "          for j in range(len(output_word)):\n",
    "            if(output_word[j]>0 and output_word[j]<64):\n",
    "              word2 += marathi_char_list[output_word[j] - 1]\n",
    "          temp = [word1, word2]\n",
    "          if(data_type == \"train\"):\n",
    "            predictions_vanilla_train.append(temp)\n",
    "          if(data_type == \"validation\"):\n",
    "            predictions_vanilla_val.append(temp)\n",
    "          if(data_type == \"test\"):\n",
    "            predictions_vanilla_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9QCnSmaTMRSR"
   },
   "outputs": [],
   "source": [
    "def neural_network(cell_type, bidirectional, enc_layers, dec_layers, batch_size, embedding_size, hidden_size, enc_dropout, dec_dropout, attention):\n",
    "  learning_rate = 1e-3                                                                                                                                                                                                                                                              \n",
    "  num_epochs = 20\n",
    "  input_size_encoder = len(english_char_list) + 2  \n",
    "  input_size_decoder = len(marathi_char_list) + 2  \n",
    "  output_size        = len(marathi_char_list) + 2  \n",
    "\n",
    "  encoder_net = Encoder(input_size_encoder, embedding_size, hidden_size, enc_layers, enc_dropout, cell_type,bidirectional).to(device)\n",
    "  if(attention):\n",
    "    decoder_net = Atten_decoder(input_size_decoder,embedding_size,hidden_size,output_size,dec_layers,dec_dropout, cell_type, bidirectional).to(device)\n",
    "  else:\n",
    "    decoder_net = Decoder(input_size_decoder,embedding_size,hidden_size,output_size,dec_layers,dec_dropout, cell_type).to(device)\n",
    "\n",
    "  model = Seq2Seq(encoder_net, decoder_net, cell_type, bidirectional, enc_layers, dec_layers).to(device)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  pad_idx = len(marathi_char_list) + 1  # 64 # pading index for marathi\n",
    "  criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print(\"epoch = \",epoch)\n",
    "\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      step = 0\n",
    "      for batch_idx in range((int)(len(english_matrix) / batch_size)):\n",
    "\n",
    "          inp_data = english_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "          target = marathi_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
    "          target = target.T\n",
    "          output = model(inp_data.T, target)\n",
    "\n",
    "          output = output[1:].reshape(-1, output.shape[2])\n",
    "          target = target[1:].reshape(-1)\n",
    "          optimizer.zero_grad()\n",
    "          loss = criterion(output, target)\n",
    "          total_loss += loss\n",
    "          loss.backward()\n",
    "\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          step += 1\n",
    "      print(\"total loss = \",total_loss / step)\n",
    "      training_accuracy = Accuracy(model, english_matrix, marathi_matrix, epoch, batch_size)\n",
    "      print(\"Training Accuracy = \", training_accuracy)\n",
    "      val_accuracy = Accuracy(model, english_matrix_val, marathi_matrix_val, epoch, batch_size)\n",
    "      print(\"Validation accuracy = \",val_accuracy)\n",
    "      if(epoch > 10 and training_accuracy < 10):\n",
    "        return\n",
    "      if(epoch > 5 and training_accuracy < 1):\n",
    "        return # useless code\n",
    "  matrix_to_words(model, english_matrix, marathi_matrix, batch_size, \"train\")\n",
    "  print(\"train done\")\n",
    "  matrix_to_words(model, english_matrix_val, marathi_matrix_val, batch_size, \"validation\")\n",
    "  print(\"val done\")\n",
    "  matrix_to_words(model, english_matrix_test, marathi_matrix_test, batch_size, \"test\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkkdB4_mxLWV",
    "outputId": "b72ad08d-3643-4cb1-c283-cb55a2ad4700"
   },
   "outputs": [],
   "source": [
    "cell_type = \"LSTM\"\n",
    "bidirectional = True\n",
    "enc_layers = 2\n",
    "dec_layers = 2\n",
    "batch_size = 256\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "enc_dropout = 0\n",
    "dec_dropout = 0\n",
    "attention = True\n",
    "\n",
    "\n",
    "model = neural_network(cell_type, bidirectional, enc_layers, dec_layers, batch_size, embedding_size, hidden_size, enc_dropout, dec_dropout, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIS3GbEF5dsP",
    "outputId": "2f4b1594-3f7a-4502-f7e5-994c753b7a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"bi_predictions_vanilla_train_attention.csv\" has been created.\n",
      "CSV file \"bi_predictions_vanilla_val_attention.csv\" has been created.\n",
      "CSV file \"bi_predictions_vanilla_test_attention.csv\" has been created.\n"
     ]
    }
   ],
   "source": [
    "# creating csv file of predictions\n",
    "filename_train  = 'bi_predictions_vanilla_train_attention.csv'\n",
    "filename_val = 'bi_predictions_vanilla_val_attention.csv'\n",
    "filename_test = 'bi_predictions_vanilla_test_attention.csv'\n",
    "\n",
    "with open(filename_train, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_NONE)\n",
    "    writer.writerows(predictions_vanilla_train)\n",
    "\n",
    "print(f'CSV file \"{filename_train}\" has been created.')\n",
    "\n",
    "\n",
    "with open(filename_val, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_NONE)\n",
    "    writer.writerows(predictions_vanilla_val)\n",
    "\n",
    "print(f'CSV file \"{filename_val}\" has been created.')\n",
    "\n",
    "\n",
    "with open(filename_test, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_NONE)\n",
    "    writer.writerows(predictions_vanilla_test)\n",
    "\n",
    "print(f'CSV file \"{filename_test}\" has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_to_words(model, english_matrix_test, marathi_matrix_test, batch_size, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file \"predictions/bi_attn.csv\" has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# --- define data ---\n",
    "filename_test = 'predictions/bi_attn.csv'\n",
    "\n",
    "# english_words: list of English strings\n",
    "# predictions_vanilla_test: list of [actual_word, predicted_word] pairs\n",
    "\n",
    "\n",
    "\n",
    "# Combine into rows: [English, Actual, Predicted]\n",
    "rows = []\n",
    "for eng, (actual, predicted) in zip(english_words_test, predictions_vanilla_test):\n",
    "    rows.append([eng, actual, predicted])\n",
    "\n",
    "# --- write to CSV ---\n",
    "with open(filename_test, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['eng', 'actual', 'predicted'])  # header\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f'✅ CSV file \\\"{filename_test}\\\" has been created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_accuracies(csv_path):\n",
    "    # Read CSV (no headers)\n",
    "    df = pd.read_csv(csv_path, header=None, sep=',', names=['actual', 'predicted'])\n",
    "    \n",
    "    # Drop any empty rows\n",
    "    df = df.dropna()\n",
    "    \n",
    "    total_words = len(df)\n",
    "    correct_words = 0\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        actual = str(row['actual']).strip()\n",
    "        predicted = str(row['predicted']).strip()\n",
    "\n",
    "        # --- Word-level accuracy ---\n",
    "        if actual == predicted:\n",
    "            correct_words += 1\n",
    "\n",
    "        # --- Character-level accuracy ---\n",
    "        max_len = max(len(actual), len(predicted))\n",
    "        total_chars += max_len\n",
    "        for a, b in zip(actual, predicted):\n",
    "            if a == b:\n",
    "                correct_chars += 1\n",
    "\n",
    "    word_acc = correct_words / total_words * 100 if total_words > 0 else 0\n",
    "    char_acc = correct_chars / total_chars * 100 if total_chars > 0 else 0\n",
    "\n",
    "    print(f\"✅ Word-level accuracy: {word_acc:.2f}%\")\n",
    "    print(f\"✅ Character-level accuracy: {char_acc:.2f}%\")\n",
    "\n",
    "    return word_acc, char_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "67svBk48GAaX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word-level accuracy: 43.16%\n",
      "✅ Character-level accuracy: 73.80%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43.15905448717949, 73.79789660609586)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"bi_predictions_vanilla_test_attention.csv\"\n",
    "calculate_accuracies(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "bW22hUA2vTpV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, f\"model_{cell_type}_{'bi' if bidirectional else 'uni'}_{'attn' if attention else 'noattn'}.pt\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BMyJeinM-xUS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(28, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Atten_decoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(66, 256)\n",
       "    (lstm): LSTM(512, 512, num_layers=2)\n",
       "    (fc): Linear(in_features=512, out_features=66, bias=True)\n",
       "    (attn): Linear(in_features=768, out_features=31, bias=True)\n",
       "    (attn_combine): Linear(in_features=1280, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"models/model_LSTM_bi_attn.pt\")\n",
    "model.eval()  # Important: set to eval mode before testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "d9sr6TtgPdmn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_accuracies(csv_path):\n",
    "    # Read CSV (no headers)\n",
    "    df = pd.read_csv(csv_path, header=None, sep=',', names=['actual', 'predicted'])\n",
    "    \n",
    "    # Drop any empty rows\n",
    "    df = df.dropna()\n",
    "    \n",
    "    total_words = len(df)\n",
    "    correct_words = 0\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        actual = str(row['actual']).strip()\n",
    "        predicted = str(row['predicted']).strip()\n",
    "\n",
    "        # --- Word-level accuracy ---\n",
    "        if actual == predicted:\n",
    "            correct_words += 1\n",
    "\n",
    "        # --- Character-level accuracy ---\n",
    "        max_len = max(len(actual), len(predicted))\n",
    "        total_chars += max_len\n",
    "        for a, b in zip(actual, predicted):\n",
    "            if a == b:\n",
    "                correct_chars += 1\n",
    "\n",
    "    word_acc = correct_words / total_words * 100 if total_words > 0 else 0\n",
    "    char_acc = correct_chars / total_chars * 100 if total_chars > 0 else 0\n",
    "\n",
    "    print(f\"✅ Word-level accuracy: {word_acc:.2f}%\")\n",
    "    print(f\"✅ Character-level accuracy: {char_acc:.2f}%\")\n",
    "\n",
    "    return word_acc, char_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crRv0WD-Hu6V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LyTr7QZKMqQG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 1. Load saved model ===\n",
    "model_path = \"models/model_LSTM_bi_attn.pt\"   # <-- change filename as per your saved one\n",
    "model = torch.load(model_path, map_location=device)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully\")\n",
    "\n",
    "# === 2. Helper function to convert English word to tensor ===\n",
    "def encode_word(word, lang_dict, max_len):\n",
    "    tensor = torch.zeros(max_len, dtype=torch.long)\n",
    "    for i, ch in enumerate(word):\n",
    "        if ch in lang_dict:\n",
    "            tensor[i] = lang_dict[ch] + 1\n",
    "        else:\n",
    "            tensor[i] = 0  # unknown char or padding\n",
    "    return tensor.unsqueeze(1).to(device)  # shape: (seq_len, 1)\n",
    "\n",
    "# === 3. Helper function to decode tensor back to Marathi string ===\n",
    "def decode_tensor(tensor, marathi_char_list):\n",
    "    word = \"\"\n",
    "    for idx in tensor:\n",
    "        if 0 < idx < len(marathi_char_list) + 1:\n",
    "            word += marathi_char_list[idx - 1]\n",
    "    return word\n",
    "\n",
    "# === 2. Predict for a single English word ===\n",
    "def predict_word(model, english_word):\n",
    "    # convert to numeric tensor using your own function\n",
    "    eng_vec = word2vec(english_word, \"english\")\n",
    "    inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)  # shape [seq_len, 1]\n",
    "    \n",
    "    # dummy target tensor (zeroed out)\n",
    "    target_tensor = torch.zeros(max_length_word_marathi + 2, 1, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.forward(inp_tensor, target_tensor, 0)  # teacher forcing = 0\n",
    "        output = nn.Softmax(dim=2)(output)\n",
    "        output = torch.argmax(output, dim=2).squeeze(1)  # shape: [seq_len]\n",
    "    \n",
    "    # decode the predicted indices into Marathi characters\n",
    "    predicted_word = \"\"\n",
    "    for idx in output:\n",
    "        idx = idx.item()\n",
    "        if 0 < idx <= len(marathi_char_list):\n",
    "            predicted_word += marathi_char_list[idx - 1]\n",
    "    \n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1reXcQN1-xUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: kaustubh\n",
      "Predicted Marathi: कौस्तुभ\n"
     ]
    }
   ],
   "source": [
    "# === 5. Example usage ===\n",
    "english_word = \"kaustubh\"  # any test English transliteration\n",
    "predicted_marathi = predict_word(\n",
    "    model, \n",
    "    english_word\n",
    ")\n",
    "print(f\"English: {english_word}\")\n",
    "print(f\"Predicted Marathi: {predicted_marathi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mrNT-rKM-xUT"
   },
   "outputs": [],
   "source": [
    "def predict_with_sos(model, english_word, word2vec, max_length_word_marathi, marathi_char_list, SOS_IDX=65, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eng_vec = word2vec(english_word, \"english\")\n",
    "        inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)   # (seq_len, 1)\n",
    "\n",
    "        target_len = max_length_word_marathi\n",
    "        target_tensor = torch.zeros(target_len, 1, dtype=torch.long).to(device)\n",
    "        target_tensor[0, 0] = SOS_IDX   # <-- important\n",
    "\n",
    "        if hasattr(model, \"reset_hidden\"):\n",
    "            model.reset_hidden(batch_size=1)\n",
    "\n",
    "        logits = model.forward(inp_tensor, target_tensor, 0)    # (seq_len_dec, 1, vocab)\n",
    "        preds = torch.argmax(torch.softmax(logits, dim=2), dim=2).squeeze(1).cpu().tolist()\n",
    "\n",
    "        # robust decoding: skip leading special tokens (PAD or SOS) then decode until EOS if present\n",
    "        PAD_IDX = 0\n",
    "        EOS_IDX = None  # set if you know it\n",
    "        # skip leading PAD/SOS\n",
    "        i = 0\n",
    "        while i < len(preds) and (preds[i] == PAD_IDX or preds[i] == SOS_IDX):\n",
    "            i += 1\n",
    "        decoded = []\n",
    "        for idx in preds[i:]:\n",
    "            if EOS_IDX is not None and idx == EOS_IDX:\n",
    "                break\n",
    "            if 0 < idx <= len(marathi_char_list):\n",
    "                decoded.append(marathi_char_list[idx - 1])\n",
    "        predicted_word = \"\".join(decoded)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aPoWLZG3-xUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10112/10112 [02:54<00:00, 58.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to bi_lstm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load the test CSV (no headers) ---\n",
    "original_csv = \"../../aks_dataset/hin/test.csv\"\n",
    "df_test = pd.read_csv(original_csv, header=None)\n",
    "\n",
    "# --- Give column names to df_test ---\n",
    "df_test.columns = ['eng', 'actual']  # first column = English, second column = reference Hindi\n",
    "\n",
    "# --- Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Make sure your model is on the device\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# --- Predict function wrapper ---\n",
    "def predict_words_batch(model, words):\n",
    "    \"\"\"Predict multiple words using GPU.\"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for word in tqdm(words):\n",
    "            word = str(word).strip()\n",
    "            pred = predict_with_sos( \n",
    "    model, word, word2vec, max_length_word_marathi, marathi_char_list, 65, device\n",
    ")  # your existing function\n",
    "            predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# --- Get first column (English words) ---\n",
    "first_column = df_test['eng'].tolist()\n",
    "\n",
    "# --- Run predictions ---\n",
    "predicted = predict_words_batch(model, first_column)\n",
    "\n",
    "# --- Save predictions back to DataFrame ---\n",
    "df_test['predicted'] = predicted\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_test.to_csv(\"predictions/greedy/bi_attn.csv\", index=False)\n",
    "print(\"Predictions saved to bi_lstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load the test CSV (no headers) ---\n",
    "original_csv = \"../../aks_dataset/hin/test.csv\"\n",
    "df_test = pd.read_csv(original_csv, header=None)\n",
    "\n",
    "# --- Give column names to df_test ---\n",
    "df_test.columns = ['eng', 'actual']  # first column = English, second column = reference Hindi\n",
    "\n",
    "# --- Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Make sure your model is on the device\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# --- Predict function wrapper ---\n",
    "def predict_words_batch(model, words):\n",
    "    \"\"\"Predict multiple words using GPU.\"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for word in tqdm(words):\n",
    "            word = str(word).strip()\n",
    "            pred, beams = beam_search_predict(\n",
    "    model=model,\n",
    "    english_word=word,\n",
    "    word2vec=word2vec,\n",
    "    max_length_word_marathi=max_length_word_marathi,\n",
    "    marathi_char_list=marathi_char_list,\n",
    "    SOS_IDX=SOS_IDX,            # set this from your training config / Counter output\n",
    "    device=device,\n",
    "    beam_width=4,\n",
    "    EOS_IDX=None,            # set to your EOS index if known, otherwise None\n",
    "    length_norm_alpha=0.0,\n",
    "    return_beams=True\n",
    ")  # your existing function\n",
    "            predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# --- Get first column (English words) ---\n",
    "first_column = df_test['eng'].tolist()\n",
    "\n",
    "# --- Run predictions ---\n",
    "predicted = predict_words_batch(model, first_column)\n",
    "\n",
    "# --- Save predictions back to DataFrame ---\n",
    "df_test['predicted'] = predicted\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_test.to_csv(\"predictions/beam/uni_attn.csv\", index=False)\n",
    "print(\"Predictions saved to bi_lstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from heapq import nlargest\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def beam_search_predict(\n",
    "    model,\n",
    "    english_word: str,\n",
    "    word2vec,\n",
    "    max_length_word_marathi: int,\n",
    "    marathi_char_list: List[str],\n",
    "    SOS_IDX: int,\n",
    "    device: torch.device,\n",
    "    beam_width: int = 4,\n",
    "    PAD_IDX: int = 0,\n",
    "    EOS_IDX: Optional[int] = None,\n",
    "    length_norm_alpha: float = 0.0,   # 0.0 -> no length norm; >0 gives light normalization\n",
    "    return_beams: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Beam-search inference for a single example using model.forward(inp, target, 0)\n",
    "    - model.forward expects (seq_len_enc, batch, ...) for inp and (seq_len_dec, batch) for target\n",
    "    - Works for batch size = 1\n",
    "    - beam_width: number of beams kept\n",
    "    - length_norm_alpha: if >0, use (score / (length**alpha)) scoring for tie-breaking\n",
    "    - EOS_IDX: optional; if provided the beam ends when EOS is generated\n",
    "    Returns: (predicted_word, beams) if return_beams else predicted_word\n",
    "    Beams format: list of tuples (token_list, score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # prepare encoder input\n",
    "        eng_vec = word2vec(english_word, \"english\")\n",
    "        inp_tensor = torch.tensor(eng_vec, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len_enc, 1)\n",
    "\n",
    "        # reset model state if applicable\n",
    "        if hasattr(model, \"reset_hidden\"):\n",
    "            try:\n",
    "                model.reset_hidden(batch_size=1)\n",
    "            except TypeError:\n",
    "                model.reset_hidden()\n",
    "\n",
    "        # beam represented as tuple (token_list, logprob_sum, finished_flag)\n",
    "        # token_list includes SOS at position 0\n",
    "        init_beam = ([SOS_IDX], 0.0, False)\n",
    "        beams: List[Tuple[List[int], float, bool]] = [init_beam]\n",
    "\n",
    "        for step in range(1, max_length_word_marathi):\n",
    "            all_candidates: List[Tuple[List[int], float, bool]] = []\n",
    "\n",
    "            # expand each beam\n",
    "            for tokens, logprob, finished in beams:\n",
    "                if finished:\n",
    "                    # carry over finished beams unchanged\n",
    "                    all_candidates.append((tokens, logprob, True))\n",
    "                    continue\n",
    "\n",
    "                # build target tensor for this partial sequence\n",
    "                tgt_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)  # (cur_len, 1)\n",
    "                # forward - we only need logits at last timestep\n",
    "                logits = model.forward(inp_tensor, tgt_tensor, 0)   # (seq_len_dec, batch=1, vocab)\n",
    "                last_logits = logits[-1, 0, :]                      # (vocab,)\n",
    "                log_probs = F.log_softmax(last_logits, dim=0)       # (vocab,)\n",
    "\n",
    "                # pick top-k token candidates for expansion (local pruning)\n",
    "                topk_vals, topk_idx = torch.topk(log_probs, k=min(beam_width, log_probs.size(0)))\n",
    "                topk_vals = topk_vals.cpu().tolist()\n",
    "                topk_idx = topk_idx.cpu().tolist()\n",
    "\n",
    "                for v, idx_token in zip(topk_vals, topk_idx):\n",
    "                    new_tokens = tokens + [int(idx_token)]\n",
    "                    new_logprob = logprob + float(v)\n",
    "                    is_finished = (EOS_IDX is not None and idx_token == EOS_IDX)\n",
    "                    all_candidates.append((new_tokens, new_logprob, is_finished))\n",
    "\n",
    "            # select top beam_width candidates by score (apply optional length norm for ranking)\n",
    "            def score_for_ranking(candidate):\n",
    "                token_list, s, finished_flag = candidate\n",
    "                # length used for normalization: exclude SOS\n",
    "                length = max(1, len(token_list) - 1)\n",
    "                if length_norm_alpha > 0.0:\n",
    "                    return s / (length ** length_norm_alpha)\n",
    "                return s\n",
    "\n",
    "            # keep the top-k candidates\n",
    "            beams = nlargest(beam_width, all_candidates, key=score_for_ranking)\n",
    "\n",
    "            # if all beams finished early, stop\n",
    "            if all(f for (_, _, f) in beams):\n",
    "                break\n",
    "\n",
    "        # choose final best beam: prefer finished beams (if any)\n",
    "        finished_beams = [b for b in beams if b[2]]\n",
    "        if finished_beams:\n",
    "            best_beam = max(finished_beams, key=lambda b: score_for_ranking(b))\n",
    "        else:\n",
    "            # fallback: choose best by raw score (or normalized)\n",
    "            best_beam = max(beams, key=lambda b: score_for_ranking(b))\n",
    "\n",
    "        best_tokens, best_logprob, _ = best_beam\n",
    "\n",
    "        # robust decoding: drop leading PAD/SOS then decode until EOS (if provided)\n",
    "        i = 0\n",
    "        while i < len(best_tokens) and (best_tokens[i] == PAD_IDX or best_tokens[i] == SOS_IDX):\n",
    "            i += 1\n",
    "\n",
    "        decoded_chars = []\n",
    "        for idx in best_tokens[i:]:\n",
    "            if EOS_IDX is not None and idx == EOS_IDX:\n",
    "                break\n",
    "            if 0 < idx <= len(marathi_char_list):\n",
    "                decoded_chars.append(marathi_char_list[idx - 1])\n",
    "\n",
    "        predicted_word = \"\".join(decoded_chars)\n",
    "\n",
    "        if return_beams:\n",
    "            # produce readable beams: (token_list, normalized_score)\n",
    "            readable = []\n",
    "            for tokens, s, finished in beams:\n",
    "                norm_score = s / (max(1, len(tokens) - 1) ** length_norm_alpha) if length_norm_alpha > 0 else s\n",
    "                readable.append((tokens, float(norm_score), finished))\n",
    "            return predicted_word, readable\n",
    "\n",
    "        return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                         | 524/10112 [04:17<1:18:28,  2.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m first_column \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# --- Run predictions ---\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_words_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# --- Save predictions back to DataFrame ---\u001b[39;00m\n\u001b[1;32m     50\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted\n",
      "Cell \u001b[0;32mIn[27], line 27\u001b[0m, in \u001b[0;36mpredict_words_batch\u001b[0;34m(model, words)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tqdm(words):\n\u001b[1;32m     26\u001b[0m             word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(word)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 27\u001b[0m             pred, beams \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43menglish_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword2vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length_word_marathi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length_word_marathi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmarathi_char_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarathi_char_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSOS_IDX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m65\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# set this from your training config / Counter output\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEOS_IDX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# set to your EOS index if known, otherwise None\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_norm_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# your existing function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m             predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[0;32mIn[25], line 61\u001b[0m, in \u001b[0;36mbeam_search_predict\u001b[0;34m(model, english_word, word2vec, max_length_word_marathi, marathi_char_list, SOS_IDX, device, beam_width, PAD_IDX, EOS_IDX, length_norm_alpha, return_beams)\u001b[0m\n\u001b[1;32m     59\u001b[0m tgt_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (cur_len, 1)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# forward - we only need logits at last timestep\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (seq_len_dec, batch=1, vocab)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m last_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]                      \u001b[38;5;66;03m# (vocab,)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(last_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)       \u001b[38;5;66;03m# (vocab,)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, target_len):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m         output, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     33\u001b[0m         output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, encoder_output, hidden)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mAtten_decoder.forward\u001b[0;34m(self, x, output, hidden, cell)\u001b[0m\n\u001b[1;32m     39\u001b[0m     outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(op, hidden)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(outputs)\n\u001b[1;32m     43\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:908\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    905\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:825\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    824\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_cell_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[1] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:260\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    259\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load the test CSV (no headers) ---\n",
    "original_csv = \"../../aks_dataset/hin/test.csv\"\n",
    "df_test = pd.read_csv(original_csv, header=None)\n",
    "\n",
    "# --- Give column names to df_test ---\n",
    "df_test.columns = ['eng', 'actual']  # first column = English, second column = reference Hindi\n",
    "\n",
    "# --- Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Make sure your model is on the device\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# --- Predict function wrapper ---\n",
    "def predict_words_batch(model, words):\n",
    "    \"\"\"Predict multiple words using GPU.\"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for word in tqdm(words):\n",
    "            word = str(word).strip()\n",
    "            pred, beams = beam_search_predict(\n",
    "    model=model,\n",
    "    english_word=word,\n",
    "    word2vec=word2vec,\n",
    "    max_length_word_marathi=max_length_word_marathi,\n",
    "    marathi_char_list=marathi_char_list,\n",
    "    SOS_IDX=65,            # set this from your training config / Counter output\n",
    "    device=device,\n",
    "    beam_width=2,\n",
    "    EOS_IDX=None,            # set to your EOS index if known, otherwise None\n",
    "    length_norm_alpha=0.0,\n",
    "    return_beams=True\n",
    ")  # your existing function\n",
    "            predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# --- Get first column (English words) ---\n",
    "first_column = df_test['eng'].tolist()\n",
    "\n",
    "# --- Run predictions ---\n",
    "predicted = predict_words_batch(model, first_column)\n",
    "\n",
    "# --- Save predictions back to DataFrame ---\n",
    "df_test['predicted'] = predicted\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_test.to_csv(\"predictions/beam/bi_attn.csv\", index=False)\n",
    "print(\"Predictions saved to bi_lstm_predictions.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
