{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRdpoWePeYHn"
   },
   "source": [
    "## Importing Libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0LBvFtYGCNgJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 16 13:58:31 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   58C    P0             132W / 300W |  12488MiB / 81920MiB |     17%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              46W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   57C    P0              51W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              41W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z4ZVrIumZcDt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1760645402.556510] [bfd74565809f:110106:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import wandb\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwL09v65CIse",
    "outputId": "5ea72523-6a50-474c-b617-b77e16d72ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44xIRolL_T_d"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y4zemXiyE6Fi"
   },
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {'#': 0, '$': 1, '^': 2}   # '^': start of sequence, '$' : unknown char, '#' : padding\n",
    "        self.index2char = {0: '#', 1: '$', 2: '^'}\n",
    "        self.vocab_size = 3  # Count\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.vocab_size\n",
    "            self.index2char[self.vocab_size] = char\n",
    "            self.vocab_size += 1\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.char2index[ch] for ch in s]\n",
    "\n",
    "    def decode(self, l):\n",
    "        return ''.join([self.index2char[i] for i in l])\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.char2index.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns maximum length of input and output words\n",
    "def maxLength(data):\n",
    "    ip_mlen, op_mlen = 0, 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        input = data[0][i]\n",
    "        output = data[1][i]\n",
    "        if(len(input)>ip_mlen):\n",
    "            ip_mlen=len(input)\n",
    "\n",
    "        if(len(output)>op_mlen):\n",
    "            op_mlen=len(output)\n",
    "\n",
    "    return ip_mlen, op_mlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getMaxLengthValues(lang):\n",
    "    base_path = f\"../../aks_dataset/{lang}\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(f\"{base_path}/train.csv\", header=None)\n",
    "    val_df = pd.read_csv(f\"{base_path}/valid.csv\", header=None)\n",
    "    test_df = pd.read_csv(f\"{base_path}/test.csv\", header=None)\n",
    "\n",
    "    # Initialize language vocabularies\n",
    "    input_lang = Language('eng')\n",
    "    output_lang = Language(lang)\n",
    "    \n",
    "    # Build vocabulary only from train data\n",
    "    for _, row in train_df.iterrows():\n",
    "        input_lang.addWord(str(row[0]))\n",
    "        output_lang.addWord(str(row[1]))\n",
    "    \n",
    "    # Compute max input/output lengths for each split\n",
    "    m1, m01 = maxLength(train_df)\n",
    "    m2, m02 = maxLength(test_df)\n",
    "    m3, m03 = maxLength(val_df)\n",
    "\n",
    "    # Return the largest values across all splits\n",
    "    return max(m1, m2, m3), max(m01, m02, m03)\n",
    "\n",
    "# Example usage\n",
    "input_max_len, output_max_len = getMaxLengthValues('hin')\n",
    "print(input_max_len, output_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IDGaCO8DkYpc"
   },
   "outputs": [],
   "source": [
    "input_shape = 0\n",
    "def preprocess(data, input_lang, output_lang, input_max_len, output_max_len, s=''):\n",
    "\n",
    "    unknown = input_lang.char2index['$']\n",
    "\n",
    "    n = len(data)\n",
    "    input = torch.zeros((n, input_max_len + 1), device = device)\n",
    "    output = torch.zeros((n, output_max_len + 2), device = device)\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        inp = data[0][i].ljust(input_max_len + 1, '#')\n",
    "        op = '^' + data[1][i]       # add start symbol to output\n",
    "        op = op.ljust(output_max_len + 2, '#')\n",
    "\n",
    "        for index, char in enumerate(inp):\n",
    "            if char in input_lang.char2index:\n",
    "                input[i][index] = input_lang.char2index[char]\n",
    "            else:\n",
    "                input[i][index] = unknown\n",
    "\n",
    "        for index, char in enumerate(op):\n",
    "            if char in output_lang.char2index:\n",
    "                output[i][index] = output_lang.char2index[char]\n",
    "            else:\n",
    "                output[i][index] = unknown\n",
    "\n",
    "    print(s, ' dataset')\n",
    "    print(input.shape)\n",
    "    print(output.shape)\n",
    "\n",
    "    return TensorDataset(input.to(torch.int32), output.to(torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdS5OXKxfdCX",
    "outputId": "283fb51a-9a4a-4fc5-bad1-ea66373b29b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  dataset\n",
      "torch.Size([100000, 30])\n",
      "torch.Size([100000, 28])\n",
      "validation  dataset\n",
      "torch.Size([6357, 30])\n",
      "torch.Size([6357, 28])\n",
      "test  dataset\n",
      "torch.Size([10112, 30])\n",
      "torch.Size([10112, 28])\n"
     ]
    }
   ],
   "source": [
    "def load_prepare_data(lang):\n",
    "    train_df = pd.read_csv(f\"../../aks_dataset/{lang}/train.csv\", header = None)\n",
    "    val_df = pd.read_csv(f\"../../aks_dataset/{lang}/valid.csv\", header = None)\n",
    "    test_df = pd.read_csv(f\"../../aks_dataset/{lang}/test.csv\", header = None)\n",
    "\n",
    "    input_lang = Language('eng')\n",
    "    output_lang = Language(lang)\n",
    "\n",
    "    # create vocablury\n",
    "    for i in range(len(train_df)):\n",
    "        input_lang.addWord(train_df[0][i]) # 'eng'\n",
    "        output_lang.addWord(train_df[1][i]) # 'hin'\n",
    "\n",
    "    # encode the datasets\n",
    "    train_data = preprocess(train_df, input_lang, output_lang,input_max_len, output_max_len, 'train')\n",
    "    val_data = preprocess(val_df, input_lang, output_lang,input_max_len, output_max_len, 'validation')\n",
    "    test_data = preprocess(test_df, input_lang, output_lang,input_max_len, output_max_len, 'test')\n",
    "\n",
    "    return train_data, val_data, test_data, input_lang, output_lang\n",
    "\n",
    "\n",
    "train_data, val_data, test_data, input_lang, output_lang = load_prepare_data('hin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nu-NTR6BDj8e",
    "outputId": "bd3dba2a-092d-4846-a5fb-f703f119b56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urjapurna#####################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^उर्जापूर्ण#################'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_lang.decode(train_data[23][0].tolist()))\n",
    "output_lang.decode(train_data[23][1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJI8iU6dBSE0",
    "outputId": "818815ee-503e-4dcd-b7a6-5f00a06b5ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 42, 15, 18,  8, 12,  3, 29, 15, 18, 43,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[23][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt') as input_file:\n",
    "    key = input_file.read()\n",
    "key = key.split('\\n')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvmzS5Lt_Jnl",
    "outputId": "1387d646-ea3c-4fbf-b44f-c071e2b07784"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msshejole132\u001b[0m (\u001b[33msshejole132-iit-bombay\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key =key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1TioafYgICa"
   },
   "source": [
    "# seq2seq tranformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K94_u35dCk7-"
   },
   "source": [
    "### hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PugX7KHvc65u"
   },
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "n_head = 4 # other options factors of 32 like 2, 8\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "epochs = 50\n",
    "\n",
    "# encoder specific detail\n",
    "input_vocab_size = input_lang.vocab_size\n",
    "encoder_block_size = len(train_data[0][0])\n",
    "\n",
    "# decoder specific detail\n",
    "output_vocab_size = output_lang.vocab_size\n",
    "decoder_block_size = len(train_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdltQ7oJCq1j"
   },
   "source": [
    "### Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uiluDiY7FAMU"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one self-attention head \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, d_k, dropout, mask=0): # d_k is dimention of key , nomaly d_k = n_embd / 4\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.key = nn.Linear(n_embd, d_k, bias=False, device=device)\n",
    "        self.query = nn.Linear(n_embd, d_k, bias=False, device=device)\n",
    "        self.value = nn.Linear(n_embd, d_k, bias=False, device=device)\n",
    "        if mask:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(encoder_block_size, encoder_block_size, device=device)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output = None):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        if encoder_output is not None:\n",
    "            k = self.key(encoder_output)\n",
    "            Be, Te, Ce = encoder_output.shape\n",
    "        else:\n",
    "            k = self.key(x) # (B,T,d_k)\n",
    "\n",
    "        q = self.query(x) # (B,T,d_k)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,T)\n",
    "\n",
    "        if self.mask:\n",
    "            if encoder_output is not None:\n",
    "                wei = wei.masked_fill(self.tril[:T, :Te] == 0, float('-inf')) # (B,T,T)\n",
    "            else:\n",
    "                wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform weighted aggregation of values\n",
    "        if encoder_output is not None:\n",
    "            v = self.value(encoder_output)\n",
    "        else:\n",
    "            v = self.value(x)\n",
    "        out = wei @ v # (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple self attention heads in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_head, d_k, dropout, mask=0):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, d_k, dropout, mask) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        out = torch.cat([h(x, encoder_output) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" multiple self attention heads in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class encoderBlock(nn.Module):\n",
    "    \"\"\" Tranformer encoder block : communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        d_k = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, d_k, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        x = x + self.sa(self.ln1(x), encoder_output)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(input_vocab_size, n_embd) # n_embd: input embedding dimension\n",
    "        self.position_embedding_table = nn.Embedding(encoder_block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[encoderBlock(n_embd, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
    "        x = self.blocks(x) # apply one attention layer (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgPU486JC8Mz"
   },
   "source": [
    "### Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JteOV0CdC_bv"
   },
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    \"\"\" Tranformer decoder block : self communication then cross communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        d_k = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, d_k, dropout, mask = 1)\n",
    "        self.ca = MultiHeadAttention(n_embd, n_head, d_k, dropout, mask = 1)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, device=device)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, device=device)\n",
    "        self.ln3 = nn.LayerNorm(n_embd, device=device)\n",
    "\n",
    "    def forward(self, x_encoder_output):\n",
    "        x = x_encoder_output[0]\n",
    "        encoder_output = x_encoder_output[1]\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ca(self.ln2(x), encoder_output)\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return (x,encoder_output)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(output_vocab_size, n_embd) # n_embd: input embedding dimension\n",
    "        self.position_embedding_table = nn.Embedding(decoder_block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[decoderBlock(n_embd, n_head=n_head, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, output_vocab_size)\n",
    "\n",
    "    def forward(self, idx, encoder_output, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
    "\n",
    "        x =self.blocks((x, encoder_output))\n",
    "        x = self.ln_f(x[0]) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,output_vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            temp_logits = logits.view(B*T, C)\n",
    "            targets = targets.reshape(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(temp_logits, targets.long())\n",
    "\n",
    "        # print(logits)\n",
    "        # out = torch.argmax(logits)\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBjmsIcklM8Y"
   },
   "source": [
    "# Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLfHEDk8FNfY"
   },
   "source": [
    "## sweep config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nDcRZmb80msE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: olbj21xb\n",
      "Sweep URL: https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb\n"
     ]
    }
   ],
   "source": [
    "# Define sweep config\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"val_acc\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [64, 128, 256]},\n",
    "        \"epochs\": {\"values\": [20, 40, 50, 100]},\n",
    "        \"lr\": {\"max\": 0.1, \"min\": 0.0001},\n",
    "        \"n_embd\": {\"values\": [16, 32, 64]},\n",
    "        \"n_head\": {\"values\": [2, 4, 8]},\n",
    "        \"n_layers\": {\"values\": [2]},\n",
    "        \"dropout\": {\"values\": [0, .1, .2, .3]}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Tranliteration-Tranformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9CguGUG5_1NL"
   },
   "outputs": [],
   "source": [
    "# wandb.sweep_cancel(sweep_id)\n",
    "# wandb.finish()\n",
    "# wandb.run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5T58TQRECbZ"
   },
   "source": [
    "## train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3GWnCggNFLs3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train():\n",
    "    run = wandb.init()\n",
    "\n",
    "    n_embd = wandb.config.n_embd\n",
    "    n_head = wandb.config.n_head\n",
    "    n_layers = wandb.config.n_layers\n",
    "    dropout = wandb.config.dropout\n",
    "    epochs = wandb.config.epochs\n",
    "    batch_size = wandb.config.batch_size\n",
    "    learning_rate = wandb.config.lr\n",
    "\n",
    "\n",
    "    encoder = Encoder(n_embd, n_head, n_layers, dropout)\n",
    "    decoder = Decoder(n_embd, n_head, n_layers, dropout)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "\n",
    "    # print the number of parameters in the model\n",
    "    print(sum([p.numel() for p in encoder.parameters()] + [p.numel() for p in decoder.parameters()])/1e3, 'K model parameters')\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    encoder_optimizer = torch.optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# print('Step | Training Loss | Validation Loss   |   Training Accuracy %  |  Validation Accuracy %')\n",
    "\n",
    "    least_error = float('inf')\n",
    "    patience = 20  # The number of epochs without improvement to wait before stopping\n",
    "    no_improvement = 0\n",
    "\n",
    "    for i in tqdm(range(epochs)):\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        for j,(train_x,train_y) in enumerate(train_loader):\n",
    "            train_x = train_x.to(device)\n",
    "            train_y = train_y.to(device)\n",
    "\n",
    "            encoder_optimizer.zero_grad(set_to_none=True)\n",
    "            decoder_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            encoder_output = encoder(train_x)\n",
    "            logits, loss = decoder(train_y[:, :-1], encoder_output, train_y[:, 1:])\n",
    "\n",
    "            encoder_optimizer.zero_grad(set_to_none=True)\n",
    "            decoder_optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss\n",
    "            pred_decoder_output = torch.argmax(logits, dim=-1)\n",
    "            # print(pred_decoder_output, \" target: \", train_y[:, 1:])\n",
    "            train_correct += (pred_decoder_output == train_y[:, 1:]).sum().item()\n",
    "\n",
    "\n",
    "        ## validation code\n",
    "        running_loss_val, val_correct = 0, 0\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        for j,(val_x,val_y) in enumerate(val_loader):\n",
    "            val_x = val_x.to(device)\n",
    "            val_y = val_y.to(device)\n",
    "\n",
    "            encoder_output = encoder(val_x)\n",
    "            logits, loss = decoder(val_y[:, :-1], encoder_output, val_y[:, 1:])\n",
    "\n",
    "            running_loss_val += loss\n",
    "            pred_decoder_output = torch.argmax(logits, dim=-1)\n",
    "            val_correct += torch.sum(pred_decoder_output == val_y[:, 1:])\n",
    "\n",
    "\n",
    "        if running_loss_val < least_error:\n",
    "            least_error = running_loss_val\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {i}\")\n",
    "            break\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss\": running_loss / len(train_data),\n",
    "                \"val_loss\": (running_loss_val/len(val_data)),\n",
    "                \"train_acc\": ((train_correct*100) / (len(train_data)* (decoder_block_size-1))),\n",
    "                \"val_acc\": ((val_correct*100)/(len(val_data)* (decoder_block_size-1))),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxzRR9cjEGDm"
   },
   "source": [
    "## run sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295,
     "referenced_widgets": [
      ""
     ]
    },
    "id": "u_QFbYe32t7r",
    "outputId": "97153eab-b36f-454b-9fed-53ae0287aee1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3o6074mu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.06495675751598949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_140036-3o6074mu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/3o6074mu' target=\"_blank\">good-sweep-1</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/3o6074mu' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/3o6074mu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████▊                       | 36/50 [24:27<09:30, 40.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▃▄▄▅▅▄▄▄▄▄▅▆▂▄▇▇▆▅▄▅▃▃▃▅▃▃▃▃▄▄▅▅▇█</td></tr><tr><td>train_loss</td><td>█▅▅▄▄▃▃▄▃▅▄▄▄▂█▅▂▁▃▃▃▃▅▄▄▃▆▅▅▅▄▄▃▃▂▁</td></tr><tr><td>val_acc</td><td>▂▂▃▄▅▄▂▄▃▄▄▄▅▆▃▄█▅▄▂▃▁▁▃▃▅▂▃▃▂▃▃▆▆▆▆</td></tr><tr><td>val_loss</td><td>▇▆▅▅▄▅▆▆▅▅▅▄▄▃█▄▁▄▅▅▅▇▇▅▅▄█▆▆▆▅▆▃▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>75.60189</td></tr><tr><td>train_loss</td><td>0.00719</td></tr><tr><td>val_acc</td><td>79.19354</td></tr><tr><td>val_loss</td><td>0.00601</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-1</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/3o6074mu' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/3o6074mu</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_140036-3o6074mu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f2qrfdvz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.06325632562397139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_142520-f2qrfdvz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/f2qrfdvz' target=\"_blank\">jolly-sweep-2</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/f2qrfdvz' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/f2qrfdvz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.083 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▉                                    | 56/100 [54:35<42:53, 58.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▂▃▄▅▆▃▁▂▄▄▆▅▃▂▃▄▄▂▃▂▇▅▂▂▂▆█▇▄▅▅▅▂▃▄▆▆▆▅▁</td></tr><tr><td>train_loss</td><td>▇▅▅▄▄█▆▅▄▃▃▆▇▆▅▄▇▆▆▅▂▆▆▇▃▁▁▂▄▄▅▄▇▅▄▃▃▃▄▇</td></tr><tr><td>val_acc</td><td>▄▃▅▆▄▆▄▂▄▅▅▆▂▂▃▄▄▁▁▄▆▃▄▄▅▇█▇▅█▅▇▆▂▆▅▇▇█▃</td></tr><tr><td>val_loss</td><td>▆▆▅▅▆▆▇▆▅▅▅▇▇▆▆▅▇█▆▅▅▅▅▅▄▁▁▃▅▄▃▇▃▆▅▃▃▃▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>73.65207</td></tr><tr><td>train_loss</td><td>0.00762</td></tr><tr><td>val_acc</td><td>77.01338</td></tr><tr><td>val_loss</td><td>0.00657</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-sweep-2</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/f2qrfdvz' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/f2qrfdvz</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_142520-f2qrfdvz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7t4l11zq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.06567673788630166\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_152004-7t4l11zq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/7t4l11zq' target=\"_blank\">divine-sweep-3</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/7t4l11zq' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/7t4l11zq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████▎                                     | 54/100 [10:11<08:40, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▄▄▃▂▅▄▄▅▄▃▃▄▄▃▄▅▆▇▇▇▂▃▅▆▇█▇▆▇▆▅▅▄▄▅▅▆▆▆</td></tr><tr><td>train_loss</td><td>█▅▇▆▆▄▄▄▅▅▆▄▅▄▅▄▃▂▁▂▂▆▆▅▃▂▂▁▂▃▃▃▃▅▄▃▂▂▂▃</td></tr><tr><td>val_acc</td><td>▃▄▃▅▄▅▆▃▅▅▂▄▂▄▄▄▄▅▅▆▅▆▁▂▅▆▇█▆▆▄▅▄▅▄▅▄▅▇▆</td></tr><tr><td>val_loss</td><td>▆█▅▅▆▃▅▄▄█▆▅▆▅▅▅▃▃▃▃▇▆▄▃▃▂▁▂▃▂▄▄▅▄▅▃▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>73.68726</td></tr><tr><td>train_loss</td><td>0.00389</td></tr><tr><td>val_acc</td><td>77.90537</td></tr><tr><td>val_loss</td><td>0.00325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-3</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/7t4l11zq' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/7t4l11zq</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_152004-7t4l11zq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sprn15hj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.02257347420160668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_153029-sprn15hj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/sprn15hj' target=\"_blank\">giddy-sweep-4</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/sprn15hj' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/sprn15hj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [21:59<00:00, 26.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▄▆▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>95.55844</td></tr><tr><td>train_loss</td><td>0.00051</td></tr><tr><td>val_acc</td><td>96.42797</td></tr><tr><td>val_loss</td><td>0.00043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-sweep-4</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/sprn15hj' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/sprn15hj</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_153029-sprn15hj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dgyvqndu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0286256627835069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_155303-dgyvqndu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/dgyvqndu' target=\"_blank\">olive-sweep-5</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/dgyvqndu' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/dgyvqndu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [17:46<00:00, 26.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▃▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████▇█▇▇</td></tr><tr><td>val_loss</td><td>█▆▆▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>87.06459</td></tr><tr><td>train_loss</td><td>0.00166</td></tr><tr><td>val_acc</td><td>91.00903</td></tr><tr><td>val_loss</td><td>0.0011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-sweep-5</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/dgyvqndu' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/dgyvqndu</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_155303-dgyvqndu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4mw8e64o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.011861753734323697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_161103-4mw8e64o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/4mw8e64o' target=\"_blank\">effortless-sweep-6</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/4mw8e64o' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/4mw8e64o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [22:43<00:00, 27.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▆▇▇▇▇▇▇████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▇▆▇▇▇▇▇▇▇▇███████▇████████████████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>96.10641</td></tr><tr><td>train_loss</td><td>0.00044</td></tr><tr><td>val_acc</td><td>96.68374</td></tr><tr><td>val_loss</td><td>0.0004</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-sweep-6</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/4mw8e64o' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/4mw8e64o</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_161103-4mw8e64o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cgkm9666 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007296185855534168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_163353-cgkm9666</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/cgkm9666' target=\"_blank\">northern-sweep-7</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/cgkm9666' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/cgkm9666</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [18:48<00:00, 28.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▆▇▇▇▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▆▇▆▇▇▇▇▇▇█▇▇▇▇█▇████▇▇███████▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>97.40063</td></tr><tr><td>train_loss</td><td>0.00029</td></tr><tr><td>val_acc</td><td>96.85211</td></tr><tr><td>val_loss</td><td>0.0004</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-7</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/cgkm9666' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/cgkm9666</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_163353-cgkm9666/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x7lyojxw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.015215530714441816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_165249-x7lyojxw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/x7lyojxw' target=\"_blank\">dauntless-sweep-8</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/x7lyojxw' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/x7lyojxw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [16:11<00:00, 24.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇▇▇▇██▇█▇█████████████████████████</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>96.97322</td></tr><tr><td>train_loss</td><td>0.00034</td></tr><tr><td>val_acc</td><td>96.7822</td></tr><tr><td>val_loss</td><td>0.0004</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-8</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/x7lyojxw' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/x7lyojxw</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_165249-x7lyojxw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ol3n0jph with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0023829004625305324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_170919-ol3n0jph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ol3n0jph' target=\"_blank\">atomic-sweep-9</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ol3n0jph' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ol3n0jph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [07:45<00:00, 23.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▆▇▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>97.05419</td></tr><tr><td>train_loss</td><td>0.00033</td></tr><tr><td>val_acc</td><td>96.65926</td></tr><tr><td>val_loss</td><td>0.00042</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-9</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ol3n0jph' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ol3n0jph</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_170919-ol3n0jph/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 86triun7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.030047885208825384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_171713-86triun7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/86triun7' target=\"_blank\">jolly-sweep-10</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/86triun7' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/86triun7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [19:19<00:00, 28.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▃▃▃▃▃▃▃▄▄▄▄▅▄▄▅▅▅▅▅▅▆▆▇▇▇▇█████▇▅▄▄▅▅▅▅</td></tr><tr><td>train_loss</td><td>█▆▆▆▅▆▅▆▅▅▅▅▄▅▄▄▄▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▂▃▄▄▄▄▄▄</td></tr><tr><td>val_acc</td><td>▁▂▂▂▂▂▂▂▃▃▃▄▄▃▄▄▄▄▄▅▅▆▆▇▇▇█▇███▇▇▄▄▄▄▄▄▄</td></tr><tr><td>val_loss</td><td>█▇▆▆▆▇▆▆▆▆▅▅▅▅▅▅▅▄▅▄▃▃▂▂▂▂▁▁▁▁▁▂▂▅▅▅▄▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>83.87774</td></tr><tr><td>train_loss</td><td>0.00216</td></tr><tr><td>val_acc</td><td>86.87244</td></tr><tr><td>val_loss</td><td>0.00176</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-sweep-10</strong> at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/86triun7' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/86triun7</a><br> View project at: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251016_171713-86triun7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ay9in1mq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006086129056137431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_embd: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/English-Hindi-Transliteration-using-Deep-Learning/notebooks/wandb/run-20251016_173646-ay9in1mq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ay9in1mq' target=\"_blank\">devoted-sweep-11</a></strong> to <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/sweeps/olbj21xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ay9in1mq' target=\"_blank\">https://wandb.ai/sshejole132-iit-bombay/Tranliteration-Tranformers/runs/ay9in1mq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.787 K model parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████████████████▊                            | 33/50 [14:26<07:13, 25.50s/it]"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNtTaEc6kxuC"
   },
   "source": [
    "# Test Time\n",
    "Since this is the best model(validation accuracy) , we will train it on both train and validation data.\n",
    "We will then test the model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcgfjfD9lvWJ"
   },
   "source": [
    "## Best Hyperparameter from validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q7SXqJhekxuC",
    "outputId": "17c0dfd2-2e0b-4449-80fe-9f7a2ce68c28"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 9\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(n_embd, n_head, n_layers, dropout)\n\u001b[1;32m     11\u001b[0m encoder\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[10], line 93\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, n_embd, n_head, n_layers, dropout)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_embd, n_head, n_layers, dropout):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[43minput_vocab_size\u001b[49m, n_embd) \u001b[38;5;66;03m# n_embd: input embedding dimension\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(encoder_block_size, n_embd)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[encoderBlock(n_embd, n_head, dropout) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "n_embd = 64\n",
    "batch_size = 256\n",
    "learning_rate = 0.003699\n",
    "n_head = 4\n",
    "n_layers = 6\n",
    "dropout = 0\n",
    "epochs = 20\n",
    "\n",
    "encoder = Encoder(n_embd, n_head, n_layers, dropout)\n",
    "decoder = Decoder(n_embd, n_head, n_layers, dropout)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "print(\"✅ Hyperparameters set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0-9k1L6l0iZ"
   },
   "source": [
    "## Train on train_data + val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQVFJyvlTMjS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum([p.numel() for p in encoder.parameters()] + [p.numel() for p in decoder.parameters()])/1e3, 'K model parameters')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "encoder_optimizer = torch.optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# print('Step | Training Loss | Validation Loss   |   Training Accuracy %  |  Validation Accuracy %')\n",
    "\n",
    "least_error = float('inf')\n",
    "patience = 20  # The number of epochs without improvement to wait before stopping\n",
    "no_improvement = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for j,(train_x,train_y) in enumerate(train_loader):\n",
    "        train_x = train_x.to(device)\n",
    "        train_y = train_y.to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad(set_to_none=True)\n",
    "        decoder_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        encoder_output = encoder(train_x)\n",
    "        logits, loss = decoder(train_y[:, :-1], encoder_output, train_y[:, 1:])\n",
    "\n",
    "        encoder_optimizer.zero_grad(set_to_none=True)\n",
    "        decoder_optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        running_loss += loss\n",
    "        pred_decoder_output = torch.argmax(logits, dim=-1)\n",
    "        # print(pred_decoder_output, \" target: \", train_y[:, 1:])\n",
    "        train_correct += (pred_decoder_output == train_y[:, 1:]).sum().item()\n",
    "\n",
    "    for j,(train_x,train_y) in enumerate(val_loader):\n",
    "        train_x = train_x.to(device)\n",
    "        train_y = train_y.to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad(set_to_none=True)\n",
    "        decoder_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        encoder_output = encoder(train_x)\n",
    "        logits, loss = decoder(train_y[:, :-1], encoder_output, train_y[:, 1:])\n",
    "\n",
    "        encoder_optimizer.zero_grad(set_to_none=True)\n",
    "        decoder_optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        running_loss += loss\n",
    "        pred_decoder_output = torch.argmax(logits, dim=-1)\n",
    "        # print(pred_decoder_output, \" target: \", train_y[:, 1:])\n",
    "        train_correct += (pred_decoder_output == train_y[:, 1:]).sum().item()\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "            \"train_loss\": running_loss.cpu().detach().numpy() / (len(train_data)+len(val_data)),\n",
    "            \"train_acc\": ((train_correct*100) / ((len(train_data)+len(val_data))* (decoder_block_size-1))),\n",
    "        }\n",
    "    if i % 5 == 0:\n",
    "        print(\"Step: \",i)\n",
    "        print(\"train_loss: \", metrics[\"train_loss\"])\n",
    "        print(\"train_acc: \", metrics[\"train_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hAjg5s0IkxuC"
   },
   "outputs": [],
   "source": [
    "PATH = 'models/transformer6-layer-encoder.pth'\n",
    "torch.save(encoder, PATH)\n",
    "PATH = 'models/transformer-6-layer-decoder.pth'\n",
    "torch.save(decoder, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4M3aMxTl-zb"
   },
   "source": [
    "## generate output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mfIxu6njkxuD"
   },
   "outputs": [],
   "source": [
    "def generate(input):\n",
    "    B, T = input.shape\n",
    "    encoder_output = encoder(input)\n",
    "    idx = torch.full((B, 1), 2, dtype=torch.long, device=device) # (B,1)\n",
    "\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(decoder_block_size-1):\n",
    "        # get the predictions\n",
    "        logits, loss = decoder(idx, encoder_output) # logits (B, T, vocab_size)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeB2nYeFmXy8"
   },
   "source": [
    "## Check Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dIzXiSLBkxuD",
    "outputId": "ebe1d201-32bb-4372-e64a-62ebe173799d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         val_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39msum(output[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m val_y[:, \u001b[38;5;241m1\u001b[39m:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest accuracy(word level) : \u001b[39m\u001b[38;5;124m\"\u001b[39m, ((val_correct\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_data)))\n\u001b[0;32m---> 24\u001b[0m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mcheck\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m():\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## validation code\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     running_loss_val, val_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m     decoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "def check():\n",
    "## validation code\n",
    "    running_loss_val, val_correct = 0, 0\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    for _ in range(50):\n",
    "        val_x,val_y = next(iter(test_loader))\n",
    "\n",
    "        val_x = val_x.to(device)\n",
    "        val_y = val_y.to(device)\n",
    "\n",
    "        output = generate(val_x)\n",
    "\n",
    "        encoder_output = encoder(val_x)\n",
    "        logits, loss = decoder(val_y[:, :-1], encoder_output, val_y[:, 1:])\n",
    "\n",
    "        running_loss_val += loss\n",
    "        # checking val_correct for the whole sequence\n",
    "        val_correct += torch.sum(torch.sum(output[:, 1:] != val_y[:, 1:], dim=-1) == 0)\n",
    "\n",
    "    print(\"test accuracy(word level) : \", ((val_correct.cpu().detach().numpy()*100) / len(test_data)))\n",
    "\n",
    "check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDP4KvWdFnIL"
   },
   "source": [
    "# Plotting the Attention HeatMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WfJEdcgFmiI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.font_manager import FontProperties\n",
    "tel_font = FontProperties(fname = 'TiroDevanagariHindi-Regular.ttf')\n",
    "# Assuming you have attention_weights of shape (batch_size, output_sequence_length, batch_size, input_sequence_length)\n",
    "# and prediction_matrix of shape (batch_size, output_sequence_length)\n",
    "# and input_matrix of shape (batch_size, input_sequence_length)\n",
    "\n",
    "# Define the grid dimensions\n",
    "rows = int(np.ceil(np.sqrt(12)))\n",
    "cols = int(np.ceil(12 / rows))\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(9, 9))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < 12:\n",
    "        prediction = [opLang.index2char[j.item()] for j in pred[i+1]]\n",
    "\n",
    "        pred_word=\"\"\n",
    "        input_word=\"\"\n",
    "\n",
    "        for j in range(len(prediction)):\n",
    "            # Ignore padding\n",
    "            if(prediction[j] != '#'):\n",
    "                pred_word += prediction[j]\n",
    "            else :\n",
    "                break\n",
    "        input_seq = [ipLang.index2char[j.item()] for j in testData[i][0]]\n",
    "\n",
    "        for j in range(len(input_seq)):\n",
    "            if(input_seq[j] != '#'):\n",
    "                    input_word += input_seq[j]\n",
    "            else :\n",
    "                break\n",
    "        attn_weights = atten_weights[i, :len(pred_word), :len(input_word)].detach().cpu().numpy()\n",
    "        ax.imshow(attn_weights.T, cmap='hot', interpolation='nearest')\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        ax.set_title(f'Example {i+1}')\n",
    "        ax.set_xlabel('Output predicted')\n",
    "        ax.set_ylabel('Input word')\n",
    "        ax.set_xticks(np.arange(len(pred_word)))\n",
    "        ax.set_xticklabels(pred_word, rotation = 90, fontproperties = tel_font,fontdict={'fontsize':8})\n",
    "        ax.xaxis.tick_top()\n",
    "\n",
    "        ax.set_yticks(np.arange(len(input_word)))\n",
    "        ax.set_yticklabels(input_word, rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "wandb.init(project='CS6910_Assignment_3')\n",
    "\n",
    "# Convert the matplotlib figure to an image\n",
    "fig.canvas.draw()\n",
    "image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "# Log the image in wandb\n",
    "wandb.log({\"attention_heatmaps\": [wandb.Image(image)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnHR_oql6-S4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hRdpoWePeYHn",
    "44xIRolL_T_d",
    "XdltQ7oJCq1j",
    "GgPU486JC8Mz",
    "658W9RARGEUf",
    "q7fAgs5uQni_",
    "n4rGh7vuQqaa",
    "nvyRJWUUbR2f",
    "8ETW0BG_Pa24",
    "MQPGy32rnD3V",
    "z_aYZvDD1OHU",
    "pKvBd5mKf0Hf",
    "FYMa5jTQRUaB",
    "zfuv5FoA1wt2",
    "W7CYNChRGuGK"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4721249,
     "sourceId": 8013732,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
