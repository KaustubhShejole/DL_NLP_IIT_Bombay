{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d40ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hin_train.json:\n",
      "  Total instances: 1299155\n",
      "  Instances with 'score' field: 1299155\n",
      "----------------------------------------\n",
      "hin_valid.json:\n",
      "  Total instances: 6357\n",
      "  Instances with 'score' field: 6357\n",
      "----------------------------------------\n",
      "hin_test.json:\n",
      "  Total instances: 10112\n",
      "  Instances with 'score' field: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "files = [\"hin_train.json\", \"hin_valid.json\", \"hin_test.json\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "    \n",
    "    total = len(data)\n",
    "    with_score = [d for d in data if \"score\" in d]\n",
    "    \n",
    "    print(f\"{fname}:\")\n",
    "    print(f\"  Total instances: {total}\")\n",
    "    print(f\"  Instances with 'score' field: {len(with_score)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afbbfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed JSON saved and converted to .txt successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Files\n",
    "train_file = \"hin_train.json\"\n",
    "test_file = \"hin_test.json\"\n",
    "\n",
    "# Step 1: Process & random subsample\n",
    "def process_json(input_file, output_file, sample_size=None):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "    \n",
    "    if sample_size is not None:\n",
    "        data = random.sample(data, min(sample_size, len(data)))  # random subsample\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for item in data:\n",
    "            json.dump(item, out_f, ensure_ascii=False)\n",
    "            out_f.write(\"\\n\")\n",
    "    return output_file\n",
    "\n",
    "train_processed_file = process_json(train_file, \"hin_train__processed_random.json\", sample_size=100000)\n",
    "test_processed_file = process_json(test_file, \"hin_test__processed.json\")  # keep full test\n",
    "\n",
    "# Step 2: Convert processed JSON to tab-separated .txt\n",
    "def json_to_txt(json_file, txt_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f, open(txt_file, 'w', encoding='utf-8') as out_f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                eng_word = item.get('english word', '').strip()\n",
    "                hin_word = item.get('native word', '').strip()\n",
    "                if eng_word and hin_word:\n",
    "                    out_f.write(f\"{eng_word}\\t{hin_word}\\n\")\n",
    "\n",
    "json_to_txt(train_processed_file, \"dataset/train_translit.txt\")\n",
    "json_to_txt(test_processed_file, \"dataset/test_translit.txt\")\n",
    "\n",
    "print(\"Processed JSON saved and converted to .txt successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7bf0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified sampling complete — processed JSON & .csv files saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "os.makedirs(\"aks_dataset\", exist_ok=True)\n",
    "\n",
    "# Files\n",
    "train_file = \"hin_train.json\"\n",
    "test_file = \"hin_test.json\"\n",
    "valid_file = \"hin_valid.json\"\n",
    "\n",
    "def process_json_stratified(input_file, output_file, sample_size=None):\n",
    "    # Step 1: Load JSON lines\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "    \n",
    "    if sample_size is not None:\n",
    "        # Step 2: Group by 'source' field\n",
    "        groups = defaultdict(list)\n",
    "        for item in data:\n",
    "            src = item.get('source', 'unknown')\n",
    "            groups[src].append(item)\n",
    "        \n",
    "        total_count = len(data)\n",
    "        stratified_sample = []\n",
    "\n",
    "        # Step 3: Sample proportionally per source\n",
    "        for src, items in groups.items():\n",
    "            proportion = len(items) / total_count\n",
    "            n_samples = int(round(proportion * sample_size))\n",
    "            n_samples = min(n_samples, len(items))  # avoid oversampling\n",
    "            stratified_sample.extend(random.sample(items, n_samples))\n",
    "        \n",
    "        data = stratified_sample\n",
    "        random.shuffle(data)\n",
    "    \n",
    "    # Step 4: Write to output\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for item in data:\n",
    "            json.dump(item, out_f, ensure_ascii=False)\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# Step 1: Stratified sample for train (100k)\n",
    "train_processed_file = process_json_stratified(\n",
    "    train_file,\n",
    "    \"dataset/hin_train__processed_stratified.json\",\n",
    "    sample_size=100000\n",
    ")\n",
    "\n",
    "# Step 2: Keep full test and valid sets\n",
    "test_processed_file = process_json_stratified(\n",
    "    test_file, \"dataset/hin_test__processed.json\"\n",
    ")\n",
    "valid_processed_file = process_json_stratified(\n",
    "    valid_file, \"dataset/hin_valid__processed.json\"\n",
    ")\n",
    "\n",
    "# Step 3: Convert processed JSON → CSV (no headers)\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f, open(csv_file, 'w', newline='', encoding='utf-8') as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                eng_word = item.get('english word', '').strip()\n",
    "                hin_word = item.get('native word', '').strip()\n",
    "                if eng_word and hin_word:\n",
    "                    writer.writerow([eng_word, hin_word])\n",
    "\n",
    "json_to_csv(train_processed_file, \"aks_dataset/train_s_translit.csv\")\n",
    "json_to_csv(test_processed_file, \"aks_dataset/test_translit.csv\")\n",
    "json_to_csv(valid_processed_file, \"aks_dataset/valid_translit.csv\")\n",
    "\n",
    "print(\"Stratified sampling complete — processed JSON & .csv files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51b2d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0e120\">\n",
       "  <caption>Original train source distribution (counts & %)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0e120_level0_col0\" class=\"col_heading level0 col0\" >source</th>\n",
       "      <th id=\"T_0e120_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n",
       "      <th id=\"T_0e120_level0_col2\" class=\"col_heading level0 col2\" >proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0e120_row0_col0\" class=\"data row0 col0\" >IndicCorp</td>\n",
       "      <td id=\"T_0e120_row0_col1\" class=\"data row0 col1\" >956,190</td>\n",
       "      <td id=\"T_0e120_row0_col2\" class=\"data row0 col2\" >73.601%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0e120_row1_col0\" class=\"data row1 col0\" >Samanantar</td>\n",
       "      <td id=\"T_0e120_row1_col1\" class=\"data row1 col1\" >152,778</td>\n",
       "      <td id=\"T_0e120_row1_col2\" class=\"data row1 col2\" >11.760%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0e120_row2_col0\" class=\"data row2 col0\" >Existing</td>\n",
       "      <td id=\"T_0e120_row2_col1\" class=\"data row2 col1\" >131,773</td>\n",
       "      <td id=\"T_0e120_row2_col2\" class=\"data row2 col2\" >10.143%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0e120_row3_col0\" class=\"data row3 col0\" >Dakshina</td>\n",
       "      <td id=\"T_0e120_row3_col1\" class=\"data row3 col1\" >24,727</td>\n",
       "      <td id=\"T_0e120_row3_col2\" class=\"data row3 col2\" >1.903%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0e120_row4_col0\" class=\"data row4 col0\" >Wikidata</td>\n",
       "      <td id=\"T_0e120_row4_col1\" class=\"data row4 col1\" >24,528</td>\n",
       "      <td id=\"T_0e120_row4_col2\" class=\"data row4 col2\" >1.888%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0e120_row5_col0\" class=\"data row5 col0\" >AK-Freq</td>\n",
       "      <td id=\"T_0e120_row5_col1\" class=\"data row5 col1\" >9,159</td>\n",
       "      <td id=\"T_0e120_row5_col2\" class=\"data row5 col2\" >0.705%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e120_level0_row6\" class=\"row_heading level0 row6\" >sum</th>\n",
       "      <td id=\"T_0e120_row6_col0\" class=\"data row6 col0\" >sum</td>\n",
       "      <td id=\"T_0e120_row6_col1\" class=\"data row6 col1\" >1,299,155</td>\n",
       "      <td id=\"T_0e120_row6_col2\" class=\"data row6 col2\" >100.000%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa62091ec20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1e848\">\n",
       "  <caption>Stratified data (Our Train Set) (total=100k)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1e848_level0_col0\" class=\"col_heading level0 col0\" >source</th>\n",
       "      <th id=\"T_1e848_level0_col1\" class=\"col_heading level0 col1\" >sampled_count</th>\n",
       "      <th id=\"T_1e848_level0_col2\" class=\"col_heading level0 col2\" >sampled_proportion_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1e848_row0_col0\" class=\"data row0 col0\" >IndicCorp</td>\n",
       "      <td id=\"T_1e848_row0_col1\" class=\"data row0 col1\" >73601</td>\n",
       "      <td id=\"T_1e848_row0_col2\" class=\"data row0 col2\" >73.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1e848_row1_col0\" class=\"data row1 col0\" >Samanantar</td>\n",
       "      <td id=\"T_1e848_row1_col1\" class=\"data row1 col1\" >11760</td>\n",
       "      <td id=\"T_1e848_row1_col2\" class=\"data row1 col2\" >11.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1e848_row2_col0\" class=\"data row2 col0\" >Existing</td>\n",
       "      <td id=\"T_1e848_row2_col1\" class=\"data row2 col1\" >10143</td>\n",
       "      <td id=\"T_1e848_row2_col2\" class=\"data row2 col2\" >10.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1e848_row3_col0\" class=\"data row3 col0\" >Dakshina</td>\n",
       "      <td id=\"T_1e848_row3_col1\" class=\"data row3 col1\" >1903</td>\n",
       "      <td id=\"T_1e848_row3_col2\" class=\"data row3 col2\" >1.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1e848_row4_col0\" class=\"data row4 col0\" >Wikidata</td>\n",
       "      <td id=\"T_1e848_row4_col1\" class=\"data row4 col1\" >1888</td>\n",
       "      <td id=\"T_1e848_row4_col2\" class=\"data row4 col2\" >1.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1e848_row5_col0\" class=\"data row5 col0\" >AK-Freq</td>\n",
       "      <td id=\"T_1e848_row5_col1\" class=\"data row5 col1\" >705</td>\n",
       "      <td id=\"T_1e848_row5_col2\" class=\"data row5 col2\" >0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e848_level0_row6\" class=\"row_heading level0 row6\" >sum</th>\n",
       "      <td id=\"T_1e848_row6_col0\" class=\"data row6 col0\" >sum</td>\n",
       "      <td id=\"T_1e848_row6_col1\" class=\"data row6 col1\" >100000</td>\n",
       "      <td id=\"T_1e848_row6_col2\" class=\"data row6 col2\" >100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa62091ec20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)  # reproducible sampling\n",
    "\n",
    "TRAIN_FILE = \"rough/hin_train.json\"\n",
    "SAMPLE_SIZE = 100000  # desired stratified sample size\n",
    "\n",
    "# Load jsonl\n",
    "with open(TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Build counts by source\n",
    "groups = defaultdict(list)\n",
    "for item in data:\n",
    "    src = item.get('source', 'unknown')\n",
    "    groups[src].append(item)\n",
    "\n",
    "# Original distribution dataframe\n",
    "orig_rows = []\n",
    "total_count = len(data)\n",
    "for src, items in sorted(groups.items(), key=lambda x: -len(x[1])):\n",
    "    count = len(items)\n",
    "    proportion = count / total_count if total_count > 0 else 0.0\n",
    "    orig_rows.append((src, count, proportion))\n",
    "\n",
    "orig_df = pd.DataFrame(orig_rows, columns=['source', 'count', 'proportion'])\n",
    "orig_df['proportion'] = (orig_df['proportion'] * 100).round(3)  # show as percent\n",
    "orig_df.loc['sum'] = ['sum', orig_df['count'].sum(), orig_df['proportion'].sum()]\n",
    "\n",
    "# Create stratified sample in-memory (no saving)\n",
    "# If sample size > total, fall back to full data (no sampling)\n",
    "if SAMPLE_SIZE >= total_count:\n",
    "    sampled = data.copy()\n",
    "else:\n",
    "    sampled = []\n",
    "    for src, items in groups.items():\n",
    "        proportion = len(items) / total_count\n",
    "        n_samples = int(round(proportion * SAMPLE_SIZE))\n",
    "        # avoid oversampling adjustments (clip to available)\n",
    "        n_samples = min(n_samples, len(items))\n",
    "        if n_samples > 0:\n",
    "            sampled.extend(random.sample(items, n_samples))\n",
    "    # If rounding caused fewer/more than SAMPLE_SIZE, adjust by random picks\n",
    "    # If fewer, add random items from remaining; if more, trim randomly\n",
    "    if len(sampled) < SAMPLE_SIZE:\n",
    "        remaining = [itm for itm in data if itm not in sampled]\n",
    "        need = SAMPLE_SIZE - len(sampled)\n",
    "        if remaining:\n",
    "            sampled.extend(random.sample(remaining, min(need, len(remaining))))\n",
    "    elif len(sampled) > SAMPLE_SIZE:\n",
    "        sampled = random.sample(sampled, SAMPLE_SIZE)\n",
    "\n",
    "sample_total = len(sampled)\n",
    "\n",
    "# Compute sampled distribution\n",
    "sample_groups = defaultdict(int)\n",
    "for item in sampled:\n",
    "    src = item.get('source', 'unknown')\n",
    "    sample_groups[src] += 1\n",
    "\n",
    "sample_rows = []\n",
    "for src, items in sorted(groups.items(), key=lambda x: -len(x[1])):\n",
    "    sampled_count = sample_groups.get(src, 0)\n",
    "    sampled_prop = sampled_count / sample_total * 100 if sample_total > 0 else 0.0\n",
    "    sample_rows.append((src, sampled_count, round(sampled_prop, 3)))\n",
    "\n",
    "sample_df = pd.DataFrame(sample_rows, columns=['source', 'sampled_count', 'sampled_proportion_pct'])\n",
    "sample_df.loc['sum'] = ['sum', sample_df['sampled_count'].sum(), sample_df['sampled_proportion_pct'].sum()]\n",
    "\n",
    "# Print nicely: in notebooks display will show styled tables, otherwise fallback to to_string()\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(orig_df.style.set_caption(\"Original train source distribution (counts & %)\").format({'count':'{:,}','proportion':'{:.3f}%'}))\n",
    "    display(sample_df.style.set_caption(f\"Stratified data (Our Train Set) (total=100k)\"))\n",
    "except Exception:\n",
    "    print(\"\\nOriginal train source distribution (counts & %):\")\n",
    "    print(orig_df.to_string(index=False))\n",
    "    print(f\"\\nStratified data (Our Train Set) (total=100k):\")\n",
    "    print(sample_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_txt(json_file, txt_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f, open(txt_file, 'w', encoding='utf-8') as out_f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                eng_word = item.get('english word', '').strip()\n",
    "                hin_word = item.get('native word', '').strip()\n",
    "                if eng_word and hin_word:\n",
    "                    out_f.write(f\"{eng_word}\\t{hin_word}\\n\")\n",
    "\n",
    "json_to_txt(train_processed_file, \"dataset/train_s_translit.txt\")\n",
    "json_to_txt(test_processed_file, \"dataset/test_translit.txt\")\n",
    "\n",
    "print(\"✅ Stratified sampling complete — processed JSON & .txt files saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
