## CS772: Deep Learning for Natural Language Processing - Course Projects

This repository contains two major project assignments completed for the **CS772: Deep Learning for NLP** course at **IIT Bombay**. The work explores various modeling techniques, from traditional statistical methods to modern deep learning architectures and Large Language Models (LLMs), for core sequence prediction problems.

---

### ğŸ“ Course and Authors

* **Course:** CS772 - Deep Learning for Natural Language Processing
* **Institution:** IIT Bombay
* **Authors:**
    * Shalaka Thorat (24M0848)
    * Kaustubh Shivshankar Shejole (24M2109)

---

### ğŸ“š Project Summary

The assignments focus on implementing, training, and rigorously comparing different models to solve fundamental NLP tasks.

| Assignment | Topic | Goal | Models Implemented | Key Concepts Highlighted |
| :--- | :--- | :--- | :--- | :--- |
| **Assignment 1** | Part-of-Speech (POS) Tagging | Assign the correct grammatical tag to each word in a sequence. | HMM (Viterbi), LSTM, LLMs (Mistral/GPT) | Dynamic Programming, Contextual Modeling, Unseen Word Handling (Laplace Smoothing). |
| **Assignment 2** | Transliteration | Convert text from **Roman (Latin)** script to **Devanagari (Hindi)** script. | LSTM Encoder-Decoder, Transformer (Attention), LLM (Mistral) | Character-level Sequence-to-Sequence, Self-Attention, Phonetic-Orthographic Ambiguity. |

---

### ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ aksharantar_sampled/
â”‚   â”œâ”€â”€ asm/
â”‚   â”œâ”€â”€ ben/
â”‚   â”œâ”€â”€ brx/
â”‚   â”œâ”€â”€ guj/
â”‚   â”œâ”€â”€ hin/
â”‚   â”œâ”€â”€ kan/
â”‚   â”œâ”€â”€ kas/
â”‚   â”œâ”€â”€ kok/
â”‚   â”œâ”€â”€ mai/
â”‚   â”œâ”€â”€ mal/
â”‚   â”œâ”€â”€ mar/
â”‚   â”œâ”€â”€ mni/
â”‚   â”œâ”€â”€ ori/
â”‚   â”œâ”€â”€ pan/
â”‚   â”œâ”€â”€ san/
â”‚   â”œâ”€â”€ sid/
â”‚   â”œâ”€â”€ tam/
â”‚   â”œâ”€â”€ tel/
â”‚   â””â”€â”€ urd/
â”œâ”€â”€ aks_dataset/
â”‚   â””â”€â”€ hin/
â”œâ”€â”€ Assignment1/
â”‚   â”œâ”€â”€ comparison_with_gpt5/
â”‚   â”œâ”€â”€ final_hmm/
â”‚   â”œâ”€â”€ final_llm/
â”‚   â”œâ”€â”€ final_lstm/
â”‚   â””â”€â”€ lstm2/
â””â”€â”€ Assignment2/
    â”œâ”€â”€ dataset/
    â”œâ”€â”€ final_llm/
    â”‚   â””â”€â”€ .gradio/
    â””â”€â”€ notebooks/
        â”œâ”€â”€ models/
        â””â”€â”€ predictions/

```
---

### â­ Comparative Analysis & Key Learnings

The projects provided a strong comparative view of modeling techniques:

| Comparison | Observation | Conclusion |
| :--- | :--- | :--- |
| **HMM vs. LSTM** | LSTMs (e.g., **96% accuracy** on POS) consistently outperformed HMMs (e.g., **95%**) because they capture **long-range contextual dependencies**, while HMMs are limited to the Markov assumption (local context). | **Deep Learning models** are superior for tasks requiring sophisticated context modeling. |
| **LSTM vs. Transformer** | **Transformers** excel in tasks requiring complex alignment and non-sequential dependencies (like Transliteration) due to the **Self-Attention mechanism**, which processes all tokens in parallel. | **Attention-based architectures** provide a powerful, generalized framework for complex sequence-to-sequence problems. |
| **Traditional DL vs. LLMs** | **LLMs** (like GPT-5-mini in POS Tagging) demonstrate superior zero/few-shot performance and overall generalization (e.g., **94% accuracy**). However, they sometimes fail on highly specific, rule-based character mappings (Transliteration). | **LLMs are state-of-the-art** for semantic and general NLP tasks, but dedicated **Seq2Seq models are essential** for explicit, character-level transformations. |

---

### ğŸ”— Common References

* **Textbook:** Pushpak Bhattacharyya and Aditya Madhav Joshi, *Natural Language Processing*.
* **Theory:** Jurafsky & Martin, *Speech and Language Processing*.
* **Corpora:** NLTK (Brown, Penn Treebank), Aksharantar Corpus.
